[
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "notes",
    "section": "",
    "text": "This is meant as a more casual place to post more unstructured notes that I still want to share.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nAug 28, 2025\n\n\nImai et al. (notes, mostly from memory)\n\n\nVilgot Huhn\n\n\n\n\nMay 27, 2025\n\n\nmediation_sim_may_WIP\n\n\nVilgot Huhn\n\n\n\n\nMar 21, 2025\n\n\nStats2IndividualAssignment\n\n\nVilgot Huhn\n\n\n\n\nFeb 23, 2025\n\n\nStats2_seminar_4\n\n\nVilgot Huhn\n\n\n\n\nFeb 22, 2025\n\n\nStats2_seminar_3\n\n\nVilgot Huhn\n\n\n\n\nFeb 21, 2025\n\n\nStats2_seminar_2\n\n\nVilgot Huhn\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/20250506_lmm_explorer/index.html",
    "href": "posts/20250506_lmm_explorer/index.html",
    "title": "Longitudinal Mixed Model Explorer App",
    "section": "",
    "text": "So far learning about mixed models have been challenging but fun. I guess it’s par for the course for doctoral students to do a lot of self-supervised non-linear learning.\nWhen it comes to understanding statistics I feel like simulating data and applying methods to that has been helpful. I quite often get stuck, sometimes on dumb coding brainfarts, but other times because I have misunderstood something important. Especially with mixed models this has helped me understand why they’re interesting in the first place.\nStill I got a bit frustrated with having to re-run my data generating function when exploring parameters, so now that the function is good enough I threw it in a shiny app.\n\n\n\n\n\nThe app shows 20 “patients” over 8 weeks, modeled with random intercepts and slopes: lmer(y ~ week + (week|id), data). The intercept standard deviation of the intercepts is 1 (on average), so the fixed effect can be understood in relation to that.\nBuilding the app went very quickly, but my first version re-generated the data every time the user changed a parameter. I thought that would be less helpful as a teaching tool. The finished app instead has scaling parameters that affects the residuals and the random slopes. Remaking it meant making the app a bit more complicated, and since the app re-fits the lme4 model every time, the advantage wasn’t as clear as I had hoped. Oh well.\nOne small thing I learned was that I should probably be thinking about whether it makes sense to set the intercept at week 0 or not. (It’s visually very clear that the intercepts are more stable for changes in the data than the predicted random effects at week 8).\nI might improve the app further in the future (at least visually) if I find the time. For now it’s been a good learning experience that can hopefully be used as a minor teaching tool."
  },
  {
    "objectID": "posts/20231216_berkson/index.html",
    "href": "posts/20231216_berkson/index.html",
    "title": "Oh No! Berkson’s paradox in clinical theories",
    "section": "",
    "text": "Mike works as a clinical psychologist at a general psychiatric outpatient clinic. The people Mike sees have a wide range of issues, but all of them have real problems. Unemployment, broken relationships, loneliness, wasted potential, and usually a heavy mix of anxiety and hopelessness. In other words, mental illness, with all that comes with it. The word used among Mike’s colleagues is usually functional impairment, but he prefers to just think of misery.\nThe people Mike assess and treat are almost always referred from a primary care physician who believes this person needs specialized care. Sometimes the clinic gets a referral with a patient that isn’t really that impaired - some primary care doctors don’t know that much about psychiatry. In those cases Mike’s doctor colleagues send them somewhere else before he, or someone like him, even sees them.\nMike is a clever man, he’s good at spotting patterns. But he’s also careful. Humans in general are “pattern matchers”, and he knows this. We overfit our models. We see faces in clouds and rocks. After four heads in a row we think there’s bound to be a tail.\nLately, however, Mike is becoming more and more confident in a certain pattern. He’s even starting to become confident in his theory as to why the pattern arises. He has noticed a trade-off between two types of mental processes - excitingly two things that aren’t trivially related. Some people he meets have a bit of a loose grip on reality, more than others. They usually have “weird beliefs”, supernatural stuff, stuff about fate, alternative theories. Nothing psychotic, in his experience, but perhaps “schizotypal”. Other people are really Anxious. Interestingly the people that are more anxious seem to have a more realistic view of the world, and conversely people who have more weird1 spiritual beliefs about fate and the universe and how everything is connected seem to be less anxious.\nMaybe these beliefs and this style of thinking protects against anxiety? Maybe detaching yourself from the real world, giving yourself space to think more freely and openly guards you against the pain of reality. Since Mike is aware that the human mind often finds illusory correlations he instead chooses to gather data on his patients via other clinicians, blind to his theory. He explains his ideas of anxiousness and supernatural/weird beliefs and lets his colleagues get calibrated in scoring imaginary patient cases. Then they rate the patients they meet on these scales and hand the data to Mike.\nWhen the data has been collected the correlation is clearly visible and significant even at his relatively small sample! Heureka!\n\n\n\n\n\nIt really makes sense doesn’t it? Most of the people he meets at the clinic live lives full of real tangible problems. Taking a step away from reality is bound to protect emotionally against that, at least a bit. Meanwhile having a realistic outlook on your marginalized situation is bound to fill you with some anxiety. The mental processes that lead to “weird beliefs” do seem to come with their own problems, sometimes conflicts with others, or trouble with goal oriented behaviour. But this may be a small price to pay for some peace of mind."
  },
  {
    "objectID": "posts/20231216_berkson/index.html#lets-start-with-a-story.",
    "href": "posts/20231216_berkson/index.html#lets-start-with-a-story.",
    "title": "Oh No! Berkson’s paradox in clinical theories",
    "section": "",
    "text": "Mike works as a clinical psychologist at a general psychiatric outpatient clinic. The people Mike sees have a wide range of issues, but all of them have real problems. Unemployment, broken relationships, loneliness, wasted potential, and usually a heavy mix of anxiety and hopelessness. In other words, mental illness, with all that comes with it. The word used among Mike’s colleagues is usually functional impairment, but he prefers to just think of misery.\nThe people Mike assess and treat are almost always referred from a primary care physician who believes this person needs specialized care. Sometimes the clinic gets a referral with a patient that isn’t really that impaired - some primary care doctors don’t know that much about psychiatry. In those cases Mike’s doctor colleagues send them somewhere else before he, or someone like him, even sees them.\nMike is a clever man, he’s good at spotting patterns. But he’s also careful. Humans in general are “pattern matchers”, and he knows this. We overfit our models. We see faces in clouds and rocks. After four heads in a row we think there’s bound to be a tail.\nLately, however, Mike is becoming more and more confident in a certain pattern. He’s even starting to become confident in his theory as to why the pattern arises. He has noticed a trade-off between two types of mental processes - excitingly two things that aren’t trivially related. Some people he meets have a bit of a loose grip on reality, more than others. They usually have “weird beliefs”, supernatural stuff, stuff about fate, alternative theories. Nothing psychotic, in his experience, but perhaps “schizotypal”. Other people are really Anxious. Interestingly the people that are more anxious seem to have a more realistic view of the world, and conversely people who have more weird1 spiritual beliefs about fate and the universe and how everything is connected seem to be less anxious.\nMaybe these beliefs and this style of thinking protects against anxiety? Maybe detaching yourself from the real world, giving yourself space to think more freely and openly guards you against the pain of reality. Since Mike is aware that the human mind often finds illusory correlations he instead chooses to gather data on his patients via other clinicians, blind to his theory. He explains his ideas of anxiousness and supernatural/weird beliefs and lets his colleagues get calibrated in scoring imaginary patient cases. Then they rate the patients they meet on these scales and hand the data to Mike.\nWhen the data has been collected the correlation is clearly visible and significant even at his relatively small sample! Heureka!\n\n\n\n\n\nIt really makes sense doesn’t it? Most of the people he meets at the clinic live lives full of real tangible problems. Taking a step away from reality is bound to protect emotionally against that, at least a bit. Meanwhile having a realistic outlook on your marginalized situation is bound to fill you with some anxiety. The mental processes that lead to “weird beliefs” do seem to come with their own problems, sometimes conflicts with others, or trouble with goal oriented behaviour. But this may be a small price to pay for some peace of mind."
  },
  {
    "objectID": "posts/20231216_berkson/index.html#oh-no",
    "href": "posts/20231216_berkson/index.html#oh-no",
    "title": "Oh No! Berkson’s paradox in clinical theories",
    "section": "Oh no…",
    "text": "Oh no…\nYou know we were getting to this. His selection of patients is conditioned. In this example our psychologist isn’t falling victim to some “illusory correlation”. He’s not seeing a relationship where none exist. In his data-set the correlation really is quite strong. What he misses is why it exists.\nLet’s say “weird beliefs” are bad for your level of “functional impairment” in a simple linear way. The more weird beliefs the more impairment.\nLet’s assume that anxiety is also bad for your functioning (it is). Adding these together we have impairment = anxiety + weird beliefs. So with one unit of anxiety and five units of “weird beliefs” you have six units of impairment.\nWe can draw it like a gradient to make it really clear. The x and y axes represent weird beliefs and anxiety levels. Redder colour means more functional impairment.\n\n\n\n\n\nAt Mikes clinic, people who are not impaired enough don’t need to seek professional help at a clinic, or are sent elsewhere by whoever screens the referrals. Additionally, people who are in an acute crisis may be sent to even more specialized clinics, even inpatient care.\nThe effect is that Mike’s patients exist in a limited range of functional impairment. Meanwhile if we were to look at a larger random sample of the population we see that there’s no relationship between anxiety and weird beliefs.\n\n\n\nI remind you this is simulated data.\n\n\nI remind you that this is simulated data. Indeed, the paradoxiest2 part of Berkson’s paradox is that it can even reverse the direction of a correlation. Generally in psychiatry negative things are correlated, both for psychological reasons (for example negative emotions causing sleep issues) and for practical reasons (for example a depression having a negative impact on your financial security) and perhaps even for biological reason (there are positive genetic correlations between many different disorders). This has led to some researchers suggesting there’s a general factor of psychopathology3. But even in those conditions, where there’s a positive correlation between the traits we look at, we can get a reversed correlation given the right selection effect:"
  },
  {
    "objectID": "posts/20231216_berkson/index.html#is-this-common-does-this-happen",
    "href": "posts/20231216_berkson/index.html#is-this-common-does-this-happen",
    "title": "Oh No! Berkson’s paradox in clinical theories",
    "section": "Is this common? Does this happen?",
    "text": "Is this common? Does this happen?\nIn my experience the culture in clinical psychology can be a bit contradictory. On one hand our education teaches us to be very sceptical. We’re taught to be careful not to make scientifically unwarranted claims. We’re taught to be suspicious of simplifications. Aware of unknown confounders. All that jazz. At the same time psychology often reveres clinical intuition. From a scientific point of view we often don’t know more than the fact that a bunch of questionnaires are internally consistent and correlate with each other in this and that way. Meanwhile in the land of clinical experience we can imagine ourselves knowing with high precision exactly why someone has a symptom, given their personality, their defences, their context (depending on who you ask). I don’t want to come of as too negative about that. Maybe people really are too multifaceted for “reductionism”? At the very least I think people’s psychological intuitions are often based on a sound understanding of how other humans work. I’m a human living among humans, so that intuition should be based off good data at least.\nThe field of judgement and decision-making would have a lot to criticize about the above statement. They would point to our tendency to form bad intuitions in contexts where we can’t reasonably expect to form them, i.e. where feedback is delayed and noisy. Working as a therapist, unfortunately, is one such context. But in the story above that isn’t the problem. Even if you’re careful and well calibrated, you can’t make up for biased data. “Garbage in, garbage out” as we often say when it comes to meta-analyses and machine learning.\nI don’t mean to argue that this particular mistake is necessarily super common. I really don’t know. But no doubt it’s easy for clinicians to make their own theories, see potential trade-offs or see how patients cluster into types. Worry as a defence against sadness. Externalizing symptoms as a way to not feel anxiety. I get that these theories have supporting literature and arguments that go deeper than simple observations, but situations we often find ourselves in as clinicians may make these theories feel convincing for the wrong reasons.\nAnd I think this potential pattern is important to be aware of in a bunch of different contexts. For example you could imagine teachers at some sort of prestigious private school seeing a trade-off between being good at math and good at writing, in a way that makes them buy into questionable research about learning styles. Instead the pattern is because students get in to the school based on a sum of their grades, some more language-related, some more math-related. The same teachers could also plausibly spot false negative relationships between learning fast and being industrious (“they’re smart so they don’t have to learn any study-strategies”). Life is full of unrepresentative samples.\nBasically you always need to think about where your sample comes from, and if the reason your subjects get sampled could plausibly be related to the variables you’re looking at you need to be careful what you conclude. And it’s worthwhile to remember that even if you’re well calibrated and careful, there’s a good reason that scientific research has a higher status than your individual observations."
  },
  {
    "objectID": "posts/20231216_berkson/index.html#footnotes",
    "href": "posts/20231216_berkson/index.html#footnotes",
    "title": "Oh No! Berkson’s paradox in clinical theories",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWeird according to Mike, that is.↩︎\nYes.↩︎\nI have not looked into it and though I prima facie trust the finding I tend to have a negative reaction to these efforts to squish a bunch of things together and say they’re all part of the same underlying thing.↩︎"
  },
  {
    "objectID": "notes/Stats2_seminar_3.html",
    "href": "notes/Stats2_seminar_3.html",
    "title": "Stats2_seminar_3",
    "section": "",
    "text": "For the model definition below, simulate observed y values from the prior (not the posterior). \\[\n\\begin{align}\ny_i &\\sim Normal(µ,σ) \\\\\nµ &\\sim Normal(0,10) \\\\\nσ &\\sim Exponential(1)\n\\end{align}\n\\]\n\n#I think this simulates from prior.\nn &lt;- 100\nsigma &lt;- rexp(n, 1)\nmu &lt;- rnorm(n, mean = 0, sd = 10)\ny &lt;- rnorm(n, mu, sigma) #we're only plugging in values drawn from our prior.\n\ndens(y)"
  },
  {
    "objectID": "notes/Stats2_seminar_3.html#m1",
    "href": "notes/Stats2_seminar_3.html#m1",
    "title": "Stats2_seminar_3",
    "section": "",
    "text": "For the model definition below, simulate observed y values from the prior (not the posterior). \\[\n\\begin{align}\ny_i &\\sim Normal(µ,σ) \\\\\nµ &\\sim Normal(0,10) \\\\\nσ &\\sim Exponential(1)\n\\end{align}\n\\]\n\n#I think this simulates from prior.\nn &lt;- 100\nsigma &lt;- rexp(n, 1)\nmu &lt;- rnorm(n, mean = 0, sd = 10)\ny &lt;- rnorm(n, mu, sigma) #we're only plugging in values drawn from our prior.\n\ndens(y)"
  },
  {
    "objectID": "notes/Stats2_seminar_3.html#m2.-translate-the-model-just-above-into-a-quap-formula.",
    "href": "notes/Stats2_seminar_3.html#m2.-translate-the-model-just-above-into-a-quap-formula.",
    "title": "Stats2_seminar_3",
    "section": "4M2. Translate the model just above into a quap formula.",
    "text": "4M2. Translate the model just above into a quap formula.\n\nflist &lt;- alist(\n  y ~ dnorm(mu, sigma),\n  mu ~ dnorm(0, 10),\n  sigma ~ dexp(1)\n)"
  },
  {
    "objectID": "notes/Stats2_seminar_3.html#m3.-translate-the-quap-model-formula-below-into-a-mathematical-model-definition.",
    "href": "notes/Stats2_seminar_3.html#m3.-translate-the-quap-model-formula-below-into-a-mathematical-model-definition.",
    "title": "Stats2_seminar_3",
    "section": "4M3. Translate the quap model formula below into a mathematical model definition.",
    "text": "4M3. Translate the quap model formula below into a mathematical model definition.\n\\[\n\\begin{align}\ny_i &\\sim Normal(\\mu_i,\\sigma) \\\\\n\\mu_i &= a + b*x_i \\\\\na &\\sim Normal(0,10) \\\\\nb &\\sim Uniform(0,1) \\\\\n\\sigma &\\sim Exponential(1)\n\\end{align}\n\\]\nflist &lt;- alist(\n y ~ dnorm( mu , sigma ),\n mu &lt;- a + b*x,\n a ~ dnorm( 0 , 10 ),\n b ~ dunif( 0 , 1 ),\n sigma ~ dexp( 1 )\n )"
  },
  {
    "objectID": "notes/Stats2_seminar_3.html#h1.",
    "href": "notes/Stats2_seminar_3.html#h1.",
    "title": "Stats2_seminar_3",
    "section": "4H1.",
    "text": "4H1.\nThe weights listed below were recorded in the!Kung census, but heights were not recorded for these individuals. Provide predicted heights and 89% intervals for each of these individuals. That is, fill in the table below, using model-based predictions.\n\nt &lt;- data.frame(individual = 1:5, weights = c(46.95,43.72,64.78,32.59,54.63), expected_height = rep(NA, times = 5), low_89_PI = rep(NA, times = 5), high_89_PI = rep(NA, times = 5),xbar = rep(NA, times = 5))\nt\n\n  individual weights expected_height low_89_PI high_89_PI xbar\n1          1   46.95              NA        NA         NA   NA\n2          2   43.72              NA        NA         NA   NA\n3          3   64.78              NA        NA         NA   NA\n4          4   32.59              NA        NA         NA   NA\n5          5   54.63              NA        NA         NA   NA\n\ndata(Howell1)\nd &lt;- Howell1\nd &lt;- d[d$age &gt;= 18,]\nxbar &lt;- mean(d$weight)\n\nmdl &lt;- quap(\n  alist(\n  height ~ dnorm(mu, sigma),\n  mu &lt;- a + b * (weight - xbar), #where is xbar defined? Must be part of quap built in?\n  a ~ dnorm( 178, 20), \n  b ~ dlnorm(0, 1),\n  sigma ~ dunif(0,50)\n), data = d\n)\nprecis(mdl)\n\n             mean         sd        5.5%       94.5%\na     154.6013573 0.27030758 154.1693536 155.0333610\nb       0.9032811 0.04192362   0.8362791   0.9702831\nsigma   5.0718795 0.19115465   4.7663774   5.3773815\n\n#post &lt;- extract.samples( mdl ) \n#data &lt;- c(46.95,43.72,64.78,32.59,54.63)\n#for (i in data) {\n # y &lt;- rnorm( 1e5 , post$a + post$b*i , post$sigma ) \n  #print(mean(y))\n  #print(HPDI(y,prob=0.89))\n#}\n\npost &lt;- extract.samples(mdl, n = 1000)\n\nt$expected_height &lt;- apply(sim(mdl, data=list(weight=t$weights)),2, mean)\n#I guess we could also do it by formula.\nt$low_89_PI &lt;- apply(sim(mdl, data=list(weight=t$weights)),2, PI)[1,]\nt$high_89_PI &lt;- apply(sim(mdl, data=list(weight=t$weights)),2, PI)[2,]\n\nt\n\n  individual weights expected_height low_89_PI high_89_PI xbar\n1          1   46.95        156.2150  147.9285   164.0839   NA\n2          2   43.72        153.2816  145.8356   162.1845   NA\n3          3   64.78        172.6121  164.2527   180.4767   NA\n4          4   32.59        143.3276  135.4016   151.6196   NA\n5          5   54.63        162.8802  155.3697   171.1801   NA\n\n\nMats suggest we should do this manually too: i.e. don’t use link() or sim()"
  },
  {
    "objectID": "notes/Stats2_seminar_3.html#h2.",
    "href": "notes/Stats2_seminar_3.html#h2.",
    "title": "Stats2_seminar_3",
    "section": "4H2.",
    "text": "4H2.\nSelect out all the rows in the Howell1 data with ages below 18 years of age. If you do it right, you should end up with a new data frame with 192 rows in it.\n\nFit a linear regression to these data, using quap. Present and interpret the estimates. For every 10 units of increase in weight, how much taller does the model predict a child gets? Answer: 27.18372\nPlot the raw data, with height on the vertical axis and weight on the horizontal axis. Super impose the MAP regression line and 89% interval for the mean. Also superimpose the 89% interval for predicted heights.\nWhat aspects of the model fit concern you? Describe the kinds of assumptions you would change, if any, to improve the model. You don’t have to write any new code. Just explain what the model appears to be doing a bad job of, and what you hypothesize would be a better model.\n\n\n#0.\ndata(Howell1)\nd &lt;- Howell1\nd &lt;- d[d$age &lt; 18,]\n#\nd &lt;- d[sample(1:nrow(d),size = 10),]\nxbar &lt;- mean(d$weight)\n\n#1.\n#challanging to chose priors for kids 0-18. I'm going with this. A tall 18 is 190cm. So half that.\n#and I don't know what I'm talking about, so wide SD.\n#I don't know enough to mess with the other priors but something positive so I'm keeping the dlnorm(0,10)\n#and I don't know how close the relationship will be so model sigma between 0 and 10 seems fine\nmdl2 &lt;- quap(\n  alist(\n  height ~ dnorm(mu, sigma),\n  mu &lt;- a + b * (weight - xbar),\n  a ~ dnorm( 95, 20), \n  b ~ dlnorm(0, 1),\n  sigma ~ dunif(0,10)\n), data = d\n)\nprecis(mdl2)\n\n            mean        sd       5.5%      94.5%\na     112.076281 1.5129022 109.658371 114.494191\nb       3.393750 0.2158979   3.048704   3.738797\nsigma   4.795944 1.0749938   3.077896   6.513992\n\nprecis(mdl2)[2,1]*10\n\n[1] 33.9375\n\n#2. plot MAP line and PI89\npost &lt;- extract.samples(mdl2)\nweight.seq &lt;- c(4:45)\nmu &lt;- link(mdl2, data=data.frame(weight=weight.seq))\nmu.PI &lt;- apply(mu,2,PI)\nsim.heights &lt;- sim(mdl2, data=data.frame(weight=weight.seq), post = post)\nheight.PI &lt;- apply(sim.heights, 2, PI, prob = 0.97)\nplot(height ~ weight, data = d, ylim = c(50,180)); curve(mean(post$a) + mean(post$b)*(x-xbar),add = TRUE); shade(mu.PI, weight.seq); shade(height.PI, weight.seq)\n\n\n\n\n\n\n\n#My attempt at writing an own sim() function\n#post are paired parameters drawn from the model.\nweight.seq.random &lt;- sample(weight.seq, size = 1000, replace = TRUE)\npredicted_points &lt;- rnorm(1000, mean = post$a + post$b*(weight.seq.random - xbar), sd = post$sigma)\nplot(weight.seq.random, predicted_points) #don't know hot to get a PI here\n\n\n\n\n\n\n\n#seems the errors should propagate but it's not easy to see visually\n\n#second attempt. my own sim for prediction intervals\nn &lt;- 1000\ns &lt;- c(5:45)\npost &lt;- rethinking::extract.samples(mdl2, n = n)\nd2f &lt;- matrix(nrow = n, ncol = 0)\n  for(i in s){\n    d2f &lt;- cbind(d2f,rnorm(n, mean = post$a + post$b*(s[1+i-min(s)] - xbar), sd = post$sigma))\n  } #super dumb for loop man...\nmy.sim.PI &lt;- apply(d2f,2,PI)\nplot(my.sim.PI[2,] - my.sim.PI[1,])\n\n\n\n\n\n\n\n#here we can see what is less clear with the naked eye\n#plot(height.PI[2,] - height.PI[1,]) \n#plot(mu.PI[2,] - mu.PI[1,]) \n\nAnswer 3: I’d be concerned about how far off the MAP-line is from the data-points, especially in the middle. The relationship looks non-linear."
  },
  {
    "objectID": "notes/Stats2_seminar_3.html#h3.",
    "href": "notes/Stats2_seminar_3.html#h3.",
    "title": "Stats2_seminar_3",
    "section": "4H3.",
    "text": "4H3.\nSuppose a colleague of yours, who works on allometry, glances at the practice problems just above. Your colleague exclaims, “That’s silly. Everyone knows that it’s only the logarithm of body weight that scales with height!” Let’s take your colleague’s advice and see what happens.\n\nModel the relationship between height(cm)and the natural logarithm of weight(log-kg). Use the entire Howell1 data frame, all 544 rows, adults and non-adults. Fit this model, using quadratic approximation: \\[\n\\begin{align}\nh_i &∼ Normal(µ_i,σ) \\\\\nµ_i &= α+βlog(w_i) \\\\\nα &∼Normal(178,20) \\\\\nβ &∼Log−Normal(0,1) \\\\\nσ &∼Uniform(0,50)\n\\end{align}\n\\] where \\(h_i\\) is the height of individual i and wi is the weight (in kg) of individual i. The function for computing a natural log in R is just log. Can you interpret the resulting estimates?\nBegin with this plot:\n\n\ndata(\"Howell1\")\nd &lt;- Howell1\n\nmdl3 &lt;- quap(\n  alist(\n  height ~ dnorm(mu, sigma),\n  mu &lt;- a + b * log(weight),#note that we're not doing \"- mean(weight)\" now, it wasn't in the formula for some reason.\n  a ~ dnorm( 178, 20), \n  b ~ dlnorm(0, 1),\n  sigma ~ dunif(0,10)\n  ), data = d)\nprecis(mdl3)\n\n            mean        sd       5.5%     94.5%\na     -22.871048 1.3343316 -25.003568 -20.73853\nb      46.816794 0.3823358  46.205748  47.42784\nsigma   5.137211 0.1558943   4.888062   5.38636\n\npost &lt;- extract.samples(mdl3) #using samples from the posterior (1)\nweight.seq &lt;- c(4:63)\nmu &lt;- link(mdl3, data=data.frame(weight=weight.seq))\nmu.mean &lt;- apply(mu,2,mean)\nmu.PI &lt;- apply(mu,2,PI, prob = 0.97)\nsim.heights &lt;- sim(mdl3, data=data.frame(weight=weight.seq))\nheight.PI &lt;- apply(sim.heights, 2, PI, prob = 0.97)\n\nplot(height ~ weight , data=Howell1 ,\n col=col.alpha(rangi2,0.4), main = \"97% PI for mu\", ylim = c(0,190) );  lines(weight.seq,mu.mean); shade(mu.PI, weight.seq)\n\n\n\n\n\n\n\n#but it's actually cooler to do it with curve actually...\nplot(height ~ weight , data=Howell1 , col=col.alpha(rangi2,0.4), main = \"97% PI for predicted height\"  );  curve(mean(post$a) + mean(post$b) * log(x), add = TRUE);  shade(height.PI, weight.seq)\n\n\n\n\n\n\n\n\nThen use samples from the quadratic approximate posterior of the model in (a) to superimpose on the plot: (1) the predicted mean height as a function of weight, (2) the 97% interval for the mean, and (3) the 97% interval for predicted heights."
  },
  {
    "objectID": "notes/Stats2IndividualAssignment.html",
    "href": "notes/Stats2IndividualAssignment.html",
    "title": "Stats2IndividualAssignment",
    "section": "",
    "text": "My data will be self-rated GAD-7 scores from a clinical trial. Patients receive one out of two possible active treatments over ten weeks. I want to use multi-level modeling to model the effect of treatment over time, and compare that between treatments.\nThe formula is presented below for 20 patients (\\(j\\)). My outcome \\(y\\) is GAD-7 score. \\(time\\) will be the week the measurement comes from, ranging from 0 to 9, so that the intercepts \\(\\alpha\\) represent model estimations at start of treatment.\n\\[\n\\begin{align*}\ny_i \\sim Normal(\\mu_i,\\sigma) \\\\\n\\mu_{i,j} = \\alpha_{patient[i]} \\ + \\ \\beta_{patient[i]}*time_i \\\\\n\\alpha_j \\sim Normal(\\bar{\\alpha},\\sigma_a) \\ \\ ,for \\ j = 1,2,...20 \\\\\n\\bar{\\alpha} \\sim Normal(0,1) \\\\\n\\sigma_a \\sim Exponential(1) \\\\\n\\beta_j \\sim Normal(\\bar{\\beta},\\sigma_b) \\ \\ ,for \\ j = 1,2,...20 \\\\\n\\bar{\\beta} \\sim Normal(0,1) \\\\\n\\sigma_b \\sim Exponential(1) \\\\\n\\sigma \\sim Exponential(1)\n\\end{align*}\n\\]\nI have not figured out how to specify a group effect yet but one cheap solution is to just fit two models and compare. In this case that solution is probably adequate since I don’t want the estimate for group A to affect the estimate from group B anyway. However, in time I want to look at contrasts between competing mediators and then it may be better to include group as a factor.\nI have a function that generates data in long format from a previous project. Since the data simulates intercepts ~ Normal(0,1), it can be read as if I were to standardize the scores at week 0. With this standardization the avg_slope argument times the length of the treatment nweeks corresponds to the standardized effect size for growth models following Feingold (2007).\n\n#function that simulates data\nlong.data.sim &lt;- function(n = 30, avg_slope = 0.1, slope_intercept_correlation = -0.15, slope_sigma = 0.1, nweeks = 10, error = 0.3){\n  intercepts &lt;- rnorm(n, m = 0, sd = 1)\n  slopes &lt;- avg_slope + (slope_intercept_correlation*intercepts + rnorm(n,m=0,sd=sqrt(1-slope_intercept_correlation^2)) )*slope_sigma\n  weeks &lt;- 0:(nweeks-1)\n  df &lt;- data.frame()\n  for(i in 1:n){\n    err &lt;- rnorm(nweeks, m = 0, sd = error) #will equal residual std.dev. = sigma(mdl)\n    df &lt;- rbind.data.frame(df,data.frame(id=rep(i, times = nweeks),y=intercepts[i]+slopes[i]*weeks+err,weeks))\n  }\n  return(df)\n}\n\n#generate data\nd &lt;- long.data.sim(n = 20, avg_slope = 0.2, slope_intercept_correlation = -0.15, slope_sigma = 0.1, nweeks = 10, error = 0.3)\n\n#Let's also see if it runs if some patients have missing data some weeks (it will)\nd &lt;- d[-sample(1:nrow(d), 15, replace = FALSE),]\n\nI then built the models step by step increasing complexity.\n\n#single level model (with quadratic aproximation)\nmdl_non &lt;- quap(alist(\n  y ~ dnorm(mu,sigma),\n  mu &lt;- a + b*weeks,\n  a ~ dnorm(0,1),\n  b ~ dnorm(0,0.3),\n  sigma ~ dexp(1)\n), data = d)\nprecis(mdl_non)\n\n            mean         sd      5.5%     94.5%\na     0.07157867 0.14607351 -0.161875 0.3050323\nb     0.21349254 0.02770973  0.169207 0.2577780\nsigma 1.09237163 0.05654070  1.002009 1.1827346\n\n#multi-level model, random intercepts.\nmdl_random_intercept &lt;- ulam(alist(\n  y ~ dnorm(mu,sigma),\n  mu &lt;- a[id] + b*weeks,\n  a[id] ~ dnorm(a_bar,sigma_a),\n  a_bar ~ dnorm(0,1),\n  sigma_a ~ dexp(1),\n  b ~ dnorm(0,0.3),\n  sigma ~ dexp(1)\n), data = d, chains = 4, cores = 4)\n\nIn file included from stan/lib/stan_math/stan/math/prim/prob/von_mises_lccdf.hpp:5,\n                 from stan/lib/stan_math/stan/math/prim/prob/von_mises_ccdf_log.hpp:4,\n                 from stan/lib/stan_math/stan/math/prim/prob.hpp:356,\n                 from stan/lib/stan_math/stan/math/prim.hpp:16,\n                 from stan/lib/stan_math/stan/math/rev.hpp:14,\n                 from stan/lib/stan_math/stan/math.hpp:19,\n                 from stan/src/stan/model/model_header.hpp:4,\n                 from C:/Users/vilgo/AppData/Local/Temp/Rtmpy0zvj8/model-28bc500540dc.hpp:2:\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp: In function 'stan::return_type_t&lt;T_x, T_sigma, T_l&gt; stan::math::von_mises_cdf(const T_x&, const T_mu&, const T_k&)':\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: '-Wmisleading-indentation' is disabled from this point onwards, since column-tracking was disabled due to the size of the code/headers\n  194 |       if (cdf_n &lt; 0.0)\n      | \n\n\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: adding '-flarge-source-files' will allow for more column-tracking support, at the expense of compilation time and memory\n\n\nRunning MCMC with 4 parallel chains, with 1 thread(s) per chain...\n\nChain 1 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 1 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 1 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 1 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 1 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 2 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 2 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 3 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 3 Iteration: 100 / 1000 [ 10%]  (Warmup) \n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: normal_lpdf: Scale parameter is 0, but must be positive! (in 'C:/Users/vilgo/AppData/Local/Temp/Rtmpy0zvj8/model-28bc500540dc.stan', line 19, column 4 to column 34)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 4 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 1 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 1 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 1 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 1 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 1 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 1 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 1 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 1 finished in 0.2 seconds.\nChain 2 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 2 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 2 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 2 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 2 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 2 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 2 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 2 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 2 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 2 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 2 finished in 0.2 seconds.\nChain 3 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 3 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 3 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 3 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 3 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 3 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 3 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 3 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 3 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 3 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 3 finished in 0.2 seconds.\nChain 4 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 4 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 4 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 4 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 4 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 4 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 4 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 4 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 4 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 4 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 4 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 4 finished in 0.2 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.2 seconds.\nTotal execution time: 0.4 seconds.\n\nprecis(mdl_random_intercept, depth = 2)  #seems to run fine with missing rows.\n\n               mean         sd        5.5%       94.5%      rhat ess_bulk\na[1]    -0.02520281 0.14921578 -0.26603355  0.21877512 1.0010833 2202.248\na[2]    -1.10287172 0.13450625 -1.31535830 -0.89130662 1.0038579 3926.797\na[3]    -0.73813835 0.13973373 -0.95917408 -0.51288988 1.0032422 3060.956\na[4]    -0.65834556 0.13519752 -0.86900740 -0.44759664 1.0067085 3126.022\na[5]    -1.15892221 0.13544008 -1.37606300 -0.94658577 1.0046948 3002.016\na[6]     0.75545638 0.13922541  0.53084204  0.97754093 0.9996994 2850.140\na[7]     0.06189972 0.13673783 -0.15652840  0.28013837 1.0027208 2742.055\na[8]    -0.66021047 0.14058414 -0.87995546 -0.43961912 0.9990020 2557.988\na[9]     0.77055111 0.13624650  0.55998342  0.99166465 1.0018397 3225.269\na[10]    2.70448036 0.14066007  2.47965205  2.93030730 1.0014380 2975.519\na[11]    2.28466059 0.14146399  2.04461900  2.50176265 1.0026248 3201.186\na[12]    0.40256383 0.13609763  0.18062447  0.61482962 1.0008079 2271.321\na[13]   -0.76115211 0.13820998 -0.98172314 -0.52767381 1.0057816 3545.426\na[14]   -1.07380916 0.13974519 -1.30386230 -0.84637135 0.9989985 2880.575\na[15]    0.51210060 0.13764298  0.29422378  0.72660891 1.0080993 2904.486\na[16]   -0.50313231 0.14101022 -0.73081864 -0.27814424 1.0016774 3045.534\na[17]    0.12823507 0.13916734 -0.09294399  0.34924909 1.0001611 2645.378\na[18]    0.35461277 0.13950420  0.13398911  0.57701194 1.0032782 3072.307\na[19]   -0.15271404 0.14287922 -0.37555582  0.07263233 1.0015358 3385.546\na[20]    0.57261419 0.14237918  0.34603018  0.79546032 1.0028629 3044.263\na_bar    0.08987243 0.24292037 -0.27879912  0.48183952 1.0004217 3048.228\nsigma_a  1.08438611 0.18586145  0.83071493  1.42053585 1.0031046 3442.261\nb        0.21272936 0.01015007  0.19638681  0.22871962 1.0000291 1129.199\nsigma    0.39928193 0.02262948  0.36390590  0.43656543 1.0156441 3378.770\n\nprecis(mdl_random_intercept) #most relevant parameters\n\n20 vector or matrix parameters hidden. Use depth=2 to show them.\n\n\n              mean         sd       5.5%     94.5%     rhat ess_bulk\na_bar   0.08987243 0.24292037 -0.2787991 0.4818395 1.000422 3048.228\nsigma_a 1.08438611 0.18586145  0.8307149 1.4205358 1.003105 3442.261\nb       0.21272936 0.01015007  0.1963868 0.2287196 1.000029 1129.199\nsigma   0.39928193 0.02262948  0.3639059 0.4365654 1.015644 3378.770\n\n#multi-level model, random intercepts and slopes.\nmdl_random &lt;- ulam(alist(\n  y ~ dnorm(mu,sigma),\n  mu &lt;- a[id] + b[id]*weeks,\n  a[id] ~ dnorm(a_bar,sigma_a),\n  a_bar ~ dnorm(0,1),\n  sigma_a ~ dexp(1),\n  b[id] ~ dnorm(b_bar,sigma_b),\n  b_bar ~ dnorm(0,1),\n  sigma_b ~ dexp(1),\n  sigma ~ dexp(1)\n), data = d, chains = 4, cores = 4)\n\nIn file included from stan/lib/stan_math/stan/math/prim/prob/von_mises_lccdf.hpp:5,\n                 from stan/lib/stan_math/stan/math/prim/prob/von_mises_ccdf_log.hpp:4,\n                 from stan/lib/stan_math/stan/math/prim/prob.hpp:356,\n                 from stan/lib/stan_math/stan/math/prim.hpp:16,\n                 from stan/lib/stan_math/stan/math/rev.hpp:14,\n                 from stan/lib/stan_math/stan/math.hpp:19,\n                 from stan/src/stan/model/model_header.hpp:4,\n                 from C:/Users/vilgo/AppData/Local/Temp/Rtmpy0zvj8/model-28bc22401029.hpp:2:\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp: In function 'stan::return_type_t&lt;T_x, T_sigma, T_l&gt; stan::math::von_mises_cdf(const T_x&, const T_mu&, const T_k&)':\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: '-Wmisleading-indentation' is disabled from this point onwards, since column-tracking was disabled due to the size of the code/headers\n  194 |       if (cdf_n &lt; 0.0)\n      | \n\n\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: adding '-flarge-source-files' will allow for more column-tracking support, at the expense of compilation time and memory\n\n\nRunning MCMC with 4 parallel chains, with 1 thread(s) per chain...\n\nChain 1 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 1 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 1 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 2 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 2 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 3 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 4 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 4 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 1 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 1 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 1 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 1 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 1 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 1 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 2 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 2 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 2 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 2 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 2 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 2 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 2 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 3 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 3 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 3 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 3 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 3 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 3 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 3 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 4 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 4 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 4 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 4 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 4 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 4 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 1 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 1 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 2 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 2 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 2 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 3 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 3 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 4 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 4 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 4 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 1 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 1 finished in 0.5 seconds.\nChain 2 finished in 0.4 seconds.\nChain 3 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 3 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 4 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 3 finished in 0.5 seconds.\nChain 4 finished in 0.4 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.5 seconds.\nTotal execution time: 0.7 seconds.\n\nprecis(mdl_random, depth = 2)\n\n               mean         sd         5.5%        94.5%      rhat ess_bulk\na[1]     0.13763362 0.20401689 -0.192673100  0.453791880 1.0016809 2400.392\na[2]    -1.16918868 0.18464673 -1.469531450 -0.868549215 1.0011623 2636.317\na[3]    -0.63657915 0.18564790 -0.930696490 -0.333380320 1.0004997 3304.281\na[4]    -0.94837438 0.18188301 -1.238314950 -0.646965225 1.0022509 2728.661\na[5]    -0.70659566 0.18588272 -1.013064950 -0.407646580 1.0002304 2441.167\na[6]     0.49739330 0.19678131  0.186874515  0.809785695 1.0002165 2855.974\na[7]     0.02333268 0.18673091 -0.270914670  0.329632750 0.9999923 2491.078\na[8]    -0.76884871 0.18410184 -1.058877250 -0.469463195 0.9991601 2677.698\na[9]     1.05371714 0.17832851  0.763279650  1.342631350 1.0007241 2479.162\na[10]    2.17960233 0.18870650  1.863835250  2.471498400 1.0013048 2302.449\na[11]    1.73940472 0.20379633  1.414371750  2.071898100 1.0003578 2338.700\na[12]    1.00962617 0.18431233  0.717254315  1.308446850 1.0026771 2955.093\na[13]   -0.78732622 0.18317331 -1.084791550 -0.491850020 1.0021275 2132.853\na[14]   -1.28735082 0.19064200 -1.605387700 -0.987822590 1.0004825 3185.551\na[15]    0.30584927 0.18758471  0.013459617  0.595883325 0.9999719 2612.923\na[16]   -0.28621499 0.18199055 -0.575899155 -0.001304038 0.9993551 2638.725\na[17]    0.09551850 0.18117889 -0.200388885  0.389704220 1.0046455 2603.652\na[18]    0.29510087 0.18552802  0.007532375  0.586815365 0.9999263 2773.765\na[19]    0.13720548 0.18507220 -0.159640890  0.428200625 1.0003737 2454.380\na[20]    0.75101203 0.22185446  0.408661745  1.095551350 1.0011165 3136.567\na_bar    0.07554967 0.22148420 -0.273502570  0.411122280 1.0002413 3887.325\nsigma_a  0.99857044 0.17603429  0.760254025  1.302518550 1.0029860 3147.878\nb[1]     0.17555283 0.03698009  0.116280280  0.235469125 1.0035438 2290.194\nb[2]     0.22601328 0.03282678  0.173460355  0.277233385 0.9996436 2763.655\nb[3]     0.18553127 0.03795384  0.126349360  0.245725895 1.0004982 3224.542\nb[4]     0.27818150 0.03423851  0.225094610  0.332742825 1.0025901 2709.904\nb[5]     0.11093633 0.03423799  0.057080752  0.165571915 1.0013364 2391.697\nb[6]     0.26696488 0.03458176  0.210526000  0.323312385 1.0008511 2661.686\nb[7]     0.22119065 0.03415080  0.166000185  0.275100210 0.9989249 2554.785\nb[8]     0.23681654 0.03334569  0.183751640  0.289308810 0.9989245 2609.819\nb[9]     0.14993050 0.03249067  0.098182279  0.202939615 1.0005990 2371.687\nb[10]    0.33164854 0.03448216  0.276640175  0.387383035 1.0000744 2236.169\nb[11]    0.34819513 0.04018706  0.283062535  0.411618505 1.0029311 2295.675\nb[12]    0.07602954 0.03424976  0.019293557  0.130471585 1.0029694 2609.973\nb[13]    0.21824780 0.03672063  0.161055030  0.276723140 1.0066972 2158.237\nb[14]    0.25820921 0.03389141  0.204913350  0.312336550 1.0028233 3318.883\nb[15]    0.26031897 0.03374351  0.207539000  0.315102325 0.9993097 2533.134\nb[16]    0.15981830 0.03510816  0.102849735  0.214442950 0.9994200 2727.007\nb[17]    0.22076857 0.03376817  0.164851810  0.274731880 1.0042257 2633.818\nb[18]    0.22793371 0.03656095  0.168902895  0.284702780 1.0008931 2824.816\nb[19]    0.14635664 0.03339399  0.091356924  0.199918775 0.9998916 2376.836\nb[20]    0.17686581 0.03819172  0.116701955  0.237775075 1.0006119 2964.347\nb_bar    0.21410393 0.02131508  0.180765165  0.248140580 1.0003901 2671.844\nsigma_b  0.08171234 0.01789594  0.057558535  0.110501015 1.0014209 2002.953\nsigma    0.33128625 0.01939546  0.302480820  0.363382790 1.0004200 2698.237\n\nprecis(mdl_random)\n\n40 vector or matrix parameters hidden. Use depth=2 to show them.\n\n\n              mean         sd        5.5%     94.5%     rhat ess_bulk\na_bar   0.07554967 0.22148420 -0.27350257 0.4111223 1.000241 3887.325\nsigma_a 0.99857044 0.17603429  0.76025402 1.3025186 1.002986 3147.878\nb_bar   0.21410393 0.02131508  0.18076517 0.2481406 1.000390 2671.844\nsigma_b 0.08171234 0.01789594  0.05755853 0.1105010 1.001421 2002.953\nsigma   0.33128625 0.01939546  0.30248082 0.3633828 1.000420 2698.237\n\n\nThe main estimand is the fixed effect, which turn out pretty similar regardless of model in this cleanly simulated data.\n\n\nNow let’s look at those priors I chose.\n\nprior &lt;- extract.prior(mdl_random)\n\nIn file included from stan/lib/stan_math/stan/math/prim/prob/von_mises_lccdf.hpp:5,\n                 from stan/lib/stan_math/stan/math/prim/prob/von_mises_ccdf_log.hpp:4,\n                 from stan/lib/stan_math/stan/math/prim/prob.hpp:356,\n                 from stan/lib/stan_math/stan/math/prim.hpp:16,\n                 from stan/lib/stan_math/stan/math/rev.hpp:14,\n                 from stan/lib/stan_math/stan/math.hpp:19,\n                 from stan/src/stan/model/model_header.hpp:4,\n                 from C:/Users/vilgo/AppData/Local/Temp/Rtmpy0zvj8/model-28bc5ee441d0.hpp:2:\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp: In function 'stan::return_type_t&lt;T_x, T_sigma, T_l&gt; stan::math::von_mises_cdf(const T_x&, const T_mu&, const T_k&)':\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: '-Wmisleading-indentation' is disabled from this point onwards, since column-tracking was disabled due to the size of the code/headers\n  194 |       if (cdf_n &lt; 0.0)\n      | \n\n\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: adding '-flarge-source-files' will allow for more column-tracking support, at the expense of compilation time and memory\n\n\nRunning MCMC with 1 chain, with 1 thread(s) per chain...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 0.8 seconds.\n\n\nWarning: 34 of 1000 (3.0%) transitions ended with a divergence.\nSee https://mc-stan.org/misc/warnings for details.\n\n\nWarning: 1 of 1 chains had an E-BFMI less than 0.2.\nSee https://mc-stan.org/misc/warnings for details.\n\n#20 patients random effects from different draws\n{plot(NULL, xlim = c(0,9), ylim = c(-3,3),\n      ylab = \"standardized symptoms\", xlab = \"weeks\", main = \"random effects\")\n  for(i in 1:20){\n    abline(a = prior$a[i,i], b = prior$b[i,i])\n  }}\n\n\n\n\n\n\n\n#20 draws fixedeffects\n{plot(NULL, xlim = c(0,9), ylim = c(-3,3),\n      ylab = \"standardized symptoms\", xlab = \"weeks\", main = \"fixed effects\")\n  for(i in 1:20){\n    abline(a = prior$a_bar[i], b = prior$b_bar[i])\n  }}\n\n\n\n\n\n\n\n\nWe can see that the priors I choose result in highly implausible effects, especially as it pertains to slopes. Let’s try to limit the variability in slopes by constraining the prior for \\(\\bar{b}\\) to 0.15.\n\nmdl_random_new &lt;- ulam(alist(\n  y ~ dnorm(mu,sigma),\n  mu &lt;- a[id] + b[id]*weeks,\n  a[id] ~ dnorm(a_bar,sigma_a),\n  a_bar ~ dnorm(0,1),\n  sigma_a ~ dexp(1.5),\n  b[id] ~ dnorm(b_bar,sigma_b),\n  b_bar ~ dnorm(0,0.15),\n  sigma_b ~ dexp(1),\n  sigma ~ dexp(1)\n), data = d, chains = 4, cores = 4)\n\nIn file included from stan/lib/stan_math/stan/math/prim/prob/von_mises_lccdf.hpp:5,\n                 from stan/lib/stan_math/stan/math/prim/prob/von_mises_ccdf_log.hpp:4,\n                 from stan/lib/stan_math/stan/math/prim/prob.hpp:356,\n                 from stan/lib/stan_math/stan/math/prim.hpp:16,\n                 from stan/lib/stan_math/stan/math/rev.hpp:14,\n                 from stan/lib/stan_math/stan/math.hpp:19,\n                 from stan/src/stan/model/model_header.hpp:4,\n                 from C:/Users/vilgo/AppData/Local/Temp/Rtmpy0zvj8/model-28bc642c4ca0.hpp:2:\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp: In function 'stan::return_type_t&lt;T_x, T_sigma, T_l&gt; stan::math::von_mises_cdf(const T_x&, const T_mu&, const T_k&)':\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: '-Wmisleading-indentation' is disabled from this point onwards, since column-tracking was disabled due to the size of the code/headers\n  194 |       if (cdf_n &lt; 0.0)\n      | \n\n\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: adding '-flarge-source-files' will allow for more column-tracking support, at the expense of compilation time and memory\n\n\nRunning MCMC with 4 parallel chains, with 1 thread(s) per chain...\n\nChain 1 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 1 Iteration: 100 / 1000 [ 10%]  (Warmup) \n\n\nChain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 1 Exception: normal_lpdf: Scale parameter is 0, but must be positive! (in 'C:/Users/vilgo/AppData/Local/Temp/Rtmpy0zvj8/model-28bc642c4ca0.stan', line 20, column 4 to column 34)\n\n\nChain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 1 \n\n\nChain 2 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 2 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 3 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 4 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 1 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 1 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 1 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 1 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 1 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 1 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 1 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 2 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 2 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 2 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 2 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 2 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 2 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 2 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 3 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 3 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 3 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 3 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 3 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 3 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 4 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 4 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 4 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 4 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 4 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 4 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 1 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 1 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 1 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 2 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 2 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 2 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 3 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 3 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 3 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 4 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 4 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 1 finished in 0.5 seconds.\nChain 2 finished in 0.5 seconds.\nChain 3 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 3 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 4 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 4 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 4 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 3 finished in 0.5 seconds.\nChain 4 finished in 0.5 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.5 seconds.\nTotal execution time: 0.7 seconds.\n\nprior &lt;- extract.prior(mdl_random_new)\n\nIn file included from stan/lib/stan_math/stan/math/prim/prob/von_mises_lccdf.hpp:5,\n                 from stan/lib/stan_math/stan/math/prim/prob/von_mises_ccdf_log.hpp:4,\n                 from stan/lib/stan_math/stan/math/prim/prob.hpp:356,\n                 from stan/lib/stan_math/stan/math/prim.hpp:16,\n                 from stan/lib/stan_math/stan/math/rev.hpp:14,\n                 from stan/lib/stan_math/stan/math.hpp:19,\n                 from stan/src/stan/model/model_header.hpp:4,\n                 from C:/Users/vilgo/AppData/Local/Temp/Rtmpy0zvj8/model-28bc5e826969.hpp:2:\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp: In function 'stan::return_type_t&lt;T_x, T_sigma, T_l&gt; stan::math::von_mises_cdf(const T_x&, const T_mu&, const T_k&)':\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: '-Wmisleading-indentation' is disabled from this point onwards, since column-tracking was disabled due to the size of the code/headers\n  194 |       if (cdf_n &lt; 0.0)\n      | \n\n\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: adding '-flarge-source-files' will allow for more column-tracking support, at the expense of compilation time and memory\n\n\nRunning MCMC with 1 chain, with 1 thread(s) per chain...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 1.0 seconds.\n\n\nWarning: 37 of 1000 (4.0%) transitions ended with a divergence.\nSee https://mc-stan.org/misc/warnings for details.\n\n\nWarning: 1 of 1 chains had an E-BFMI less than 0.2.\nSee https://mc-stan.org/misc/warnings for details.\n\n#20 patients random effects from different draws\n{plot(NULL, xlim = c(0,9), ylim = c(-3,3),\n      ylab = \"standardized symptoms\", xlab = \"weeks\", main = \"random effects\")\n  for(i in 1:20){\n    abline(a = prior$a[i,i], b = prior$b[i,i])\n  }}\n\n\n\n\n\n\n\n#20 draws fixed effects\n{plot(NULL, xlim = c(0,9), ylim = c(-3,3),\n      ylab = \"standardized symptoms\", xlab = \"weeks\", main = \"fixed effects\")\n  for(i in 1:20){\n    abline(a = prior$a_bar[i], b = prior$b_bar[i])\n  }}\n\n\n\n\n\n\n\n\nComment: Fixed effects now look more reasonable. Random/variable effects still vary a lot because even though their mean is constrained, their variability \\(\\sigma_b\\) is still large \\(\\mathbb{E}[Exponential(1)] = 1\\), which turns into a lot of variability if we recall this is per week change. Still, this shouldn’t matter too much when fitting the model since the exponential function with a rate of 1 has quite a lot of probability mass close to 0 anyway. Similarly \\(\\sigma_a\\) could probably be more narrow."
  },
  {
    "objectID": "notes/Stats2IndividualAssignment.html#my-data-and-model",
    "href": "notes/Stats2IndividualAssignment.html#my-data-and-model",
    "title": "Stats2IndividualAssignment",
    "section": "",
    "text": "My data will be self-rated GAD-7 scores from a clinical trial. Patients receive one out of two possible active treatments over ten weeks. I want to use multi-level modeling to model the effect of treatment over time, and compare that between treatments.\nThe formula is presented below for 20 patients (\\(j\\)). My outcome \\(y\\) is GAD-7 score. \\(time\\) will be the week the measurement comes from, ranging from 0 to 9, so that the intercepts \\(\\alpha\\) represent model estimations at start of treatment.\n\\[\n\\begin{align*}\ny_i \\sim Normal(\\mu_i,\\sigma) \\\\\n\\mu_{i,j} = \\alpha_{patient[i]} \\ + \\ \\beta_{patient[i]}*time_i \\\\\n\\alpha_j \\sim Normal(\\bar{\\alpha},\\sigma_a) \\ \\ ,for \\ j = 1,2,...20 \\\\\n\\bar{\\alpha} \\sim Normal(0,1) \\\\\n\\sigma_a \\sim Exponential(1) \\\\\n\\beta_j \\sim Normal(\\bar{\\beta},\\sigma_b) \\ \\ ,for \\ j = 1,2,...20 \\\\\n\\bar{\\beta} \\sim Normal(0,1) \\\\\n\\sigma_b \\sim Exponential(1) \\\\\n\\sigma \\sim Exponential(1)\n\\end{align*}\n\\]\nI have not figured out how to specify a group effect yet but one cheap solution is to just fit two models and compare. In this case that solution is probably adequate since I don’t want the estimate for group A to affect the estimate from group B anyway. However, in time I want to look at contrasts between competing mediators and then it may be better to include group as a factor.\nI have a function that generates data in long format from a previous project. Since the data simulates intercepts ~ Normal(0,1), it can be read as if I were to standardize the scores at week 0. With this standardization the avg_slope argument times the length of the treatment nweeks corresponds to the standardized effect size for growth models following Feingold (2007).\n\n#function that simulates data\nlong.data.sim &lt;- function(n = 30, avg_slope = 0.1, slope_intercept_correlation = -0.15, slope_sigma = 0.1, nweeks = 10, error = 0.3){\n  intercepts &lt;- rnorm(n, m = 0, sd = 1)\n  slopes &lt;- avg_slope + (slope_intercept_correlation*intercepts + rnorm(n,m=0,sd=sqrt(1-slope_intercept_correlation^2)) )*slope_sigma\n  weeks &lt;- 0:(nweeks-1)\n  df &lt;- data.frame()\n  for(i in 1:n){\n    err &lt;- rnorm(nweeks, m = 0, sd = error) #will equal residual std.dev. = sigma(mdl)\n    df &lt;- rbind.data.frame(df,data.frame(id=rep(i, times = nweeks),y=intercepts[i]+slopes[i]*weeks+err,weeks))\n  }\n  return(df)\n}\n\n#generate data\nd &lt;- long.data.sim(n = 20, avg_slope = 0.2, slope_intercept_correlation = -0.15, slope_sigma = 0.1, nweeks = 10, error = 0.3)\n\n#Let's also see if it runs if some patients have missing data some weeks (it will)\nd &lt;- d[-sample(1:nrow(d), 15, replace = FALSE),]\n\nI then built the models step by step increasing complexity.\n\n#single level model (with quadratic aproximation)\nmdl_non &lt;- quap(alist(\n  y ~ dnorm(mu,sigma),\n  mu &lt;- a + b*weeks,\n  a ~ dnorm(0,1),\n  b ~ dnorm(0,0.3),\n  sigma ~ dexp(1)\n), data = d)\nprecis(mdl_non)\n\n            mean         sd      5.5%     94.5%\na     0.07157867 0.14607351 -0.161875 0.3050323\nb     0.21349254 0.02770973  0.169207 0.2577780\nsigma 1.09237163 0.05654070  1.002009 1.1827346\n\n#multi-level model, random intercepts.\nmdl_random_intercept &lt;- ulam(alist(\n  y ~ dnorm(mu,sigma),\n  mu &lt;- a[id] + b*weeks,\n  a[id] ~ dnorm(a_bar,sigma_a),\n  a_bar ~ dnorm(0,1),\n  sigma_a ~ dexp(1),\n  b ~ dnorm(0,0.3),\n  sigma ~ dexp(1)\n), data = d, chains = 4, cores = 4)\n\nIn file included from stan/lib/stan_math/stan/math/prim/prob/von_mises_lccdf.hpp:5,\n                 from stan/lib/stan_math/stan/math/prim/prob/von_mises_ccdf_log.hpp:4,\n                 from stan/lib/stan_math/stan/math/prim/prob.hpp:356,\n                 from stan/lib/stan_math/stan/math/prim.hpp:16,\n                 from stan/lib/stan_math/stan/math/rev.hpp:14,\n                 from stan/lib/stan_math/stan/math.hpp:19,\n                 from stan/src/stan/model/model_header.hpp:4,\n                 from C:/Users/vilgo/AppData/Local/Temp/Rtmpy0zvj8/model-28bc500540dc.hpp:2:\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp: In function 'stan::return_type_t&lt;T_x, T_sigma, T_l&gt; stan::math::von_mises_cdf(const T_x&, const T_mu&, const T_k&)':\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: '-Wmisleading-indentation' is disabled from this point onwards, since column-tracking was disabled due to the size of the code/headers\n  194 |       if (cdf_n &lt; 0.0)\n      | \n\n\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: adding '-flarge-source-files' will allow for more column-tracking support, at the expense of compilation time and memory\n\n\nRunning MCMC with 4 parallel chains, with 1 thread(s) per chain...\n\nChain 1 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 1 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 1 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 1 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 1 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 2 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 2 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 3 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 3 Iteration: 100 / 1000 [ 10%]  (Warmup) \n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: normal_lpdf: Scale parameter is 0, but must be positive! (in 'C:/Users/vilgo/AppData/Local/Temp/Rtmpy0zvj8/model-28bc500540dc.stan', line 19, column 4 to column 34)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 4 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 1 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 1 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 1 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 1 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 1 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 1 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 1 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 1 finished in 0.2 seconds.\nChain 2 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 2 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 2 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 2 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 2 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 2 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 2 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 2 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 2 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 2 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 2 finished in 0.2 seconds.\nChain 3 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 3 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 3 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 3 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 3 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 3 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 3 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 3 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 3 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 3 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 3 finished in 0.2 seconds.\nChain 4 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 4 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 4 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 4 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 4 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 4 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 4 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 4 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 4 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 4 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 4 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 4 finished in 0.2 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.2 seconds.\nTotal execution time: 0.4 seconds.\n\nprecis(mdl_random_intercept, depth = 2)  #seems to run fine with missing rows.\n\n               mean         sd        5.5%       94.5%      rhat ess_bulk\na[1]    -0.02520281 0.14921578 -0.26603355  0.21877512 1.0010833 2202.248\na[2]    -1.10287172 0.13450625 -1.31535830 -0.89130662 1.0038579 3926.797\na[3]    -0.73813835 0.13973373 -0.95917408 -0.51288988 1.0032422 3060.956\na[4]    -0.65834556 0.13519752 -0.86900740 -0.44759664 1.0067085 3126.022\na[5]    -1.15892221 0.13544008 -1.37606300 -0.94658577 1.0046948 3002.016\na[6]     0.75545638 0.13922541  0.53084204  0.97754093 0.9996994 2850.140\na[7]     0.06189972 0.13673783 -0.15652840  0.28013837 1.0027208 2742.055\na[8]    -0.66021047 0.14058414 -0.87995546 -0.43961912 0.9990020 2557.988\na[9]     0.77055111 0.13624650  0.55998342  0.99166465 1.0018397 3225.269\na[10]    2.70448036 0.14066007  2.47965205  2.93030730 1.0014380 2975.519\na[11]    2.28466059 0.14146399  2.04461900  2.50176265 1.0026248 3201.186\na[12]    0.40256383 0.13609763  0.18062447  0.61482962 1.0008079 2271.321\na[13]   -0.76115211 0.13820998 -0.98172314 -0.52767381 1.0057816 3545.426\na[14]   -1.07380916 0.13974519 -1.30386230 -0.84637135 0.9989985 2880.575\na[15]    0.51210060 0.13764298  0.29422378  0.72660891 1.0080993 2904.486\na[16]   -0.50313231 0.14101022 -0.73081864 -0.27814424 1.0016774 3045.534\na[17]    0.12823507 0.13916734 -0.09294399  0.34924909 1.0001611 2645.378\na[18]    0.35461277 0.13950420  0.13398911  0.57701194 1.0032782 3072.307\na[19]   -0.15271404 0.14287922 -0.37555582  0.07263233 1.0015358 3385.546\na[20]    0.57261419 0.14237918  0.34603018  0.79546032 1.0028629 3044.263\na_bar    0.08987243 0.24292037 -0.27879912  0.48183952 1.0004217 3048.228\nsigma_a  1.08438611 0.18586145  0.83071493  1.42053585 1.0031046 3442.261\nb        0.21272936 0.01015007  0.19638681  0.22871962 1.0000291 1129.199\nsigma    0.39928193 0.02262948  0.36390590  0.43656543 1.0156441 3378.770\n\nprecis(mdl_random_intercept) #most relevant parameters\n\n20 vector or matrix parameters hidden. Use depth=2 to show them.\n\n\n              mean         sd       5.5%     94.5%     rhat ess_bulk\na_bar   0.08987243 0.24292037 -0.2787991 0.4818395 1.000422 3048.228\nsigma_a 1.08438611 0.18586145  0.8307149 1.4205358 1.003105 3442.261\nb       0.21272936 0.01015007  0.1963868 0.2287196 1.000029 1129.199\nsigma   0.39928193 0.02262948  0.3639059 0.4365654 1.015644 3378.770\n\n#multi-level model, random intercepts and slopes.\nmdl_random &lt;- ulam(alist(\n  y ~ dnorm(mu,sigma),\n  mu &lt;- a[id] + b[id]*weeks,\n  a[id] ~ dnorm(a_bar,sigma_a),\n  a_bar ~ dnorm(0,1),\n  sigma_a ~ dexp(1),\n  b[id] ~ dnorm(b_bar,sigma_b),\n  b_bar ~ dnorm(0,1),\n  sigma_b ~ dexp(1),\n  sigma ~ dexp(1)\n), data = d, chains = 4, cores = 4)\n\nIn file included from stan/lib/stan_math/stan/math/prim/prob/von_mises_lccdf.hpp:5,\n                 from stan/lib/stan_math/stan/math/prim/prob/von_mises_ccdf_log.hpp:4,\n                 from stan/lib/stan_math/stan/math/prim/prob.hpp:356,\n                 from stan/lib/stan_math/stan/math/prim.hpp:16,\n                 from stan/lib/stan_math/stan/math/rev.hpp:14,\n                 from stan/lib/stan_math/stan/math.hpp:19,\n                 from stan/src/stan/model/model_header.hpp:4,\n                 from C:/Users/vilgo/AppData/Local/Temp/Rtmpy0zvj8/model-28bc22401029.hpp:2:\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp: In function 'stan::return_type_t&lt;T_x, T_sigma, T_l&gt; stan::math::von_mises_cdf(const T_x&, const T_mu&, const T_k&)':\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: '-Wmisleading-indentation' is disabled from this point onwards, since column-tracking was disabled due to the size of the code/headers\n  194 |       if (cdf_n &lt; 0.0)\n      | \n\n\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: adding '-flarge-source-files' will allow for more column-tracking support, at the expense of compilation time and memory\n\n\nRunning MCMC with 4 parallel chains, with 1 thread(s) per chain...\n\nChain 1 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 1 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 1 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 2 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 2 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 3 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 4 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 4 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 1 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 1 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 1 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 1 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 1 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 1 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 2 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 2 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 2 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 2 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 2 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 2 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 2 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 3 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 3 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 3 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 3 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 3 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 3 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 3 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 4 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 4 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 4 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 4 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 4 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 4 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 1 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 1 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 2 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 2 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 2 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 3 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 3 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 4 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 4 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 4 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 1 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 1 finished in 0.5 seconds.\nChain 2 finished in 0.4 seconds.\nChain 3 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 3 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 4 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 3 finished in 0.5 seconds.\nChain 4 finished in 0.4 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.5 seconds.\nTotal execution time: 0.7 seconds.\n\nprecis(mdl_random, depth = 2)\n\n               mean         sd         5.5%        94.5%      rhat ess_bulk\na[1]     0.13763362 0.20401689 -0.192673100  0.453791880 1.0016809 2400.392\na[2]    -1.16918868 0.18464673 -1.469531450 -0.868549215 1.0011623 2636.317\na[3]    -0.63657915 0.18564790 -0.930696490 -0.333380320 1.0004997 3304.281\na[4]    -0.94837438 0.18188301 -1.238314950 -0.646965225 1.0022509 2728.661\na[5]    -0.70659566 0.18588272 -1.013064950 -0.407646580 1.0002304 2441.167\na[6]     0.49739330 0.19678131  0.186874515  0.809785695 1.0002165 2855.974\na[7]     0.02333268 0.18673091 -0.270914670  0.329632750 0.9999923 2491.078\na[8]    -0.76884871 0.18410184 -1.058877250 -0.469463195 0.9991601 2677.698\na[9]     1.05371714 0.17832851  0.763279650  1.342631350 1.0007241 2479.162\na[10]    2.17960233 0.18870650  1.863835250  2.471498400 1.0013048 2302.449\na[11]    1.73940472 0.20379633  1.414371750  2.071898100 1.0003578 2338.700\na[12]    1.00962617 0.18431233  0.717254315  1.308446850 1.0026771 2955.093\na[13]   -0.78732622 0.18317331 -1.084791550 -0.491850020 1.0021275 2132.853\na[14]   -1.28735082 0.19064200 -1.605387700 -0.987822590 1.0004825 3185.551\na[15]    0.30584927 0.18758471  0.013459617  0.595883325 0.9999719 2612.923\na[16]   -0.28621499 0.18199055 -0.575899155 -0.001304038 0.9993551 2638.725\na[17]    0.09551850 0.18117889 -0.200388885  0.389704220 1.0046455 2603.652\na[18]    0.29510087 0.18552802  0.007532375  0.586815365 0.9999263 2773.765\na[19]    0.13720548 0.18507220 -0.159640890  0.428200625 1.0003737 2454.380\na[20]    0.75101203 0.22185446  0.408661745  1.095551350 1.0011165 3136.567\na_bar    0.07554967 0.22148420 -0.273502570  0.411122280 1.0002413 3887.325\nsigma_a  0.99857044 0.17603429  0.760254025  1.302518550 1.0029860 3147.878\nb[1]     0.17555283 0.03698009  0.116280280  0.235469125 1.0035438 2290.194\nb[2]     0.22601328 0.03282678  0.173460355  0.277233385 0.9996436 2763.655\nb[3]     0.18553127 0.03795384  0.126349360  0.245725895 1.0004982 3224.542\nb[4]     0.27818150 0.03423851  0.225094610  0.332742825 1.0025901 2709.904\nb[5]     0.11093633 0.03423799  0.057080752  0.165571915 1.0013364 2391.697\nb[6]     0.26696488 0.03458176  0.210526000  0.323312385 1.0008511 2661.686\nb[7]     0.22119065 0.03415080  0.166000185  0.275100210 0.9989249 2554.785\nb[8]     0.23681654 0.03334569  0.183751640  0.289308810 0.9989245 2609.819\nb[9]     0.14993050 0.03249067  0.098182279  0.202939615 1.0005990 2371.687\nb[10]    0.33164854 0.03448216  0.276640175  0.387383035 1.0000744 2236.169\nb[11]    0.34819513 0.04018706  0.283062535  0.411618505 1.0029311 2295.675\nb[12]    0.07602954 0.03424976  0.019293557  0.130471585 1.0029694 2609.973\nb[13]    0.21824780 0.03672063  0.161055030  0.276723140 1.0066972 2158.237\nb[14]    0.25820921 0.03389141  0.204913350  0.312336550 1.0028233 3318.883\nb[15]    0.26031897 0.03374351  0.207539000  0.315102325 0.9993097 2533.134\nb[16]    0.15981830 0.03510816  0.102849735  0.214442950 0.9994200 2727.007\nb[17]    0.22076857 0.03376817  0.164851810  0.274731880 1.0042257 2633.818\nb[18]    0.22793371 0.03656095  0.168902895  0.284702780 1.0008931 2824.816\nb[19]    0.14635664 0.03339399  0.091356924  0.199918775 0.9998916 2376.836\nb[20]    0.17686581 0.03819172  0.116701955  0.237775075 1.0006119 2964.347\nb_bar    0.21410393 0.02131508  0.180765165  0.248140580 1.0003901 2671.844\nsigma_b  0.08171234 0.01789594  0.057558535  0.110501015 1.0014209 2002.953\nsigma    0.33128625 0.01939546  0.302480820  0.363382790 1.0004200 2698.237\n\nprecis(mdl_random)\n\n40 vector or matrix parameters hidden. Use depth=2 to show them.\n\n\n              mean         sd        5.5%     94.5%     rhat ess_bulk\na_bar   0.07554967 0.22148420 -0.27350257 0.4111223 1.000241 3887.325\nsigma_a 0.99857044 0.17603429  0.76025402 1.3025186 1.002986 3147.878\nb_bar   0.21410393 0.02131508  0.18076517 0.2481406 1.000390 2671.844\nsigma_b 0.08171234 0.01789594  0.05755853 0.1105010 1.001421 2002.953\nsigma   0.33128625 0.01939546  0.30248082 0.3633828 1.000420 2698.237\n\n\nThe main estimand is the fixed effect, which turn out pretty similar regardless of model in this cleanly simulated data.\n\n\nNow let’s look at those priors I chose.\n\nprior &lt;- extract.prior(mdl_random)\n\nIn file included from stan/lib/stan_math/stan/math/prim/prob/von_mises_lccdf.hpp:5,\n                 from stan/lib/stan_math/stan/math/prim/prob/von_mises_ccdf_log.hpp:4,\n                 from stan/lib/stan_math/stan/math/prim/prob.hpp:356,\n                 from stan/lib/stan_math/stan/math/prim.hpp:16,\n                 from stan/lib/stan_math/stan/math/rev.hpp:14,\n                 from stan/lib/stan_math/stan/math.hpp:19,\n                 from stan/src/stan/model/model_header.hpp:4,\n                 from C:/Users/vilgo/AppData/Local/Temp/Rtmpy0zvj8/model-28bc5ee441d0.hpp:2:\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp: In function 'stan::return_type_t&lt;T_x, T_sigma, T_l&gt; stan::math::von_mises_cdf(const T_x&, const T_mu&, const T_k&)':\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: '-Wmisleading-indentation' is disabled from this point onwards, since column-tracking was disabled due to the size of the code/headers\n  194 |       if (cdf_n &lt; 0.0)\n      | \n\n\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: adding '-flarge-source-files' will allow for more column-tracking support, at the expense of compilation time and memory\n\n\nRunning MCMC with 1 chain, with 1 thread(s) per chain...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 0.8 seconds.\n\n\nWarning: 34 of 1000 (3.0%) transitions ended with a divergence.\nSee https://mc-stan.org/misc/warnings for details.\n\n\nWarning: 1 of 1 chains had an E-BFMI less than 0.2.\nSee https://mc-stan.org/misc/warnings for details.\n\n#20 patients random effects from different draws\n{plot(NULL, xlim = c(0,9), ylim = c(-3,3),\n      ylab = \"standardized symptoms\", xlab = \"weeks\", main = \"random effects\")\n  for(i in 1:20){\n    abline(a = prior$a[i,i], b = prior$b[i,i])\n  }}\n\n\n\n\n\n\n\n#20 draws fixedeffects\n{plot(NULL, xlim = c(0,9), ylim = c(-3,3),\n      ylab = \"standardized symptoms\", xlab = \"weeks\", main = \"fixed effects\")\n  for(i in 1:20){\n    abline(a = prior$a_bar[i], b = prior$b_bar[i])\n  }}\n\n\n\n\n\n\n\n\nWe can see that the priors I choose result in highly implausible effects, especially as it pertains to slopes. Let’s try to limit the variability in slopes by constraining the prior for \\(\\bar{b}\\) to 0.15.\n\nmdl_random_new &lt;- ulam(alist(\n  y ~ dnorm(mu,sigma),\n  mu &lt;- a[id] + b[id]*weeks,\n  a[id] ~ dnorm(a_bar,sigma_a),\n  a_bar ~ dnorm(0,1),\n  sigma_a ~ dexp(1.5),\n  b[id] ~ dnorm(b_bar,sigma_b),\n  b_bar ~ dnorm(0,0.15),\n  sigma_b ~ dexp(1),\n  sigma ~ dexp(1)\n), data = d, chains = 4, cores = 4)\n\nIn file included from stan/lib/stan_math/stan/math/prim/prob/von_mises_lccdf.hpp:5,\n                 from stan/lib/stan_math/stan/math/prim/prob/von_mises_ccdf_log.hpp:4,\n                 from stan/lib/stan_math/stan/math/prim/prob.hpp:356,\n                 from stan/lib/stan_math/stan/math/prim.hpp:16,\n                 from stan/lib/stan_math/stan/math/rev.hpp:14,\n                 from stan/lib/stan_math/stan/math.hpp:19,\n                 from stan/src/stan/model/model_header.hpp:4,\n                 from C:/Users/vilgo/AppData/Local/Temp/Rtmpy0zvj8/model-28bc642c4ca0.hpp:2:\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp: In function 'stan::return_type_t&lt;T_x, T_sigma, T_l&gt; stan::math::von_mises_cdf(const T_x&, const T_mu&, const T_k&)':\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: '-Wmisleading-indentation' is disabled from this point onwards, since column-tracking was disabled due to the size of the code/headers\n  194 |       if (cdf_n &lt; 0.0)\n      | \n\n\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: adding '-flarge-source-files' will allow for more column-tracking support, at the expense of compilation time and memory\n\n\nRunning MCMC with 4 parallel chains, with 1 thread(s) per chain...\n\nChain 1 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 1 Iteration: 100 / 1000 [ 10%]  (Warmup) \n\n\nChain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 1 Exception: normal_lpdf: Scale parameter is 0, but must be positive! (in 'C:/Users/vilgo/AppData/Local/Temp/Rtmpy0zvj8/model-28bc642c4ca0.stan', line 20, column 4 to column 34)\n\n\nChain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 1 \n\n\nChain 2 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 2 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 3 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 4 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 1 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 1 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 1 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 1 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 1 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 1 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 1 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 2 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 2 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 2 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 2 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 2 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 2 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 2 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 3 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 3 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 3 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 3 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 3 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 3 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 4 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 4 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 4 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 4 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 4 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 4 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 1 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 1 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 1 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 2 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 2 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 2 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 3 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 3 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 3 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 4 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 4 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 1 finished in 0.5 seconds.\nChain 2 finished in 0.5 seconds.\nChain 3 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 3 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 4 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 4 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 4 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 3 finished in 0.5 seconds.\nChain 4 finished in 0.5 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.5 seconds.\nTotal execution time: 0.7 seconds.\n\nprior &lt;- extract.prior(mdl_random_new)\n\nIn file included from stan/lib/stan_math/stan/math/prim/prob/von_mises_lccdf.hpp:5,\n                 from stan/lib/stan_math/stan/math/prim/prob/von_mises_ccdf_log.hpp:4,\n                 from stan/lib/stan_math/stan/math/prim/prob.hpp:356,\n                 from stan/lib/stan_math/stan/math/prim.hpp:16,\n                 from stan/lib/stan_math/stan/math/rev.hpp:14,\n                 from stan/lib/stan_math/stan/math.hpp:19,\n                 from stan/src/stan/model/model_header.hpp:4,\n                 from C:/Users/vilgo/AppData/Local/Temp/Rtmpy0zvj8/model-28bc5e826969.hpp:2:\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp: In function 'stan::return_type_t&lt;T_x, T_sigma, T_l&gt; stan::math::von_mises_cdf(const T_x&, const T_mu&, const T_k&)':\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: '-Wmisleading-indentation' is disabled from this point onwards, since column-tracking was disabled due to the size of the code/headers\n  194 |       if (cdf_n &lt; 0.0)\n      | \n\n\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: adding '-flarge-source-files' will allow for more column-tracking support, at the expense of compilation time and memory\n\n\nRunning MCMC with 1 chain, with 1 thread(s) per chain...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 1.0 seconds.\n\n\nWarning: 37 of 1000 (4.0%) transitions ended with a divergence.\nSee https://mc-stan.org/misc/warnings for details.\n\n\nWarning: 1 of 1 chains had an E-BFMI less than 0.2.\nSee https://mc-stan.org/misc/warnings for details.\n\n#20 patients random effects from different draws\n{plot(NULL, xlim = c(0,9), ylim = c(-3,3),\n      ylab = \"standardized symptoms\", xlab = \"weeks\", main = \"random effects\")\n  for(i in 1:20){\n    abline(a = prior$a[i,i], b = prior$b[i,i])\n  }}\n\n\n\n\n\n\n\n#20 draws fixed effects\n{plot(NULL, xlim = c(0,9), ylim = c(-3,3),\n      ylab = \"standardized symptoms\", xlab = \"weeks\", main = \"fixed effects\")\n  for(i in 1:20){\n    abline(a = prior$a_bar[i], b = prior$b_bar[i])\n  }}\n\n\n\n\n\n\n\n\nComment: Fixed effects now look more reasonable. Random/variable effects still vary a lot because even though their mean is constrained, their variability \\(\\sigma_b\\) is still large \\(\\mathbb{E}[Exponential(1)] = 1\\), which turns into a lot of variability if we recall this is per week change. Still, this shouldn’t matter too much when fitting the model since the exponential function with a rate of 1 has quite a lot of probability mass close to 0 anyway. Similarly \\(\\sigma_a\\) could probably be more narrow."
  },
  {
    "objectID": "notes/Stats2IndividualAssignment.html#extra-model-with-slope-intercept-correlation",
    "href": "notes/Stats2IndividualAssignment.html#extra-model-with-slope-intercept-correlation",
    "title": "Stats2IndividualAssignment",
    "section": "Extra: Model with slope-intercept correlation",
    "text": "Extra: Model with slope-intercept correlation\nHere I attempt a model with a slope-intercept correlation for the varying effects.\n\\[\n\\begin{align}\ny_i \\sim Normal(\\mu_i,\\sigma) \\\\\n\\mu_{i,j} = \\alpha_{patient[i]} \\ + \\ \\beta_{patient[i]}*time_i \\\\\n\\bigl[ \\substack{\\alpha_j \\\\ \\beta_j} \\bigr] \\sim MVNormal(\\bigl[ \\substack{\\alpha \\\\ \\beta} \\bigr], S) \\\\\nS = \\begin{pmatrix} 0 & \\sigma_a \\\\ \\sigma_b & 0 \\end{pmatrix} R \\begin{pmatrix} 0 & \\sigma_a \\\\ \\sigma_b & 0 \\end{pmatrix}\\\\\n\\alpha \\sim Normal(0,1) \\\\\n\\beta \\sim Normal(0,1) \\\\\n\\sigma_a \\sim Exponential(1) \\\\\n\\sigma_b \\sim Exponential(1) \\\\\n\\sigma \\sim Exponential(1) \\\\\nR \\sim LKJcorr(1.5)\n\\end{align}\n\\]\n\nmdl_random_correlated &lt;- ulam(alist(\n  y ~ dnorm(mu,sigma),\n  mu &lt;- a_id[id] + b_id[id]*weeks,\n  c(a_id,b_id)[id] ~ multi_normal(c(a,b), Rho, sigma_a), #for some reason I can only specify one sigma here. c(sigma_a, sigma_b) does not work... It treats sigma_a as a repeated vector. Annoying since the priors for sigma_a and sigma_b have quite different implications.\n  a ~ dnorm(0,1),\n  sigma_a ~ dexp(1),\n  b ~ dnorm(0,1),\n  sigma ~ dexp(1),\n  Rho ~ lkj_corr(1.5)\n), data = d, chains = 4, cores = 4)\n\nIn file included from stan/lib/stan_math/stan/math/prim/prob/von_mises_lccdf.hpp:5,\n                 from stan/lib/stan_math/stan/math/prim/prob/von_mises_ccdf_log.hpp:4,\n                 from stan/lib/stan_math/stan/math/prim/prob.hpp:356,\n                 from stan/lib/stan_math/stan/math/prim.hpp:16,\n                 from stan/lib/stan_math/stan/math/rev.hpp:14,\n                 from stan/lib/stan_math/stan/math.hpp:19,\n                 from stan/src/stan/model/model_header.hpp:4,\n                 from C:/Users/vilgo/AppData/Local/Temp/Rtmpy0zvj8/model-28bc14ca71d1.hpp:2:\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp: In function 'stan::return_type_t&lt;T_x, T_sigma, T_l&gt; stan::math::von_mises_cdf(const T_x&, const T_mu&, const T_k&)':\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: '-Wmisleading-indentation' is disabled from this point onwards, since column-tracking was disabled due to the size of the code/headers\n  194 |       if (cdf_n &lt; 0.0)\n      | \n\n\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: adding '-flarge-source-files' will allow for more column-tracking support, at the expense of compilation time and memory\n\n\nRunning MCMC with 4 parallel chains, with 1 thread(s) per chain...\n\nChain 1 Iteration:   1 / 1000 [  0%]  (Warmup) \n\n\nChain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 1 Exception: lkj_corr_lpdf: Correlation matrix is not positive definite. (in 'C:/Users/vilgo/AppData/Local/Temp/Rtmpy0zvj8/model-28bc14ca71d1.stan', line 17, column 4 to column 26)\n\n\nChain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 1 \n\n\nChain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 1 Exception: lkj_corr_lpdf: Correlation matrix is not positive definite. (in 'C:/Users/vilgo/AppData/Local/Temp/Rtmpy0zvj8/model-28bc14ca71d1.stan', line 17, column 4 to column 26)\n\n\nChain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 1 \n\n\nChain 2 Iteration:   1 / 1000 [  0%]  (Warmup) \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: lkj_corr_lpdf: Correlation matrix is not positive definite. (in 'C:/Users/vilgo/AppData/Local/Temp/Rtmpy0zvj8/model-28bc14ca71d1.stan', line 17, column 4 to column 26)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 3 Iteration:   1 / 1000 [  0%]  (Warmup) \n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: lkj_corr_lpdf: Correlation matrix is not positive definite. (in 'C:/Users/vilgo/AppData/Local/Temp/Rtmpy0zvj8/model-28bc14ca71d1.stan', line 17, column 4 to column 26)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: lkj_corr_lpdf: Correlation matrix is not positive definite. (in 'C:/Users/vilgo/AppData/Local/Temp/Rtmpy0zvj8/model-28bc14ca71d1.stan', line 17, column 4 to column 26)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: lkj_corr_lpdf: Correlation matrix is not positive definite. (in 'C:/Users/vilgo/AppData/Local/Temp/Rtmpy0zvj8/model-28bc14ca71d1.stan', line 17, column 4 to column 26)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: lkj_corr_lpdf: Correlation matrix is not positive definite. (in 'C:/Users/vilgo/AppData/Local/Temp/Rtmpy0zvj8/model-28bc14ca71d1.stan', line 17, column 4 to column 26)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: lkj_corr_lpdf: Correlation matrix is not positive definite. (in 'C:/Users/vilgo/AppData/Local/Temp/Rtmpy0zvj8/model-28bc14ca71d1.stan', line 17, column 4 to column 26)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: lkj_corr_lpdf: Correlation matrix is not positive definite. (in 'C:/Users/vilgo/AppData/Local/Temp/Rtmpy0zvj8/model-28bc14ca71d1.stan', line 17, column 4 to column 26)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: lkj_corr_lpdf: Correlation matrix is not positive definite. (in 'C:/Users/vilgo/AppData/Local/Temp/Rtmpy0zvj8/model-28bc14ca71d1.stan', line 17, column 4 to column 26)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 4 Iteration:   1 / 1000 [  0%]  (Warmup) \n\n\nChain 4 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 4 Exception: lkj_corr_lpdf: Correlation matrix is not positive definite. (in 'C:/Users/vilgo/AppData/Local/Temp/Rtmpy0zvj8/model-28bc14ca71d1.stan', line 17, column 4 to column 26)\n\n\nChain 4 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 4 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 4 \n\n\nChain 4 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 4 Exception: lkj_corr_lpdf: Correlation matrix is not positive definite. (in 'C:/Users/vilgo/AppData/Local/Temp/Rtmpy0zvj8/model-28bc14ca71d1.stan', line 17, column 4 to column 26)\n\n\nChain 4 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 4 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 4 \n\n\nChain 4 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 4 Exception: lkj_corr_lpdf: Correlation matrix is not positive definite. (in 'C:/Users/vilgo/AppData/Local/Temp/Rtmpy0zvj8/model-28bc14ca71d1.stan', line 17, column 4 to column 26)\n\n\nChain 4 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 4 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 4 \n\n\nChain 4 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 4 Exception: lkj_corr_lpdf: Correlation matrix is not positive definite. (in 'C:/Users/vilgo/AppData/Local/Temp/Rtmpy0zvj8/model-28bc14ca71d1.stan', line 17, column 4 to column 26)\n\n\nChain 4 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 4 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 4 \n\n\nChain 1 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 2 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 3 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 4 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 1 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 2 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 4 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 1 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 2 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 3 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 4 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 1 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 2 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 3 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 3 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 4 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 1 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 1 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 2 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 2 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 3 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 3 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 4 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 4 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 1 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 1 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 2 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 3 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 4 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 1 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 2 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 3 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 4 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 1 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 2 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 3 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 4 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 1 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 2 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 2 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 3 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 4 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 4 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 1 finished in 1.2 seconds.\nChain 2 finished in 1.2 seconds.\nChain 4 finished in 1.2 seconds.\nChain 3 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 3 finished in 1.2 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 1.2 seconds.\nTotal execution time: 1.4 seconds.\n\n\nI get a lot of warnings. However I interpret the warning “If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine” as an indication that this problem is pretty expected here. Looking at precis() below the rhats seem fine.\n\nprecis(mdl_random_correlated, depth = 3) \n\n                  mean         sd        5.5%         94.5%      rhat ess_bulk\nb_id[1]     0.17669626 0.03553421  0.12093220  0.2329488050 1.0012134 2917.906\nb_id[2]     0.22145559 0.03304110  0.16990658  0.2748809400 1.0031264 3413.439\nb_id[3]     0.18434513 0.03732565  0.12533119  0.2422476400 1.0010469 3465.627\nb_id[4]     0.27567014 0.03340783  0.22317796  0.3318849200 0.9999968 3554.463\nb_id[5]     0.11039179 0.03355736  0.05631473  0.1656054950 1.0005530 3129.154\nb_id[6]     0.26820252 0.03454081  0.21209475  0.3242612900 1.0011195 3031.353\nb_id[7]     0.21997077 0.03334915  0.16494550  0.2748798500 0.9992327 3309.702\nb_id[8]     0.23357677 0.03369836  0.18028684  0.2870902500 1.0008546 2465.568\nb_id[9]     0.15297642 0.03355186  0.10113697  0.2058835550 1.0028276 3146.139\nb_id[10]    0.33455924 0.03388681  0.27781557  0.3885598500 0.9995131 3707.049\nb_id[11]    0.35225692 0.04028725  0.28910959  0.4159689750 1.0002963 3190.102\nb_id[12]    0.07875229 0.03607353  0.02170077  0.1373183200 1.0013993 1965.257\nb_id[13]    0.21548517 0.03741131  0.15609782  0.2747518500 1.0034483 3767.847\nb_id[14]    0.25496926 0.03462412  0.19914025  0.3102808700 1.0044420 2581.053\nb_id[15]    0.26020372 0.03215319  0.21058762  0.3120072900 0.9995984 2913.487\nb_id[16]    0.16113164 0.03397375  0.10637139  0.2159037700 1.0040639 3211.447\nb_id[17]    0.22021784 0.03313242  0.16788602  0.2729621650 1.0108866 3676.066\nb_id[18]    0.22946066 0.03493809  0.17211903  0.2857401850 1.0008865 2604.672\nb_id[19]    0.14795302 0.03323039  0.09532655  0.2017777250 1.0002610 2772.949\nb_id[20]    0.18003044 0.03897877  0.11715083  0.2426505850 1.0002832 2916.321\na_id[1]     0.13487389 0.19770209 -0.18096550  0.4436078300 1.0041735 3062.807\na_id[2]    -1.14948816 0.18226095 -1.43846575 -0.8618922800 1.0044148 4128.470\na_id[3]    -0.62658303 0.18259524 -0.90805529 -0.3313473100 1.0029558 3482.623\na_id[4]    -0.93541606 0.17882787 -1.23048155 -0.6551982050 0.9993739 3338.520\na_id[5]    -0.70430011 0.18383821 -0.99848009 -0.4040508750 1.0005783 2850.351\na_id[6]     0.49751725 0.19400697  0.18729833  0.7930405400 0.9993051 3034.489\na_id[7]     0.03051752 0.18176117 -0.25632471  0.3232140850 0.9997550 2879.166\na_id[8]    -0.75402800 0.18258381 -1.04538775 -0.4600424300 1.0005925 3102.486\na_id[9]     1.03435234 0.18137951  0.74835034  1.3176076000 1.0047276 3193.936\na_id[10]    2.16855664 0.18074470  1.88874835  2.4673220000 0.9991712 3390.995\na_id[11]    1.72489880 0.19878958  1.41419725  2.0456650000 1.0002048 3332.470\na_id[12]    0.98958858 0.19077018  0.68032082  1.2931812500 1.0014414 2397.772\na_id[13]   -0.77661055 0.18385641 -1.06203055 -0.4823086200 0.9997682 3576.332\na_id[14]   -1.26521125 0.19680436 -1.58571485 -0.9579144200 1.0011688 2637.880\na_id[15]    0.31142792 0.17880296  0.02786370  0.5915412550 1.0011379 3054.359\na_id[16]   -0.29175907 0.18237218 -0.58808523  0.0007084818 1.0092575 3148.936\na_id[17]    0.09980958 0.17871560 -0.18794585  0.3880118550 1.0055821 3765.294\na_id[18]    0.29349224 0.17673115  0.01241182  0.5768513550 0.9997779 2637.333\na_id[19]    0.12846113 0.18458530 -0.16016782  0.4309638000 0.9989320 2752.010\na_id[20]    0.73357723 0.22412747  0.37900075  1.0996244000 1.0007233 2905.838\na           0.07904683 0.21677731 -0.26738110  0.4154573500 1.0015865 3173.621\nsigma_a[1]  1.00988666 0.17990684  0.75924678  1.3322855000 0.9997528 3562.037\nsigma_a[2]  0.08276055 0.01778055  0.05833212  0.1130584600 1.0006874 1644.031\nb           0.21357904 0.02107141  0.17973075  0.2470694150 1.0000303 2703.184\nsigma       0.33154406 0.01925279  0.30297062  0.3617909350 1.0019984 2472.055\nRho[1,1]    1.00000000 0.00000000  1.00000000  1.0000000000        NA       NA\nRho[2,1]    0.11543247 0.23298630 -0.26088732  0.4750070500 0.9998740 2094.633\nRho[1,2]    0.11543247 0.23298630 -0.26088732  0.4750070500 0.9998740 2094.633\nRho[2,2]    1.00000000 0.00000000  1.00000000  1.0000000000        NA       NA\n\npost &lt;- extract.samples(mdl_random_correlated)\nmean(post$Rho[,1,2]) #should be close-ish to -0.15, but the variable has a lot of simulation variance in smaller samples.\n\n[1] 0.1154325"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About Me",
    "section": "",
    "text": "I’m a PhD student in psychology at Karolinska Institutet, Stockholm. My research concerns Cognitive Behavioral Therapy as guided self-help online (ICBT) and Generalized Anxiety Disorder (GAD). I’m specifically focused on whether the mechanisms of change implied by our cognitive models of worry is actually at work within (I)CBT treatments. Other than that I have an interest in methods and meta-science and organize the ReproducibiliTea Journal Club at KI.\n\n\n\nI’ve been generally interested in psychology since I was a teenager. Some areas I’m especially nerdy about are:\n\nCognitive biases in decision making and belief formation.\nMotivated cognition.\nWhat are beliefs even?\nThe neuroscience of emotion.\nPsychiatric diagnostic reasoning and nosology.\n\n\n\n\nDungeons and Dragons. Kino. Going on a trek in the Swedish mountains. Graphic novels. Radiohead.\nThank you for checking out my website!"
  },
  {
    "objectID": "index.html#professional-research-interests",
    "href": "index.html#professional-research-interests",
    "title": "About Me",
    "section": "",
    "text": "I’m a PhD student in psychology at Karolinska Institutet, Stockholm. My research concerns Cognitive Behavioral Therapy as guided self-help online (ICBT) and Generalized Anxiety Disorder (GAD). I’m specifically focused on whether the mechanisms of change implied by our cognitive models of worry is actually at work within (I)CBT treatments. Other than that I have an interest in methods and meta-science and organize the ReproducibiliTea Journal Club at KI."
  },
  {
    "objectID": "index.html#less-professional-research-interests",
    "href": "index.html#less-professional-research-interests",
    "title": "About Me",
    "section": "",
    "text": "I’ve been generally interested in psychology since I was a teenager. Some areas I’m especially nerdy about are:\n\nCognitive biases in decision making and belief formation.\nMotivated cognition.\nWhat are beliefs even?\nThe neuroscience of emotion.\nPsychiatric diagnostic reasoning and nosology."
  },
  {
    "objectID": "index.html#abjectly-uprofessional-interests",
    "href": "index.html#abjectly-uprofessional-interests",
    "title": "About Me",
    "section": "",
    "text": "Dungeons and Dragons. Kino. Going on a trek in the Swedish mountains. Graphic novels. Radiohead."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "personal substack",
    "section": "",
    "text": "I write my personal blog at Substack. It’s called unconfusion and so far mostly concerns online discourse. I find some posts age quickly, but I stood by them when I wrote them and I can never change that. I’ll cross-post some statistics or psychology related under “R/stats posts”."
  },
  {
    "objectID": "notes/mediation_sim_may_WIP.html",
    "href": "notes/mediation_sim_may_WIP.html",
    "title": "mediation_sim_may_WIP",
    "section": "",
    "text": "My goal here is to build a model that describes the essential aspects of the data-generating process that our statistical test will attempt to estimate.\nEach individual will be part of either the IU treatment group or the MC treatment group. For the duration of the treatment, the mediator is assumed to gradually change in an approximately linear way. Before considering the comparisons between groups, let’s first imagine a patient in one group with a changing mediator, \\(m^a\\). The residuals of the model \\(\\sigma\\) is assumed to be normally distributed with a mean of 0. \\(t\\) is a variable that represent time, which can be weeks from 0 to 9.\n\\[\nm_i \\sim N(\\mu_i,\\sigma) \\\\\n\\mu_i = \\alpha  +  \\beta t_i\n\\]\nFor now let’s just imagine the data-generating process for a single patient. When generating data parameters like \\(\\alpha\\) and \\(\\beta\\) will have “priors” that determine the distribution that data-points are drawn from. For our purposes now the input in those distribution functions are somewhat arbitrary, and they do not represent any thought through Bayesian priors. Because I’m not yet trying to model uncertainty, I’ll skip specifying \\(\\sigma\\) as a distribution, instead it will be set to = 0.3.\n\\[\n\\alpha \\sim N(0,1) \\\\\n\\beta \\sim N(-0.1,0.05)\n\\]\n\\(\\alpha\\) now contains the normal intercept (week = 0) standard deviation for the mediator, while \\(\\beta\\) represents the average slope of the mediator, as well as the heterogeneity of slopes. Let’s simulate and plot 1 patient:\n\nn &lt;- 1\nmediator_intercept_sd &lt;- 1\navg_mediator_slope &lt;- -0.1\nmediator_slope_sd &lt;- 0.05\nweeks &lt;- 0:9\nsigma &lt;- 0.3\n\nm_1 &lt;- rnorm(n, 0, mediator_intercept_sd) + rnorm(n, avg_mediator_slope, mediator_slope_sd)*weeks + rnorm((n*length(weeks)), 0, sigma)\n\nplot(weeks,m_1, ylim = c(-2,2))\n\n\n\n\n\n\n\n\nSome additional things to be explicit about: \\(m_i\\) is changing linearly, if not for \\(\\sigma\\). This \\(\\sigma\\) can be seen as representing both “measurement error” and exogenous influences. While our treatment is hypothesized to affect the mediator, it is not likely to be the only thing affecting the mediator. We also imagine that the change is heterogenous; the treatment is not expected to work equally for everyone.\nNow let’s model how this might look for many different patients \\(j=1,...,n\\). The value of a datapoint \\(\\mu_i\\) (if not for it’s error/exogenous influences) can now be given by this formula:\n\\[\nm_i \\sim N(\\mu_i,\\sigma) \\\\\n\\mu_{i} = \\alpha_{j[i]} \\ + \\ \\beta_{j[i]}t_i \\\\\n\\alpha_j \\sim N(0,1) \\\\\n\\beta_j \\sim N(-0.1,0.05) \\\\\n\\]\nThe only thing that has changed is that we’re now imagining that the slopes and intercepts beloing to many different patients. To generate this we could simply loop the code many times.\n\nn &lt;- 7 #note that the code calls nr patients \"n\" instead of j.\nmediator_intercept_sd &lt;- 1\navg_mediator_slope &lt;- -0.1\nmediator_slope_sd &lt;- 0.05\nweeks &lt;- 0:9\nsigma &lt;- 0.3\nrho &lt;- -0.15\n\ndf &lt;- data.frame(m = c(), week = c(), id = c())\n\nfor(i in 1:n){\nm_1 &lt;- rnorm(1, 0, mediator_intercept_sd) + rnorm(1, avg_mediator_slope, mediator_slope_sd)*weeks + rnorm((length(weeks)), 0, sigma)\ndf &lt;- rbind.data.frame(df,data.frame(m = m_1, week = weeks, id = rep(i, times = 10)))\n}\n\nIn real data it is common to see a negative slope-intercept correlation where a higher value on an individuals \\(\\alpha\\) would be associated with a more negative \\(\\beta\\). Our data-generating process should model this as well. To achieve this we need to let intercepts and slopes them be drawn from a multivariate normal distribution.\n\\[\n\\begin{align}\ny_i &\\sim N(\\mu_i,\\sigma) \\\\\n\\mu_i &= \\alpha_{j[i]} + \\beta_{j[i]}t_i \\\\\n\\begin{pmatrix} \\alpha_j \\\\ \\beta_j \\end{pmatrix} &\\sim \\text{MVN}\\left(\\begin{pmatrix} \\mu_\\alpha \\\\ \\mu_\\beta \\end{pmatrix}, \\Sigma\\right) \\\\\n\\mu_\\alpha &= 0 \\\\\n\\mu_\\beta &= -0.1 \\\\\n\\Sigma &= \\begin{pmatrix} \\sigma_\\alpha^2 & \\rho\\sigma_\\alpha\\sigma_\\beta \\\\ \\rho\\sigma_\\alpha\\sigma_\\beta & \\sigma_\\beta^2 \\end{pmatrix} \\\\\n\\rho &= -0.15 \\\\\n\\sigma_\\alpha &= 1 \\\\\n\\sigma_\\beta &= 0.05\n\\end{align}\n\\]\n\\(\\Sigma\\) now represent a common covariance matrix for the slopes and intercepts. The diagonal of the matrix represent the intercept and slope variances for earlier, but we now also have a \\(\\rho\\sigma_\\alpha \\sigma_\\beta\\) that represents the correlation. When generating data I’ve set it to -0.15.\nUpdating the code and looping for 7 patients looks like this:\n\n#Now with slope-intercept correlation\nn &lt;- 7 #note that the code calls nr patients \"n\" instead of j.\nmediator_intercept_sd &lt;- 1\navg_mediator_slope &lt;- -0.1\nmediator_slope_sd &lt;- 0.05\nweeks &lt;- 0:9\nsigma &lt;- 0.3\nrho &lt;- -0.15\n\n# Set up multivariate normal parameters\nmu &lt;- c(0, avg_mediator_slope)  # means for intercept and slope\nSigma &lt;- matrix(c(mediator_intercept_sd^2, \n                  rho * mediator_intercept_sd * mediator_slope_sd,\n                  rho * mediator_intercept_sd * mediator_slope_sd, \n                  mediator_slope_sd^2), \n                nrow = 2, ncol = 2)\n\ndf &lt;- data.frame(m = c(), week = c(), id = c())\n\nfor(i in 1:n){\n  # Generate correlated intercept and slope for this participant\n  params &lt;- MASS::mvrnorm(1, mu = mu, Sigma = Sigma)\n  alpha_i &lt;- params[1]  # intercept\n  beta_i &lt;- params[2]   # slope\n  \n  # Calculate values for all weeks for this participant\n  m_1 &lt;- alpha_i + beta_i * weeks + rnorm(length(weeks), 0, sigma)\n  \n  df &lt;- rbind.data.frame(df, data.frame(m = m_1, week = weeks, id = rep(i, times = length(weeks))))\n}\n\nLet’s plot it:\n\np &lt;- ggplot(df, aes(y = m, x = week, group = as.factor(id), color = as.factor(id))) + geom_point(aes(y = m), alpha = 0.8) + \n  geom_line(aes(y = m), alpha = 0.8) +\n  theme_minimal()\n\nprint(p)\n\n\n\n\n\n\n\n\n\n\nThe outcome (worry symptoms) we imagine will be affected by treatment in a similar way, decreasing linearly as patients spend time working with the treatment protocol, while being measured with some degree of error. Importantly the outcome will be partially affected by the mediator and partially will have exogenous influences.\nWe can expand the model to now instead describe two results \\(y_i\\) and \\(m_i\\). Since we model two “outcomes” we also get two residuals \\(\\sigma_y\\) and \\(\\sigma_m\\).\n\\[\n\\begin{align}\ny_i &\\sim N(\\mu_{y,i}, \\sigma_y) \\\\\nm_i &\\sim N(\\mu_{m,i}, \\sigma_m) \\\\\n\\mu_{y,i} &= \\alpha_{y,j[i]} + \\beta_{y,j[i]}t_i + \\gamma_{j[i]}(m_i-\\bar{m_j}) \\\\\n\\mu_{m,i} &= \\alpha_{m,j[i]} + \\beta_{m,j[i]}t_i \\\\\n\\begin{pmatrix} \\alpha_{m,j} \\\\ \\beta_{m,j} \\\\ \\alpha_{y,j} \\\\ \\beta_{y,j} \\\\ \\gamma_j \\end{pmatrix} &\\sim \\text{MVN}\\left(\\begin{pmatrix} \\mu_{\\alpha,m} \\\\ \\mu_{\\beta,m} \\\\ \\mu_{\\alpha,y} \\\\ \\mu_{\\beta,y} \\\\ \\mu_{\\gamma} \\end{pmatrix}, \\Sigma\\right) \\\\\n\\end{align}\n\\]\nA lot of things have happened in this step. Our alphas and betas now have subscripts that both relate them to either the outcome or the mediator. We’ve also added a \\(\\gamma_j\\) which describe the effect of the mediator. Note that this now assumes a constant mediator-outcome relationship for each participant \\(j\\). Note Not centering the parameter meant it basically pushed y values further from zero. Now while centered it instead represents some sort of average slope of the mediator (balancing on an axis in the middle of the dataset…) Constant in the sense that it doesn’t vary over time; the relationship is however allowed to vary by participant.\nImportantly we now have a joint covariance matrix \\(\\Sigma\\) that structures the multivariate normal distribution which all individual level parameters are drawn from.\nExpanded covariance matrix:\n\\[\n\\Sigma = \\begin{pmatrix}\n\\sigma_{\\alpha_m}^2 & \\rho_{\\alpha_m,\\beta_m}\\sigma_{\\alpha_m}\\sigma_{\\beta_m} & \\rho_{\\alpha_m,\\alpha_y}\\sigma_{\\alpha_m}\\sigma_{\\alpha_y} & \\rho_{\\alpha_m,\\beta_y}\\sigma_{\\alpha_m}\\sigma_{\\beta_y} & \\rho_{\\alpha_m,\\gamma}\\sigma_{\\alpha_m}\\sigma_{\\gamma} \\\\\n\\cdot & \\sigma_{\\beta_m}^2 & \\rho_{\\beta_m,\\alpha_y}\\sigma_{\\beta_m}\\sigma_{\\alpha_y} & \\rho_{\\beta_m,\\beta_y}\\sigma_{\\beta_m}\\sigma_{\\beta_y} & \\rho_{\\beta_m,\\gamma}\\sigma_{\\beta_m}\\sigma_{\\gamma} \\\\\n\\cdot & \\cdot & \\sigma_{\\alpha_y}^2 & \\rho_{\\alpha_y,\\beta_y}\\sigma_{\\alpha_y}\\sigma_{\\beta_y} & \\rho_{\\alpha_y,\\gamma}\\sigma_{\\alpha_y}\\sigma_{\\gamma} \\\\\n\\cdot & \\cdot & \\cdot & \\sigma_{\\beta_y}^2 & \\rho_{\\beta_y,\\gamma}\\sigma_{\\beta_y}\\sigma_{\\gamma} \\\\\n\\cdot & \\cdot & \\cdot & \\cdot & \\sigma_{\\gamma}^2\n\\end{pmatrix}\n\\]\nWhere \\(\\alpha_{m,j}, \\beta_{m,j}\\) are mediator intercept and slope for participant \\(j\\). \\(\\alpha_{y,j}, \\beta_{y,j}\\) are their outcome intercept and slope, \\(\\gamma_j\\) is their mediator-outcome relationship.\nThese slopes and intercepts drawn from a multivariate normal distribution. Like before \\(\\sigma_{\\alpha}\\) and \\(\\sigma_{\\beta}\\) describe the intercept standard deviation and heterogeneity of the change over time, but they now have subscripts that specify whether they belong to the mediator or the outcome. We describe heterogeneity of the mediator-outcome relationship with \\(\\sigma_{\\gamma}\\).\nWe now also have ten (!) \\(\\rho\\) terms that capture the correlations between individuals slopes and intercepts.\n\n\n\n\n\\(\\rho_{\\alpha_m,\\beta_m}\\) is the slope-intercept correlation that we previously defined in the model without the outcome. This should probably be slightly negative to reflect that people who are already at a high level in the mediator tend to have less room to get even worse (and vice versa). A “regression towards the mean”-like effect.\n\\(\\rho_{\\alpha_m,\\alpha_y}\\) is the correlation between the intercept of the mediator and the intercept of the outcome. This should be moderately positive to reflect that individuals with a high level of our mediating variable (e.g. intolerance of uncertainty) tend to be more worried (at the start of treatment).\n\\(\\rho_{\\alpha_m,\\beta_y}\\) is more conceptually tricky. This represents whether individuals high on the mediator at start of treatment tend to change more in the outcome. My stab at it would be that this should also be a slight negative correlation, but probably weaker than the slope-intercept correlation for the mediator itself.\n\\(\\rho_{\\alpha_m,\\gamma}\\) represents the relationship between an individuals mediator-outcome relationship, and their initial level of the mediator. For example, do individuals with a higher level of negative metacognitions beforehand also have a stronger overall relationship between their negative metacognitions and their worry? I’m not sure what to make of that. My first hunch is no; while there would be an overall relationship between the mediator and the outcome, \\(\\mu_\\gamma\\), it wouldn’t necessarily vary by initial negative metacognition level.\n\\(\\rho_{\\beta_m,\\alpha_y}\\) is similarly tricky but like \\(\\rho_{\\alpha_m,\\beta_y}\\) I would argue for “slightly negative but less so than within variable intercept-slope correlation”\n\\(\\rho_{\\beta_m,\\beta_y}\\) is potentially a very relevant parameter of interest. This should be positive. Individuals whose mediator change more should have outcomes that change more.\n\\(\\rho_{\\beta_m,\\gamma}\\) is also tricky. It represents how mediator slope relates to the mediator-outcome relationship, for a given participant. One could perhaps frame it as a type of individual differences in treatment response. Perhaps participants where the mediator changes more are the same participants where the mediator is strongly related to the outcome. Now that I type it out, that sounds pretty reasonable.\n\\(\\rho_{\\alpha_y,\\beta_y}\\) is the slope-intercept correlation for the outcome. In line with previous reasoning: probably slightly negative.\n\\(\\rho_{\\alpha_y,\\gamma}\\) is the relationship between initial worry and the mediator-outcome relationship. Since that’s the flipside of \\(\\rho_{\\alpha_m,\\gamma}\\) similar reasoning should hold.\n\\(\\rho_{\\beta_y,\\gamma}\\) is also tricky. It should probably be similar to \\(\\rho_{\\beta_m,\\gamma}\\), so that those whose outcome change more tend to have a stronger mediator-outcome relationship (since we’re imagining they change because of the changing mediator.)\n\nFinally we can complicate the model even further by adding a correlation between the residuals (\\(\\sigma_m\\) and \\(\\sigma_y\\)). The outcome is then instead a linear predictor \\(y_i = \\mu_{y,i} + \\epsilon_{y,i}\\) and \\(\\epsilon\\) comes from a multivariate normal distribution that contain both \\(\\sigma^2_y\\), \\(\\sigma^2_m\\) and their correlation \\(\\rho_{\\epsilon}\\sigma_m\\sigma_y\\).\nShould we posit such a relationship? I think it’s unclear. We’re then saying that despite all these things we’re describing, our \\(\\boldsymbol\\mu\\)s and our \\(\\rho\\)s and our \\(\\gamma\\)s, there’s still an additional, not yet captured, relationship between the mediator and the outcome.\nThe code below contains it, but I will set the relationship to zero when generating data.\n(Note. Unlike the previous code I haven’t done any checks on whether this fully works).\n\nn &lt;- 7 #note that the code calls nr patients \"n\" instead of j.\n\n# Parameters for the 5-dimensional MVN\nmu_alpha_m &lt;- 0        # specify mediator intercept mean\nmu_beta_m &lt;- -0.1      # specify mediator slope mean\nmu_alpha_y &lt;- 0        # specify outcome intercept mean\nmu_beta_y &lt;- -0.11     # specify outcome slope mean  \nmu_gamma &lt;- 0.8       # specify mediator-outcome relationship mean\n\n# Standard deviations for random effects\nsd_alpha_m &lt;- 1        # specify mediator intercept SD\nsd_beta_m &lt;- 0.06      # specify mediator slope SD\nsd_alpha_y &lt;- 0.95      # specify outcome intercept SD\nsd_beta_y &lt;- 0.04      # specify outcome slope SD\nsd_gamma &lt;- 0.2        # specify mediator-outcome relationship SD\n\n# Residual parameters\nsigma_m &lt;- 0.3\nsigma_y &lt;- 0.4\nrho_epsilon &lt;- 0     # residual correlation\n\nweeks &lt;- 0:9\n\n# Set up 5-dimensional multivariate normal parameters\nmu_vec &lt;- c(mu_alpha_m, mu_beta_m, mu_alpha_y, mu_beta_y, mu_gamma)\n\n# Create 5x5 covariance matrix (you can specify correlations as needed)\nrho_alpha_m_beta_m &lt;- -0.15    # mediator slope-intercept correlation\nrho_alpha_m_alpha_y &lt;- 0.9     # mediator-outcome intercept correlation\nrho_alpha_m_beta_y &lt;- -0.1     # mediator-intercept outcome-slope correlation\nrho_alpha_m_gamma &lt;- 0         # mediator intercept-mediator effect correlation\nrho_beta_m_alpha_y &lt;- 0        # mediator slope-outcome intercept correlation\nrho_beta_m_beta_y &lt;- 0.24      # mediator slope-outcome slope correlation\nrho_beta_m_gamma &lt;- 0.25       # mediator slope-mediator effect correlation\n\nrho_alpha_y_beta_y &lt;- - 0.16   # outcome slope-intercept correlation\nrho_alpha_y_gamma &lt;- 0         # outcome intercept-mediator effect correlation\nrho_beta_y_gamma &lt;- 0.20       # outcome slope-mediator effect correlation\n\nSigma_RE &lt;- matrix(0, nrow = 5, ncol = 5)\ndiag(Sigma_RE) &lt;- c(sd_alpha_m^2, sd_beta_m^2, sd_alpha_y^2, sd_beta_y^2, sd_gamma^2)\n\n# Fill in correlations (symmetric matrix)\nSigma_RE[1,2] &lt;- Sigma_RE[2,1] &lt;- rho_alpha_m_beta_m * sd_alpha_m * sd_beta_m\nSigma_RE[1,3] &lt;- Sigma_RE[3,1] &lt;- rho_alpha_m_alpha_y * sd_alpha_m * sd_alpha_y\nSigma_RE[1,4] &lt;- Sigma_RE[4,1] &lt;- rho_alpha_m_beta_y * sd_alpha_m * sd_beta_y\nSigma_RE[1,5] &lt;- Sigma_RE[5,1] &lt;- rho_alpha_m_gamma * sd_alpha_m * sd_gamma\n\nSigma_RE[2,3] &lt;- Sigma_RE[3,2] &lt;- rho_beta_m_alpha_y * sd_beta_m * sd_alpha_y\nSigma_RE[2,4] &lt;- Sigma_RE[4,2] &lt;- rho_beta_m_beta_y * sd_beta_m * sd_beta_y\nSigma_RE[2,5] &lt;- Sigma_RE[5,2] &lt;- rho_beta_m_gamma * sd_beta_m * sd_gamma\n\nSigma_RE[3,4] &lt;- Sigma_RE[4,3] &lt;- rho_alpha_y_beta_y * sd_alpha_y * sd_beta_y\nSigma_RE[3,5] &lt;- Sigma_RE[5,3] &lt;- rho_alpha_y_gamma * sd_alpha_y * sd_gamma\n\nSigma_RE[4,5] &lt;- Sigma_RE[5,4] &lt;- rho_beta_y_gamma * sd_beta_y * sd_gamma\n\n# Residual covariance matrix\nSigma_res &lt;- matrix(c(sigma_m^2, rho_epsilon * sigma_m * sigma_y,\n                      rho_epsilon * sigma_m * sigma_y, sigma_y^2), \n                    nrow = 2, ncol = 2)\n\ndf &lt;- data.frame(m = c(), y = c(), week = c(), id = c())\n\nfor(i in 1:n){\n  # Generate 5 correlated random effects for this participant\n  params &lt;- mvrnorm(1, mu = mu_vec, Sigma = Sigma_RE)\n  alpha_m_i &lt;- params[1]  # mediator intercept\n  beta_m_i &lt;- params[2]   # mediator slope\n  alpha_y_i &lt;- params[3]  # outcome intercept\n  beta_y_i &lt;- params[4]   # outcome slope\n  gamma_i &lt;- params[5]    # mediator-outcome relationship\n  \n  # Generate correlated residuals for all timepoints\n  residuals &lt;- mvrnorm(length(weeks), mu = c(0, 0), Sigma = Sigma_res)\n  epsilon_m &lt;- residuals[, 1]\n  epsilon_y &lt;- residuals[, 2]\n  \n  # Calculate mediator values\n  mu_m &lt;- alpha_m_i + beta_m_i * weeks\n  m_vals &lt;- mu_m + epsilon_m\n  \n    # CENTER THE MEDIATOR BY PARTICIPANT\n  m_centered &lt;- m_vals - mean(m_vals)\n  \n  # Calculate outcome values using CENTERED mediator\n  mu_y &lt;- alpha_y_i + beta_y_i * weeks + gamma_i * m_centered\n  y_vals &lt;- mu_y + epsilon_y\n  \n  df &lt;- rbind.data.frame(df, data.frame(m = m_vals, y = y_vals, week = weeks,\n                                        id = rep(i, times = length(weeks))))\n}\n\nMy aim for this code is that it can be used as a tool to test and make sense of different available methods for mediation analysis.\nFor now, let’s just plot both mediator and\n\np &lt;- ggplot(df, aes(x = week, group = as.factor(id), color = as.factor(id))) + geom_point(aes(y = m), alpha = 0.8) + \n  geom_line(aes(y = m), alpha = 0.8) +\n  geom_point(aes(y = y), alpha = 0.4) +\n  geom_line(aes(y = y), alpha = 0.4, linetype = \"dashed\") +\n  labs(color = \"ID\", y = \"Value\") +\n  theme_minimal()\n\nprint(p)\n\n\n\n\n\n\n\n\n\n\n\nNote. Not yet updated \\(\\gamma\\) to apply to centered \\(m\\)\n\\[\n\\begin{align}\ny_i &= \\mu_{y,i} + \\epsilon_{y,i} \\\\\nm_i &= \\mu_{m,i} + \\epsilon_{m,i} \\\\\n\\mu_{y,i} &= \\alpha_{y,j[i]} + \\beta_{y,j[i]}t_i + \\gamma_{j[i]}m_i \\\\\n\\mu_{m,i} &= \\alpha_{m,j[i]} + \\beta_{m,j[i]}t_i \\\\\n\\begin{pmatrix} \\alpha_{m,j} \\\\ \\beta_{m,j} \\\\ \\alpha_{y,j} \\\\ \\beta_{y,j} \\\\ \\gamma_j \\end{pmatrix} &\\sim \\text{MVN}\\left(\\begin{pmatrix} \\mu_{\\alpha,m} \\\\ \\mu_{\\beta,m} \\\\ \\mu_{\\alpha,y} \\\\ \\mu_{\\beta,y} \\\\ \\mu_{\\gamma} \\end{pmatrix}, \\Sigma_{RE}\\right) \\\\\n\\begin{pmatrix} \\epsilon_{m,i} \\\\ \\epsilon_{y,i} \\end{pmatrix} &\\sim \\text{MVN}\\left(\\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} \\sigma_m^2 & \\rho_{\\epsilon}\\sigma_m\\sigma_y \\\\ \\rho_{\\epsilon}\\sigma_m\\sigma_y & \\sigma_y^2 \\end{pmatrix}\\right)\n\\end{align}\n\\]"
  },
  {
    "objectID": "notes/mediation_sim_may_WIP.html#building-a-generative-model-step-by-step",
    "href": "notes/mediation_sim_may_WIP.html#building-a-generative-model-step-by-step",
    "title": "mediation_sim_may_WIP",
    "section": "",
    "text": "My goal here is to build a model that describes the essential aspects of the data-generating process that our statistical test will attempt to estimate.\nEach individual will be part of either the IU treatment group or the MC treatment group. For the duration of the treatment, the mediator is assumed to gradually change in an approximately linear way. Before considering the comparisons between groups, let’s first imagine a patient in one group with a changing mediator, \\(m^a\\). The residuals of the model \\(\\sigma\\) is assumed to be normally distributed with a mean of 0. \\(t\\) is a variable that represent time, which can be weeks from 0 to 9.\n\\[\nm_i \\sim N(\\mu_i,\\sigma) \\\\\n\\mu_i = \\alpha  +  \\beta t_i\n\\]\nFor now let’s just imagine the data-generating process for a single patient. When generating data parameters like \\(\\alpha\\) and \\(\\beta\\) will have “priors” that determine the distribution that data-points are drawn from. For our purposes now the input in those distribution functions are somewhat arbitrary, and they do not represent any thought through Bayesian priors. Because I’m not yet trying to model uncertainty, I’ll skip specifying \\(\\sigma\\) as a distribution, instead it will be set to = 0.3.\n\\[\n\\alpha \\sim N(0,1) \\\\\n\\beta \\sim N(-0.1,0.05)\n\\]\n\\(\\alpha\\) now contains the normal intercept (week = 0) standard deviation for the mediator, while \\(\\beta\\) represents the average slope of the mediator, as well as the heterogeneity of slopes. Let’s simulate and plot 1 patient:\n\nn &lt;- 1\nmediator_intercept_sd &lt;- 1\navg_mediator_slope &lt;- -0.1\nmediator_slope_sd &lt;- 0.05\nweeks &lt;- 0:9\nsigma &lt;- 0.3\n\nm_1 &lt;- rnorm(n, 0, mediator_intercept_sd) + rnorm(n, avg_mediator_slope, mediator_slope_sd)*weeks + rnorm((n*length(weeks)), 0, sigma)\n\nplot(weeks,m_1, ylim = c(-2,2))\n\n\n\n\n\n\n\n\nSome additional things to be explicit about: \\(m_i\\) is changing linearly, if not for \\(\\sigma\\). This \\(\\sigma\\) can be seen as representing both “measurement error” and exogenous influences. While our treatment is hypothesized to affect the mediator, it is not likely to be the only thing affecting the mediator. We also imagine that the change is heterogenous; the treatment is not expected to work equally for everyone.\nNow let’s model how this might look for many different patients \\(j=1,...,n\\). The value of a datapoint \\(\\mu_i\\) (if not for it’s error/exogenous influences) can now be given by this formula:\n\\[\nm_i \\sim N(\\mu_i,\\sigma) \\\\\n\\mu_{i} = \\alpha_{j[i]} \\ + \\ \\beta_{j[i]}t_i \\\\\n\\alpha_j \\sim N(0,1) \\\\\n\\beta_j \\sim N(-0.1,0.05) \\\\\n\\]\nThe only thing that has changed is that we’re now imagining that the slopes and intercepts beloing to many different patients. To generate this we could simply loop the code many times.\n\nn &lt;- 7 #note that the code calls nr patients \"n\" instead of j.\nmediator_intercept_sd &lt;- 1\navg_mediator_slope &lt;- -0.1\nmediator_slope_sd &lt;- 0.05\nweeks &lt;- 0:9\nsigma &lt;- 0.3\nrho &lt;- -0.15\n\ndf &lt;- data.frame(m = c(), week = c(), id = c())\n\nfor(i in 1:n){\nm_1 &lt;- rnorm(1, 0, mediator_intercept_sd) + rnorm(1, avg_mediator_slope, mediator_slope_sd)*weeks + rnorm((length(weeks)), 0, sigma)\ndf &lt;- rbind.data.frame(df,data.frame(m = m_1, week = weeks, id = rep(i, times = 10)))\n}\n\nIn real data it is common to see a negative slope-intercept correlation where a higher value on an individuals \\(\\alpha\\) would be associated with a more negative \\(\\beta\\). Our data-generating process should model this as well. To achieve this we need to let intercepts and slopes them be drawn from a multivariate normal distribution.\n\\[\n\\begin{align}\ny_i &\\sim N(\\mu_i,\\sigma) \\\\\n\\mu_i &= \\alpha_{j[i]} + \\beta_{j[i]}t_i \\\\\n\\begin{pmatrix} \\alpha_j \\\\ \\beta_j \\end{pmatrix} &\\sim \\text{MVN}\\left(\\begin{pmatrix} \\mu_\\alpha \\\\ \\mu_\\beta \\end{pmatrix}, \\Sigma\\right) \\\\\n\\mu_\\alpha &= 0 \\\\\n\\mu_\\beta &= -0.1 \\\\\n\\Sigma &= \\begin{pmatrix} \\sigma_\\alpha^2 & \\rho\\sigma_\\alpha\\sigma_\\beta \\\\ \\rho\\sigma_\\alpha\\sigma_\\beta & \\sigma_\\beta^2 \\end{pmatrix} \\\\\n\\rho &= -0.15 \\\\\n\\sigma_\\alpha &= 1 \\\\\n\\sigma_\\beta &= 0.05\n\\end{align}\n\\]\n\\(\\Sigma\\) now represent a common covariance matrix for the slopes and intercepts. The diagonal of the matrix represent the intercept and slope variances for earlier, but we now also have a \\(\\rho\\sigma_\\alpha \\sigma_\\beta\\) that represents the correlation. When generating data I’ve set it to -0.15.\nUpdating the code and looping for 7 patients looks like this:\n\n#Now with slope-intercept correlation\nn &lt;- 7 #note that the code calls nr patients \"n\" instead of j.\nmediator_intercept_sd &lt;- 1\navg_mediator_slope &lt;- -0.1\nmediator_slope_sd &lt;- 0.05\nweeks &lt;- 0:9\nsigma &lt;- 0.3\nrho &lt;- -0.15\n\n# Set up multivariate normal parameters\nmu &lt;- c(0, avg_mediator_slope)  # means for intercept and slope\nSigma &lt;- matrix(c(mediator_intercept_sd^2, \n                  rho * mediator_intercept_sd * mediator_slope_sd,\n                  rho * mediator_intercept_sd * mediator_slope_sd, \n                  mediator_slope_sd^2), \n                nrow = 2, ncol = 2)\n\ndf &lt;- data.frame(m = c(), week = c(), id = c())\n\nfor(i in 1:n){\n  # Generate correlated intercept and slope for this participant\n  params &lt;- MASS::mvrnorm(1, mu = mu, Sigma = Sigma)\n  alpha_i &lt;- params[1]  # intercept\n  beta_i &lt;- params[2]   # slope\n  \n  # Calculate values for all weeks for this participant\n  m_1 &lt;- alpha_i + beta_i * weeks + rnorm(length(weeks), 0, sigma)\n  \n  df &lt;- rbind.data.frame(df, data.frame(m = m_1, week = weeks, id = rep(i, times = length(weeks))))\n}\n\nLet’s plot it:\n\np &lt;- ggplot(df, aes(y = m, x = week, group = as.factor(id), color = as.factor(id))) + geom_point(aes(y = m), alpha = 0.8) + \n  geom_line(aes(y = m), alpha = 0.8) +\n  theme_minimal()\n\nprint(p)\n\n\n\n\n\n\n\n\n\n\nThe outcome (worry symptoms) we imagine will be affected by treatment in a similar way, decreasing linearly as patients spend time working with the treatment protocol, while being measured with some degree of error. Importantly the outcome will be partially affected by the mediator and partially will have exogenous influences.\nWe can expand the model to now instead describe two results \\(y_i\\) and \\(m_i\\). Since we model two “outcomes” we also get two residuals \\(\\sigma_y\\) and \\(\\sigma_m\\).\n\\[\n\\begin{align}\ny_i &\\sim N(\\mu_{y,i}, \\sigma_y) \\\\\nm_i &\\sim N(\\mu_{m,i}, \\sigma_m) \\\\\n\\mu_{y,i} &= \\alpha_{y,j[i]} + \\beta_{y,j[i]}t_i + \\gamma_{j[i]}(m_i-\\bar{m_j}) \\\\\n\\mu_{m,i} &= \\alpha_{m,j[i]} + \\beta_{m,j[i]}t_i \\\\\n\\begin{pmatrix} \\alpha_{m,j} \\\\ \\beta_{m,j} \\\\ \\alpha_{y,j} \\\\ \\beta_{y,j} \\\\ \\gamma_j \\end{pmatrix} &\\sim \\text{MVN}\\left(\\begin{pmatrix} \\mu_{\\alpha,m} \\\\ \\mu_{\\beta,m} \\\\ \\mu_{\\alpha,y} \\\\ \\mu_{\\beta,y} \\\\ \\mu_{\\gamma} \\end{pmatrix}, \\Sigma\\right) \\\\\n\\end{align}\n\\]\nA lot of things have happened in this step. Our alphas and betas now have subscripts that both relate them to either the outcome or the mediator. We’ve also added a \\(\\gamma_j\\) which describe the effect of the mediator. Note that this now assumes a constant mediator-outcome relationship for each participant \\(j\\). Note Not centering the parameter meant it basically pushed y values further from zero. Now while centered it instead represents some sort of average slope of the mediator (balancing on an axis in the middle of the dataset…) Constant in the sense that it doesn’t vary over time; the relationship is however allowed to vary by participant.\nImportantly we now have a joint covariance matrix \\(\\Sigma\\) that structures the multivariate normal distribution which all individual level parameters are drawn from.\nExpanded covariance matrix:\n\\[\n\\Sigma = \\begin{pmatrix}\n\\sigma_{\\alpha_m}^2 & \\rho_{\\alpha_m,\\beta_m}\\sigma_{\\alpha_m}\\sigma_{\\beta_m} & \\rho_{\\alpha_m,\\alpha_y}\\sigma_{\\alpha_m}\\sigma_{\\alpha_y} & \\rho_{\\alpha_m,\\beta_y}\\sigma_{\\alpha_m}\\sigma_{\\beta_y} & \\rho_{\\alpha_m,\\gamma}\\sigma_{\\alpha_m}\\sigma_{\\gamma} \\\\\n\\cdot & \\sigma_{\\beta_m}^2 & \\rho_{\\beta_m,\\alpha_y}\\sigma_{\\beta_m}\\sigma_{\\alpha_y} & \\rho_{\\beta_m,\\beta_y}\\sigma_{\\beta_m}\\sigma_{\\beta_y} & \\rho_{\\beta_m,\\gamma}\\sigma_{\\beta_m}\\sigma_{\\gamma} \\\\\n\\cdot & \\cdot & \\sigma_{\\alpha_y}^2 & \\rho_{\\alpha_y,\\beta_y}\\sigma_{\\alpha_y}\\sigma_{\\beta_y} & \\rho_{\\alpha_y,\\gamma}\\sigma_{\\alpha_y}\\sigma_{\\gamma} \\\\\n\\cdot & \\cdot & \\cdot & \\sigma_{\\beta_y}^2 & \\rho_{\\beta_y,\\gamma}\\sigma_{\\beta_y}\\sigma_{\\gamma} \\\\\n\\cdot & \\cdot & \\cdot & \\cdot & \\sigma_{\\gamma}^2\n\\end{pmatrix}\n\\]\nWhere \\(\\alpha_{m,j}, \\beta_{m,j}\\) are mediator intercept and slope for participant \\(j\\). \\(\\alpha_{y,j}, \\beta_{y,j}\\) are their outcome intercept and slope, \\(\\gamma_j\\) is their mediator-outcome relationship.\nThese slopes and intercepts drawn from a multivariate normal distribution. Like before \\(\\sigma_{\\alpha}\\) and \\(\\sigma_{\\beta}\\) describe the intercept standard deviation and heterogeneity of the change over time, but they now have subscripts that specify whether they belong to the mediator or the outcome. We describe heterogeneity of the mediator-outcome relationship with \\(\\sigma_{\\gamma}\\).\nWe now also have ten (!) \\(\\rho\\) terms that capture the correlations between individuals slopes and intercepts.\n\n\n\n\n\\(\\rho_{\\alpha_m,\\beta_m}\\) is the slope-intercept correlation that we previously defined in the model without the outcome. This should probably be slightly negative to reflect that people who are already at a high level in the mediator tend to have less room to get even worse (and vice versa). A “regression towards the mean”-like effect.\n\\(\\rho_{\\alpha_m,\\alpha_y}\\) is the correlation between the intercept of the mediator and the intercept of the outcome. This should be moderately positive to reflect that individuals with a high level of our mediating variable (e.g. intolerance of uncertainty) tend to be more worried (at the start of treatment).\n\\(\\rho_{\\alpha_m,\\beta_y}\\) is more conceptually tricky. This represents whether individuals high on the mediator at start of treatment tend to change more in the outcome. My stab at it would be that this should also be a slight negative correlation, but probably weaker than the slope-intercept correlation for the mediator itself.\n\\(\\rho_{\\alpha_m,\\gamma}\\) represents the relationship between an individuals mediator-outcome relationship, and their initial level of the mediator. For example, do individuals with a higher level of negative metacognitions beforehand also have a stronger overall relationship between their negative metacognitions and their worry? I’m not sure what to make of that. My first hunch is no; while there would be an overall relationship between the mediator and the outcome, \\(\\mu_\\gamma\\), it wouldn’t necessarily vary by initial negative metacognition level.\n\\(\\rho_{\\beta_m,\\alpha_y}\\) is similarly tricky but like \\(\\rho_{\\alpha_m,\\beta_y}\\) I would argue for “slightly negative but less so than within variable intercept-slope correlation”\n\\(\\rho_{\\beta_m,\\beta_y}\\) is potentially a very relevant parameter of interest. This should be positive. Individuals whose mediator change more should have outcomes that change more.\n\\(\\rho_{\\beta_m,\\gamma}\\) is also tricky. It represents how mediator slope relates to the mediator-outcome relationship, for a given participant. One could perhaps frame it as a type of individual differences in treatment response. Perhaps participants where the mediator changes more are the same participants where the mediator is strongly related to the outcome. Now that I type it out, that sounds pretty reasonable.\n\\(\\rho_{\\alpha_y,\\beta_y}\\) is the slope-intercept correlation for the outcome. In line with previous reasoning: probably slightly negative.\n\\(\\rho_{\\alpha_y,\\gamma}\\) is the relationship between initial worry and the mediator-outcome relationship. Since that’s the flipside of \\(\\rho_{\\alpha_m,\\gamma}\\) similar reasoning should hold.\n\\(\\rho_{\\beta_y,\\gamma}\\) is also tricky. It should probably be similar to \\(\\rho_{\\beta_m,\\gamma}\\), so that those whose outcome change more tend to have a stronger mediator-outcome relationship (since we’re imagining they change because of the changing mediator.)\n\nFinally we can complicate the model even further by adding a correlation between the residuals (\\(\\sigma_m\\) and \\(\\sigma_y\\)). The outcome is then instead a linear predictor \\(y_i = \\mu_{y,i} + \\epsilon_{y,i}\\) and \\(\\epsilon\\) comes from a multivariate normal distribution that contain both \\(\\sigma^2_y\\), \\(\\sigma^2_m\\) and their correlation \\(\\rho_{\\epsilon}\\sigma_m\\sigma_y\\).\nShould we posit such a relationship? I think it’s unclear. We’re then saying that despite all these things we’re describing, our \\(\\boldsymbol\\mu\\)s and our \\(\\rho\\)s and our \\(\\gamma\\)s, there’s still an additional, not yet captured, relationship between the mediator and the outcome.\nThe code below contains it, but I will set the relationship to zero when generating data.\n(Note. Unlike the previous code I haven’t done any checks on whether this fully works).\n\nn &lt;- 7 #note that the code calls nr patients \"n\" instead of j.\n\n# Parameters for the 5-dimensional MVN\nmu_alpha_m &lt;- 0        # specify mediator intercept mean\nmu_beta_m &lt;- -0.1      # specify mediator slope mean\nmu_alpha_y &lt;- 0        # specify outcome intercept mean\nmu_beta_y &lt;- -0.11     # specify outcome slope mean  \nmu_gamma &lt;- 0.8       # specify mediator-outcome relationship mean\n\n# Standard deviations for random effects\nsd_alpha_m &lt;- 1        # specify mediator intercept SD\nsd_beta_m &lt;- 0.06      # specify mediator slope SD\nsd_alpha_y &lt;- 0.95      # specify outcome intercept SD\nsd_beta_y &lt;- 0.04      # specify outcome slope SD\nsd_gamma &lt;- 0.2        # specify mediator-outcome relationship SD\n\n# Residual parameters\nsigma_m &lt;- 0.3\nsigma_y &lt;- 0.4\nrho_epsilon &lt;- 0     # residual correlation\n\nweeks &lt;- 0:9\n\n# Set up 5-dimensional multivariate normal parameters\nmu_vec &lt;- c(mu_alpha_m, mu_beta_m, mu_alpha_y, mu_beta_y, mu_gamma)\n\n# Create 5x5 covariance matrix (you can specify correlations as needed)\nrho_alpha_m_beta_m &lt;- -0.15    # mediator slope-intercept correlation\nrho_alpha_m_alpha_y &lt;- 0.9     # mediator-outcome intercept correlation\nrho_alpha_m_beta_y &lt;- -0.1     # mediator-intercept outcome-slope correlation\nrho_alpha_m_gamma &lt;- 0         # mediator intercept-mediator effect correlation\nrho_beta_m_alpha_y &lt;- 0        # mediator slope-outcome intercept correlation\nrho_beta_m_beta_y &lt;- 0.24      # mediator slope-outcome slope correlation\nrho_beta_m_gamma &lt;- 0.25       # mediator slope-mediator effect correlation\n\nrho_alpha_y_beta_y &lt;- - 0.16   # outcome slope-intercept correlation\nrho_alpha_y_gamma &lt;- 0         # outcome intercept-mediator effect correlation\nrho_beta_y_gamma &lt;- 0.20       # outcome slope-mediator effect correlation\n\nSigma_RE &lt;- matrix(0, nrow = 5, ncol = 5)\ndiag(Sigma_RE) &lt;- c(sd_alpha_m^2, sd_beta_m^2, sd_alpha_y^2, sd_beta_y^2, sd_gamma^2)\n\n# Fill in correlations (symmetric matrix)\nSigma_RE[1,2] &lt;- Sigma_RE[2,1] &lt;- rho_alpha_m_beta_m * sd_alpha_m * sd_beta_m\nSigma_RE[1,3] &lt;- Sigma_RE[3,1] &lt;- rho_alpha_m_alpha_y * sd_alpha_m * sd_alpha_y\nSigma_RE[1,4] &lt;- Sigma_RE[4,1] &lt;- rho_alpha_m_beta_y * sd_alpha_m * sd_beta_y\nSigma_RE[1,5] &lt;- Sigma_RE[5,1] &lt;- rho_alpha_m_gamma * sd_alpha_m * sd_gamma\n\nSigma_RE[2,3] &lt;- Sigma_RE[3,2] &lt;- rho_beta_m_alpha_y * sd_beta_m * sd_alpha_y\nSigma_RE[2,4] &lt;- Sigma_RE[4,2] &lt;- rho_beta_m_beta_y * sd_beta_m * sd_beta_y\nSigma_RE[2,5] &lt;- Sigma_RE[5,2] &lt;- rho_beta_m_gamma * sd_beta_m * sd_gamma\n\nSigma_RE[3,4] &lt;- Sigma_RE[4,3] &lt;- rho_alpha_y_beta_y * sd_alpha_y * sd_beta_y\nSigma_RE[3,5] &lt;- Sigma_RE[5,3] &lt;- rho_alpha_y_gamma * sd_alpha_y * sd_gamma\n\nSigma_RE[4,5] &lt;- Sigma_RE[5,4] &lt;- rho_beta_y_gamma * sd_beta_y * sd_gamma\n\n# Residual covariance matrix\nSigma_res &lt;- matrix(c(sigma_m^2, rho_epsilon * sigma_m * sigma_y,\n                      rho_epsilon * sigma_m * sigma_y, sigma_y^2), \n                    nrow = 2, ncol = 2)\n\ndf &lt;- data.frame(m = c(), y = c(), week = c(), id = c())\n\nfor(i in 1:n){\n  # Generate 5 correlated random effects for this participant\n  params &lt;- mvrnorm(1, mu = mu_vec, Sigma = Sigma_RE)\n  alpha_m_i &lt;- params[1]  # mediator intercept\n  beta_m_i &lt;- params[2]   # mediator slope\n  alpha_y_i &lt;- params[3]  # outcome intercept\n  beta_y_i &lt;- params[4]   # outcome slope\n  gamma_i &lt;- params[5]    # mediator-outcome relationship\n  \n  # Generate correlated residuals for all timepoints\n  residuals &lt;- mvrnorm(length(weeks), mu = c(0, 0), Sigma = Sigma_res)\n  epsilon_m &lt;- residuals[, 1]\n  epsilon_y &lt;- residuals[, 2]\n  \n  # Calculate mediator values\n  mu_m &lt;- alpha_m_i + beta_m_i * weeks\n  m_vals &lt;- mu_m + epsilon_m\n  \n    # CENTER THE MEDIATOR BY PARTICIPANT\n  m_centered &lt;- m_vals - mean(m_vals)\n  \n  # Calculate outcome values using CENTERED mediator\n  mu_y &lt;- alpha_y_i + beta_y_i * weeks + gamma_i * m_centered\n  y_vals &lt;- mu_y + epsilon_y\n  \n  df &lt;- rbind.data.frame(df, data.frame(m = m_vals, y = y_vals, week = weeks,\n                                        id = rep(i, times = length(weeks))))\n}\n\nMy aim for this code is that it can be used as a tool to test and make sense of different available methods for mediation analysis.\nFor now, let’s just plot both mediator and\n\np &lt;- ggplot(df, aes(x = week, group = as.factor(id), color = as.factor(id))) + geom_point(aes(y = m), alpha = 0.8) + \n  geom_line(aes(y = m), alpha = 0.8) +\n  geom_point(aes(y = y), alpha = 0.4) +\n  geom_line(aes(y = y), alpha = 0.4, linetype = \"dashed\") +\n  labs(color = \"ID\", y = \"Value\") +\n  theme_minimal()\n\nprint(p)\n\n\n\n\n\n\n\n\n\n\n\nNote. Not yet updated \\(\\gamma\\) to apply to centered \\(m\\)\n\\[\n\\begin{align}\ny_i &= \\mu_{y,i} + \\epsilon_{y,i} \\\\\nm_i &= \\mu_{m,i} + \\epsilon_{m,i} \\\\\n\\mu_{y,i} &= \\alpha_{y,j[i]} + \\beta_{y,j[i]}t_i + \\gamma_{j[i]}m_i \\\\\n\\mu_{m,i} &= \\alpha_{m,j[i]} + \\beta_{m,j[i]}t_i \\\\\n\\begin{pmatrix} \\alpha_{m,j} \\\\ \\beta_{m,j} \\\\ \\alpha_{y,j} \\\\ \\beta_{y,j} \\\\ \\gamma_j \\end{pmatrix} &\\sim \\text{MVN}\\left(\\begin{pmatrix} \\mu_{\\alpha,m} \\\\ \\mu_{\\beta,m} \\\\ \\mu_{\\alpha,y} \\\\ \\mu_{\\beta,y} \\\\ \\mu_{\\gamma} \\end{pmatrix}, \\Sigma_{RE}\\right) \\\\\n\\begin{pmatrix} \\epsilon_{m,i} \\\\ \\epsilon_{y,i} \\end{pmatrix} &\\sim \\text{MVN}\\left(\\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} \\sigma_m^2 & \\rho_{\\epsilon}\\sigma_m\\sigma_y \\\\ \\rho_{\\epsilon}\\sigma_m\\sigma_y & \\sigma_y^2 \\end{pmatrix}\\right)\n\\end{align}\n\\]"
  },
  {
    "objectID": "notes/Stats2_seminar_2.html",
    "href": "notes/Stats2_seminar_2.html",
    "title": "Stats2_seminar_2",
    "section": "",
    "text": "You test an individual M’s ability to solve a certain class of logical problems. M gave answers to nine versions of the problem and you coded each as correct (1) or incorrect (0). This is the data:\n1, 0, 0, 0, 0, 1, 0, 0, 0\nYou want to estimate M’s true ability ( p ) using this model: \\[\n\\begin{align}\ny_i &\\sim Bernoulli(p) \\\\\np &\\sim Uniform(0,1) \\\\\n\\end{align}\n\\]\n\nList the critical assumptions of this model.\nUse grid approximation to derive point estimate and 95 % Compatibility Interval (CI, defined as a highest density interval).\nVerify that you obtain the same result using the binomial distribution with \\(n = 9\\), that is, equal to the number of trials (this is McElreath’s approach to his globe tossing data).\n\nAnswer Uniform prior. Independent successes. \\(p\\) is constant (all trials are equally hard).\n\nlibrary(Rlab)\n\nRlab 4.0 attached.\n\n\n\nAttaching package: 'Rlab'\n\n\nThe following objects are masked from 'package:stats':\n\n    dexp, dgamma, dweibull, pexp, pgamma, pweibull, qexp, qgamma,\n    qweibull, rexp, rgamma, rweibull\n\n\nThe following object is masked from 'package:datasets':\n\n    precip\n\np_grid &lt;- seq(from = 0, to = 1, length.out = 1000)\nprior &lt;- rep(1, times = 1000) #we can also use dbeta(p_grid,1,1)\nlikelihood &lt;- dbinom(2, size = 9, prob = p_grid)#dbern(c(1, 0, 0, 0, 0, 1, 0, 0, 0),prob = p_grid)\nposterior &lt;- likelihood * prior\nplot(p_grid,posterior, type = \"l\"); abline(v = which.max(posterior)/1000)\nd &lt;- data.frame(\n  posterior,\n  p_grid,\n  posterior_90 = posterior &gt; quantile(posterior, 0.1)\n)\nd[min(which(d$posterior_90 == TRUE)),]$p_grid #THERE WE GO YES\n\n[1] 0.001001001\n\nd[max(which(d$posterior_90 == TRUE)),]$p_grid #HAHA YES!\n\n[1] 0.9009009\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\n\n\n\n\n\n\nggplot(d, aes(x = p_grid, y = posterior, color = posterior_90)) +\n  geom_point(size = 0.02, alpha = 0.5) #ugly\n\n\n\n\n\n\n\n\nI don’t know how to compute HPDI from grid really… I guess you could use quantile() and then select based on that. I work it out"
  },
  {
    "objectID": "notes/Stats2_seminar_2.html#mn3",
    "href": "notes/Stats2_seminar_2.html#mn3",
    "title": "Stats2_seminar_2",
    "section": "",
    "text": "You test an individual M’s ability to solve a certain class of logical problems. M gave answers to nine versions of the problem and you coded each as correct (1) or incorrect (0). This is the data:\n1, 0, 0, 0, 0, 1, 0, 0, 0\nYou want to estimate M’s true ability ( p ) using this model: \\[\n\\begin{align}\ny_i &\\sim Bernoulli(p) \\\\\np &\\sim Uniform(0,1) \\\\\n\\end{align}\n\\]\n\nList the critical assumptions of this model.\nUse grid approximation to derive point estimate and 95 % Compatibility Interval (CI, defined as a highest density interval).\nVerify that you obtain the same result using the binomial distribution with \\(n = 9\\), that is, equal to the number of trials (this is McElreath’s approach to his globe tossing data).\n\nAnswer Uniform prior. Independent successes. \\(p\\) is constant (all trials are equally hard).\n\nlibrary(Rlab)\n\nRlab 4.0 attached.\n\n\n\nAttaching package: 'Rlab'\n\n\nThe following objects are masked from 'package:stats':\n\n    dexp, dgamma, dweibull, pexp, pgamma, pweibull, qexp, qgamma,\n    qweibull, rexp, rgamma, rweibull\n\n\nThe following object is masked from 'package:datasets':\n\n    precip\n\np_grid &lt;- seq(from = 0, to = 1, length.out = 1000)\nprior &lt;- rep(1, times = 1000) #we can also use dbeta(p_grid,1,1)\nlikelihood &lt;- dbinom(2, size = 9, prob = p_grid)#dbern(c(1, 0, 0, 0, 0, 1, 0, 0, 0),prob = p_grid)\nposterior &lt;- likelihood * prior\nplot(p_grid,posterior, type = \"l\"); abline(v = which.max(posterior)/1000)\nd &lt;- data.frame(\n  posterior,\n  p_grid,\n  posterior_90 = posterior &gt; quantile(posterior, 0.1)\n)\nd[min(which(d$posterior_90 == TRUE)),]$p_grid #THERE WE GO YES\n\n[1] 0.001001001\n\nd[max(which(d$posterior_90 == TRUE)),]$p_grid #HAHA YES!\n\n[1] 0.9009009\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\n\n\n\n\n\n\nggplot(d, aes(x = p_grid, y = posterior, color = posterior_90)) +\n  geom_point(size = 0.02, alpha = 0.5) #ugly\n\n\n\n\n\n\n\n\nI don’t know how to compute HPDI from grid really… I guess you could use quantile() and then select based on that. I work it out"
  },
  {
    "objectID": "notes/Stats2_seminar_2.html#sr2m1",
    "href": "notes/Stats2_seminar_2.html#sr2m1",
    "title": "Stats2_seminar_2",
    "section": "SR2m1",
    "text": "SR2m1\nRecall the globe tossing model from the chapter. Compute and plot the grid approximate posterior distribution for each of the following sets of observations. In each case, assume a uniform prior for p. (1) W,W,W (2) W,W,W,L (3) L,W,W,L,W,W,W\nLet’s try to trecreate the grid approximation method as a challenge:\n\ng &lt;- 20 #for granularity\n#A\nprior &lt;- rep(1, times = g)\ngrid &lt;- seq(from = 0, to = 1, length.out = g)\nlikelihood &lt;- dbinom(3, size = 3, prob = grid)\nposterior_unstandardized &lt;- prior*likelihood\nposterior &lt;- posterior_unstandardized / sum(posterior_unstandardized) #normalize\nplot(grid,posterior, type = \"l\", main = \"W,W,W\")\n\n\n\n\n\n\n\n#B\nprior &lt;- rep(1, times = g)\ngrid &lt;- seq(from = 0, to = 1, length.out = g)\nlikelihood &lt;- dbinom(3, size = 4, prob = grid)\nposterior_unstandardized &lt;- prior*likelihood\nposterior &lt;- posterior_unstandardized / sum(posterior_unstandardized) #normalize\nplot(grid,posterior, type = \"l\", main = \"W,W,W,L\")\n\n\n\n\n\n\n\n#C\nprior &lt;- rep(1, times = g)\ngrid &lt;- seq(from = 0, to = 1, length.out = g)\nlikelihood &lt;- dbinom(5, size = 7, prob = grid)\nposterior_unstandardized &lt;- prior*likelihood\nposterior &lt;- posterior_unstandardized / sum(posterior_unstandardized) #normalize\nplot(grid,posterior, type = \"l\", main = \"L,W,W,L,W,W,W\")"
  },
  {
    "objectID": "notes/Stats2_seminar_2.html#sr2m2",
    "href": "notes/Stats2_seminar_2.html#sr2m2",
    "title": "Stats2_seminar_2",
    "section": "SR2m2",
    "text": "SR2m2\nNow assume a prior for p that is equal to zero when p &lt; 0.5 and is a positive constant when p ≥ 0.5. Again compute and plot the grid approximate posterior distribution for each of the sets of observations in the problem just above.\n\ng &lt;- 100\ngrid &lt;- seq(from = 0, to = 1, length.out = g)\nprior &lt;- (grid &lt; 0.5) == FALSE #actually should be *2 to be a proper pdf\n\nlikelihood &lt;- dbinom(3, size = 3, prob = grid)\nposterior_unstandardized &lt;- prior*likelihood\nposterior &lt;- posterior_unstandardized / sum(posterior_unstandardized) #normalize\nplot(grid,posterior, type = \"l\", main = \"W,W,W\")\n\n\n\n\n\n\n\nlikelihood &lt;- dbinom(3, size = 4, prob = grid)\nposterior_unstandardized &lt;- prior*likelihood\nposterior &lt;- posterior_unstandardized / sum(posterior_unstandardized) #normalize\nplot(grid,posterior, type = \"l\", main = \"W,W,W,L\")\n\n\n\n\n\n\n\nlikelihood &lt;- dbinom(5, size = 7, prob = grid)\nposterior_unstandardized &lt;- prior*likelihood\nposterior &lt;- posterior_unstandardized / sum(posterior_unstandardized) #normalize\nplot(grid,posterior, type = \"l\", main = \"L,W,W,L,W,W,W\")"
  },
  {
    "objectID": "notes/Stats2_seminar_2.html#sr3e3",
    "href": "notes/Stats2_seminar_2.html#sr3e3",
    "title": "Stats2_seminar_2",
    "section": "SR3e3",
    "text": "SR3e3\nHow much posterior probability lies between p = 0.2 and p = 0.8?\n\np_grid &lt;- seq( from=0 , to=1 , length.out=1000 )\n prior &lt;- rep( 1 , 1000 )\n likelihood &lt;- dbinom( 6 , size=9 , prob=p_grid )\n posterior &lt;- likelihood * prior\n posterior &lt;- posterior / sum(posterior)\n set.seed(100)\n samples &lt;- sample( p_grid , prob=posterior , size=1e4 , replace=TRUE )\n \nlength(samples[samples &lt; 0.2 | samples &lt; 0.8])/10000 #mean() works too.\n\n[1] 0.8884\n\n#To get the exact distribution. Use the beta-function and math it."
  },
  {
    "objectID": "notes/Stats2_seminar_2.html#sr3e4",
    "href": "notes/Stats2_seminar_2.html#sr3e4",
    "title": "Stats2_seminar_2",
    "section": "SR3e4",
    "text": "SR3e4\n20% of the posterior probability lies below which value of p?\n\nsort(samples)[2000] \n\n[1] 0.5185185\n\nplot(sort(samples)[1:2000],seq(from = 0, to = 0.2, length.out = 2000))\n\n\n\n\n\n\n\nlibrary(rethinking)\n\nLoading required package: cmdstanr\n\n\nThis is cmdstanr version 0.7.1\n\n\n- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n\n\n- CmdStan path: C:/Users/vilgo/Documents/.cmdstan/cmdstan-2.33.1\n\n\n- CmdStan version: 2.33.1\n\n\n\nA newer version of CmdStan is available. See ?install_cmdstan() to install it.\nTo disable this check set option or environment variable CMDSTANR_NO_VER_CHECK=TRUE.\n\n\nLoading required package: posterior\n\n\nWarning: package 'posterior' was built under R version 4.3.3\n\n\nThis is posterior version 1.6.1\n\n\n\nAttaching package: 'posterior'\n\n\nThe following objects are masked from 'package:stats':\n\n    mad, sd, var\n\n\nThe following objects are masked from 'package:base':\n\n    %in%, match\n\n\nLoading required package: parallel\n\n\nrethinking (Version 2.42)\n\n\n\nAttaching package: 'rethinking'\n\n\nThe following objects are masked from 'package:Rlab':\n\n    dbern, rbern\n\n\nThe following object is masked from 'package:stats':\n\n    rstudent\n\ndens(samples)\n\n\n\n\n\n\n\n#hist(sort(samples)[2000], breaks = 1000) #don't really get why this doesn't work..."
  },
  {
    "objectID": "notes/Stats2_seminar_2.html#sr3e6",
    "href": "notes/Stats2_seminar_2.html#sr3e6",
    "title": "Stats2_seminar_2",
    "section": "SR3e6",
    "text": "SR3e6\nWhich values of p contain the narrowest interval equal to 66% of the posterior probability?\n\nrethinking::HPDI(samples, 0.66)\n\n    |0.66     0.66| \n0.5085085 0.7737738"
  },
  {
    "objectID": "notes/Stats2_seminar_2.html#m6.",
    "href": "notes/Stats2_seminar_2.html#m6.",
    "title": "Stats2_seminar_2",
    "section": "3M6.",
    "text": "3M6.\nSuppose you want to estimate the Earth’s proportion of water very precisely. Specifically, you want the 99% percentile interval of the posterior distribution of p to be only 0.05 wide. This means the distance between the upper and lower bound of the interval should be 0.05. How many times will you have to toss the globe to do this?\nFirst guess There should be some variance in the n required depending on how your draws end up, right? In simulation this would be “simulation variance” while in sampling it would be sampling variance. My guess is that this should turn out quite similar to normal standard errors, since we have a flat prior. If \\(\\hat{p}\\) is the true proportion and \\(x\\) is the proportion we find in our sample of \\(n\\) individuals, the standard error is \\(SE=\\sqrt{\\hat{p}(1-\\hat{p})/n}\\), we can see here that \\(\\hat{p}(1-\\hat{p})\\), meaning that the required \\(n\\) should vary depending on what \\(\\hat{p}\\) is.\n\n\n\n\n\n\n\n\n\nThis sort of makes sense if we think of a small sample and very high proportion. Three “W,W,W” would all make the distribution only thinner (taller peak), while then adding a “L” would both make it some mix of taller and wider. Visually at least this feels true to me…\nAnyways, to solve for a given \\(\\hat{p}\\) we just need to plug in numbers to the equation and solve for \\(n\\). So for example for \\(\\hat{p}=0.4\\): \\[\n\\begin{align}\n0.05&=2.57*\\sqrt{{0.4}(1-{0.4})/n} \\\\\n0.05&=2.57*\\sqrt{0.24/n} \\\\\n0.05^2&=2.57^2*\\sqrt{0.24/n}^2 \\\\\n0.0025&=6.6049*0.24/n \\\\\n0.0025&=1.585176/n \\\\\n0.0025n&=1.585176 \\\\\nn&=1.585176/0.0025 \\\\\nn &\\approx 634?\n\\end{align}\n\\]\n2.57 is \\(\\approx\\) the z-score in a normal distribution, so we’re using it to get from SE to CI, which may be questionable in this case since with high or low proportions the CI may exceed 1. We can find exact number with ´qnorm(0.995)´. We’re assuming our error is normal.\nAlso 99% percentile intervals may be pretty different from 99%CI.\nAlso, as shown in the graph above, this equation may turn out quite different for more skewed proportions. Also: I’m not that sure I’ve gotten the simple middle school algebra right.\nSecond attempt\nLet’s try to code the problem with a loop instead:\n\n\n\n\n\n\n\n\n\n[1] 0.487\n\n\nHowever it’s different if p is very skewed.\n\n\n[1] 0.049\n\n\n\n\n\n\n\n\n\n[1] 0.985\n\n\nNotably it also turns out quite different than my questionable algebraic solution.\nAlso: Turns out there’s a function in rethinking that does this. PI(x,prob) if I use samples.\n\nsamp &lt;- sample(plausibility$grid, size = 1000, prob = plausibility$prior, replace = TRUE)\nrethinking::PI(samp, prob = 0.99)\n\n       1%      100% \n0.9499499 0.9959960"
  },
  {
    "objectID": "notes/Stats2_seminar_4.html",
    "href": "notes/Stats2_seminar_4.html",
    "title": "Stats2_seminar_4",
    "section": "",
    "text": "Use McElreath’s dataset Howell2 restricted to adults (age &gt; 18). Fit two linear models using quadratic approximation, quap():\n\nWeight as outcome, with two predictors: sex as an indicator variable and age as a continuous variable.\nAs above, but allow for a curvilinear effect of age, by adding age-squared to the model. Center the age variable around its mean.\n\nFor the first model, motivate priors based on a prior predictive simulation (cf. Fig. 4.5 of the Book). Then fit the model and report regression coefficients with compatibility intervals, and illustrate model predictions in a plot of the data (weight as a function of non-centered age) with separate prediction lines for women and men. Include compatibility regions around fitted lines (e.g., as in Fig. 5.1 of the book) or add samples of fitted lines (e.g., as in Fig. 4.7).\n\ndata(\"Howell1\")\nd &lt;- Howell1\nd &lt;- d[d$age &gt; 18,]\nxbar &lt;- mean(d$age)\nd$sex &lt;- ifelse(d$male == 1, 2,1)\n\n#1. first model, adding age and sex\nmdl1 &lt;- quap(\n  alist(\n    weight ~ dnorm(mu,sigma),\n    mu &lt;- a[sex] + beta_age*(age-xbar),\n    a[sex] ~ dnorm(178,20), #this is a common prior. With dummy variables parametrised we could two priors instead.\n    beta_age ~ dnorm(0,1),\n    sigma ~ dexp(1)\n  ), data = d\n)\nprecis(mdl1, depth = 2)\n\n                mean         sd       5.5%       94.5%\na[1]     41.86609093 0.39235827 41.2390266 42.49315522\na[2]     48.68782608 0.41332269 48.0272566 49.34839556\nbeta_age -0.08067284 0.01802636 -0.1094824 -0.05186323\nsigma     5.29371947 0.19898087  4.9757096  5.61172934\n\n#plot it\ncoffs &lt;- precis(mdl1, depth = 2)[,1]\n{plot(weight ~ age, data = d)\ncurve(from = -30, to = 90, coffs[1] + coffs[3]*(x-xbar), add = TRUE)\ncurve(from = -30, to = 90, coffs[2] + coffs[3]*(x-xbar), add = TRUE)}\n\n\n\n\n\n\n\n\nComment from the seminar: When we use dummy coding that means we compound uncertainty, because while female height has one prior, male height is the combination of two priors\n\nAs above, but allow for a curvilinear effect of age, by adding age-squared to the model. Center the age variable around its mean.\n\n\n#2. add non-linear efffect\nd &lt;- Howell1\nd &lt;- d[d$age &gt; 18,]\nxbar &lt;- mean(d$age)\nd$age &lt;- d$age - mean(d$age)\nd$sex &lt;- ifelse(d$male == 1, 2,1)\nmdl2 &lt;- quap(\n  alist(\n    weight ~ dnorm(mu,sigma),\n    mu &lt;- a[sex] + beta_age_1*age + beta_age_2*(age^2),\n    a[sex] ~ dnorm(178,20),\n    beta_age_1 ~ dnorm(0,2),\n    beta_age_2 ~ dnorm(0,2),\n    sigma ~ dexp(1)\n  ), data = d\n)\n\nCaution, model may not have converged.\n\n\nCode 1: Maximum iterations reached.\n\nprecis(mdl2, depth = 2)\n\n                   mean           sd         5.5%        94.5%\na[1]       42.813724383 0.4592977796 42.079677822 43.547770943\na[2]       49.569708908 0.4690369922 48.820097204 50.319320611\nbeta_age_1 -0.043801706 0.0205120872 -0.076583983 -0.011019429\nbeta_age_2 -0.003715448 0.0009731083 -0.005270663 -0.002160233\nsigma       5.201803339 0.1952800176  4.889708154  5.513898523\n\n#plot it\nmu_women &lt;- sim(mdl2, data=list(age=c(-25:50), sex=rep(1, 76)))\nmu_women_pi &lt;- apply(mu_women, 2, PI, prob = 0.95)\nmu_men &lt;- sim(mdl2, data=list(age=c(-25:50), sex=rep(2, 76)))\nmu_men_pi &lt;- apply(mu_men, 2, PI, prob = 0.95)\n#plot it\ncoffs &lt;- precis(mdl2, depth = 2)[,1]\n{plot(weight ~ age, data = d, xlab = \"difference from mean age\")\n  curve(from = -30, to = 90, coffs[1] + coffs[3]*x + coffs[4]*(x^2), add = TRUE)\n  curve(from = -30, to = 90, coffs[2] + coffs[3]*x + coffs[4]*(x^2), add = TRUE)\n  shade(mu_women_pi, -25:50, col=col.alpha(\"red\", 0.15))\n  shade(mu_men_pi, -25:50, col=col.alpha(\"blue\", 0.15))}\n\n\n\n\n\n\n\n\nFor the first model, motivate priors based on a prior predictive simulation (cf. Fig. 4.5 of the Book). Then fit the model and report regression coefficients with compatibility intervals, and illustrate model predictions in a plot of the data (weight as a function of non-centered age) with separate prediction lines for women and men. Include compatibility regions around fitted lines (e.g., as in Fig. 5.1 of the book) or add samples of fitted lines (e.g., as in Fig. 4.7).\nDid not figure out how to connect extract.prior() with link() here…\n\ndata(\"Howell1\")\nd &lt;- Howell1\nd &lt;- d[d$age &gt; 18,]\nxbar &lt;- mean(d$age)\nd$sex &lt;- ifelse(d$male == 1, 2,1)\n\nmdl1 &lt;- quap(\n  alist(\n    weight ~ dnorm(mu,sigma),\n    mu &lt;- a + beta_age*(age-xbar),\n    a ~ dnorm(178,20),\n    beta_age ~ dnorm(0,1),\n    sigma ~ dexp(1)\n  ), data = d\n)\nprior &lt;- extract.prior(mdl1)\n{plot(NULL,xlim=c(18,90),ylim=c(0,300))\nfor (i in 1:50) abline(a = prior$a[i], b = prior$beta_age[i])}\n\n\n\n\n\n\n\n\nAs we can see, we should center age.\nDo the same for the second model.\n\nd &lt;- Howell1\nd &lt;- d[d$age &gt; 18,]\nxbar &lt;- mean(d$age)\nd$age &lt;- d$age - mean(d$age)\nd$sex &lt;- ifelse(d$male == 1, 2,1)\nmdl2 &lt;- quap(\n  alist(\n    weight ~ dnorm(mu,sigma),\n    mu &lt;- a[sex] + beta_age_1*age + beta_age_2*(age^2),\n    a[sex] ~ dnorm(178,20),\n    beta_age_1 ~ dnorm(0,2),\n    beta_age_2 ~ dnorm(0,2),\n    sigma ~ dexp(1)\n  ), data = d\n)\nprior &lt;- extract.prior(mdl2)\n{plot(NULL,xlim=c(18,90),ylim=c(0,300))\nfor (i in 1:100) curve(from = 18, to = 90, prior$a[i] + prior$beta_age_1[i]*x + prior$beta_age_2[i]*(x^2), add = TRUE )} #brakets!!"
  },
  {
    "objectID": "notes/Stats2_seminar_4.html#mn4a",
    "href": "notes/Stats2_seminar_4.html#mn4a",
    "title": "Stats2_seminar_4",
    "section": "",
    "text": "Use McElreath’s dataset Howell2 restricted to adults (age &gt; 18). Fit two linear models using quadratic approximation, quap():\n\nWeight as outcome, with two predictors: sex as an indicator variable and age as a continuous variable.\nAs above, but allow for a curvilinear effect of age, by adding age-squared to the model. Center the age variable around its mean.\n\nFor the first model, motivate priors based on a prior predictive simulation (cf. Fig. 4.5 of the Book). Then fit the model and report regression coefficients with compatibility intervals, and illustrate model predictions in a plot of the data (weight as a function of non-centered age) with separate prediction lines for women and men. Include compatibility regions around fitted lines (e.g., as in Fig. 5.1 of the book) or add samples of fitted lines (e.g., as in Fig. 4.7).\n\ndata(\"Howell1\")\nd &lt;- Howell1\nd &lt;- d[d$age &gt; 18,]\nxbar &lt;- mean(d$age)\nd$sex &lt;- ifelse(d$male == 1, 2,1)\n\n#1. first model, adding age and sex\nmdl1 &lt;- quap(\n  alist(\n    weight ~ dnorm(mu,sigma),\n    mu &lt;- a[sex] + beta_age*(age-xbar),\n    a[sex] ~ dnorm(178,20), #this is a common prior. With dummy variables parametrised we could two priors instead.\n    beta_age ~ dnorm(0,1),\n    sigma ~ dexp(1)\n  ), data = d\n)\nprecis(mdl1, depth = 2)\n\n                mean         sd       5.5%       94.5%\na[1]     41.86609093 0.39235827 41.2390266 42.49315522\na[2]     48.68782608 0.41332269 48.0272566 49.34839556\nbeta_age -0.08067284 0.01802636 -0.1094824 -0.05186323\nsigma     5.29371947 0.19898087  4.9757096  5.61172934\n\n#plot it\ncoffs &lt;- precis(mdl1, depth = 2)[,1]\n{plot(weight ~ age, data = d)\ncurve(from = -30, to = 90, coffs[1] + coffs[3]*(x-xbar), add = TRUE)\ncurve(from = -30, to = 90, coffs[2] + coffs[3]*(x-xbar), add = TRUE)}\n\n\n\n\n\n\n\n\nComment from the seminar: When we use dummy coding that means we compound uncertainty, because while female height has one prior, male height is the combination of two priors\n\nAs above, but allow for a curvilinear effect of age, by adding age-squared to the model. Center the age variable around its mean.\n\n\n#2. add non-linear efffect\nd &lt;- Howell1\nd &lt;- d[d$age &gt; 18,]\nxbar &lt;- mean(d$age)\nd$age &lt;- d$age - mean(d$age)\nd$sex &lt;- ifelse(d$male == 1, 2,1)\nmdl2 &lt;- quap(\n  alist(\n    weight ~ dnorm(mu,sigma),\n    mu &lt;- a[sex] + beta_age_1*age + beta_age_2*(age^2),\n    a[sex] ~ dnorm(178,20),\n    beta_age_1 ~ dnorm(0,2),\n    beta_age_2 ~ dnorm(0,2),\n    sigma ~ dexp(1)\n  ), data = d\n)\n\nCaution, model may not have converged.\n\n\nCode 1: Maximum iterations reached.\n\nprecis(mdl2, depth = 2)\n\n                   mean           sd         5.5%        94.5%\na[1]       42.813724383 0.4592977796 42.079677822 43.547770943\na[2]       49.569708908 0.4690369922 48.820097204 50.319320611\nbeta_age_1 -0.043801706 0.0205120872 -0.076583983 -0.011019429\nbeta_age_2 -0.003715448 0.0009731083 -0.005270663 -0.002160233\nsigma       5.201803339 0.1952800176  4.889708154  5.513898523\n\n#plot it\nmu_women &lt;- sim(mdl2, data=list(age=c(-25:50), sex=rep(1, 76)))\nmu_women_pi &lt;- apply(mu_women, 2, PI, prob = 0.95)\nmu_men &lt;- sim(mdl2, data=list(age=c(-25:50), sex=rep(2, 76)))\nmu_men_pi &lt;- apply(mu_men, 2, PI, prob = 0.95)\n#plot it\ncoffs &lt;- precis(mdl2, depth = 2)[,1]\n{plot(weight ~ age, data = d, xlab = \"difference from mean age\")\n  curve(from = -30, to = 90, coffs[1] + coffs[3]*x + coffs[4]*(x^2), add = TRUE)\n  curve(from = -30, to = 90, coffs[2] + coffs[3]*x + coffs[4]*(x^2), add = TRUE)\n  shade(mu_women_pi, -25:50, col=col.alpha(\"red\", 0.15))\n  shade(mu_men_pi, -25:50, col=col.alpha(\"blue\", 0.15))}\n\n\n\n\n\n\n\n\nFor the first model, motivate priors based on a prior predictive simulation (cf. Fig. 4.5 of the Book). Then fit the model and report regression coefficients with compatibility intervals, and illustrate model predictions in a plot of the data (weight as a function of non-centered age) with separate prediction lines for women and men. Include compatibility regions around fitted lines (e.g., as in Fig. 5.1 of the book) or add samples of fitted lines (e.g., as in Fig. 4.7).\nDid not figure out how to connect extract.prior() with link() here…\n\ndata(\"Howell1\")\nd &lt;- Howell1\nd &lt;- d[d$age &gt; 18,]\nxbar &lt;- mean(d$age)\nd$sex &lt;- ifelse(d$male == 1, 2,1)\n\nmdl1 &lt;- quap(\n  alist(\n    weight ~ dnorm(mu,sigma),\n    mu &lt;- a + beta_age*(age-xbar),\n    a ~ dnorm(178,20),\n    beta_age ~ dnorm(0,1),\n    sigma ~ dexp(1)\n  ), data = d\n)\nprior &lt;- extract.prior(mdl1)\n{plot(NULL,xlim=c(18,90),ylim=c(0,300))\nfor (i in 1:50) abline(a = prior$a[i], b = prior$beta_age[i])}\n\n\n\n\n\n\n\n\nAs we can see, we should center age.\nDo the same for the second model.\n\nd &lt;- Howell1\nd &lt;- d[d$age &gt; 18,]\nxbar &lt;- mean(d$age)\nd$age &lt;- d$age - mean(d$age)\nd$sex &lt;- ifelse(d$male == 1, 2,1)\nmdl2 &lt;- quap(\n  alist(\n    weight ~ dnorm(mu,sigma),\n    mu &lt;- a[sex] + beta_age_1*age + beta_age_2*(age^2),\n    a[sex] ~ dnorm(178,20),\n    beta_age_1 ~ dnorm(0,2),\n    beta_age_2 ~ dnorm(0,2),\n    sigma ~ dexp(1)\n  ), data = d\n)\nprior &lt;- extract.prior(mdl2)\n{plot(NULL,xlim=c(18,90),ylim=c(0,300))\nfor (i in 1:100) curve(from = 18, to = 90, prior$a[i] + prior$beta_age_1[i]*x + prior$beta_age_2[i]*(x^2), add = TRUE )} #brakets!!"
  },
  {
    "objectID": "notes/Stats2_seminar_4.html#sr5e1-which-of-the-linear-models-below-are-multiple-linear-regressions",
    "href": "notes/Stats2_seminar_4.html#sr5e1-which-of-the-linear-models-below-are-multiple-linear-regressions",
    "title": "Stats2_seminar_4",
    "section": "SR5E1 Which of the linear models below are multiple linear regressions?",
    "text": "SR5E1 Which of the linear models below are multiple linear regressions?\n\\[\n\\begin{align}\nμ_i=α+βx_i \\\\\nμ_i=β_x x_i+β_z z_i \\\\\nμ_i=α+β(x_i−z_i) \\\\\nμ_i=α+β_x x_i+β_z z_i\n\\end{align}\n\\]\nI’d say 2 and 4. I don’t know what 3 is. Looks cursed. The result though, seems to be a simple linear regression on a new variable computed from the pairs. The second one doesn’t have an intercept, which is often dumb but can be justified under some circumstances."
  },
  {
    "objectID": "notes/Stats2_seminar_4.html#sr5e2",
    "href": "notes/Stats2_seminar_4.html#sr5e2",
    "title": "Stats2_seminar_4",
    "section": "SR5E2",
    "text": "SR5E2\nWrite down a multiple regression to evaluate the claim: “Animal diversity is linearly related to latitude, but only after controlling for plant diversity”. You just need to write down the model definition.\nAnswer: So linearly related after controlling for plant diversity. I’d assume then that plant diversity is a cause we should control for. If we make a DAG, plant diversity can’t “cause” latitude. Here \\(A\\) stands for animal diversity. \\(L\\) stands for latitude. \\(P\\) stands for plant diversity.\nL –&gt; A\nL –&gt; P –&gt; A\nIf we control for P we block that path into A, estimating only the direct path from L to A. Maybe like that?\nFormula: \\[\n\\begin{align}\nA_i \\sim Normal(\\mu_i, \\sigma) \\\\\n\\mu_i = \\alpha + \\beta_L L_i + \\beta_P P_i\\\\\n\\alpha \\sim Normal(0,0.2)\\\\\n\\beta_{L} \\sim Normal(0,1) \\\\\n\\beta_{P} \\sim Normal(0,1) \\\\\n\\sigma \\sim Uniform(0,6)\n\\end{align}\n\\]"
  },
  {
    "objectID": "notes/Stats2_seminar_4.html#e3",
    "href": "notes/Stats2_seminar_4.html#e3",
    "title": "Stats2_seminar_4",
    "section": "5E3",
    "text": "5E3\nWrite down a multiple regression to evaluate the claim: Neither amount of funding nor size of laboratory is by itself a good predictor of time to PhD degree; but together these variables are both positively associated with time to degree. Write down the model definition and indicate which side of zero each slope parameter should be on.\nAnswer: \\(F\\) is amount of funding. \\(L\\) is laboratory size. \\(T\\) is time to degree. I assume we need many regressions to evaluate the claim?\nLaboratory size by itself: \\[\n\\begin{align}\nT_i \\sim Normal(\\mu_i, \\sigma) \\\\\n\\mu_i = \\alpha + \\beta_1 L_i \\\\\n...priors...\n\\end{align}\n\\]\nFunding by itself: \\[\n\\begin{align}\nT_i \\sim Normal(\\mu_i, \\sigma) \\\\\n\\mu_i = \\alpha + \\beta_2 F_i \\\\\n...priors...\n\\end{align}\n\\]\nCombined: \\[\n\\begin{align}\nT_i \\sim Normal(\\mu_i, \\sigma) \\\\\n\\mu_i = \\alpha + \\beta_1 L_i + \\beta_2 F_i \\\\\n...priors...\n\\end{align}\n\\]"
  },
  {
    "objectID": "posts/20250417_First_Blogpost/index.html",
    "href": "posts/20250417_First_Blogpost/index.html",
    "title": "Against using Nicholas Cage movies to teach correlation and causation",
    "section": "",
    "text": "I recently took a short statistics course as part of my PhD-studies and came across one of my pet-peeves in introductory statistics content. In every intro to stats book or course, one of the objectives is to teach students that correlation does not imply causation. Very often this is taught with a graph like this:\nThe students in the audience laugh. What a silly correlation! Obviously there’s no causality there! They write down “correlation does not imply causation” and then the teacher move on to the next powerpoint slide.\nI don’t like these for a number of reasons."
  },
  {
    "objectID": "posts/20250417_First_Blogpost/index.html#reason-1-wheres-the-doom-and-gloom",
    "href": "posts/20250417_First_Blogpost/index.html#reason-1-wheres-the-doom-and-gloom",
    "title": "Against using Nicholas Cage movies to teach correlation and causation",
    "section": "Reason 1: Where’s the doom and gloom?",
    "text": "Reason 1: Where’s the doom and gloom?\nMy problem with these demonstrations is not that they don’t effectively communicate the point, but that they’re too effective. The non-validity of the correlation is so clear that you barely have to think about why anyone would ever come to draw causal conclusions from observational data. But we really do that all the time. Falling into causal thinking is a strong tendency in human cognition. Counteracting that takes effort. That means one should be suspicious of the pedagogical value of demonstrations that are absorbed so effortlessly.\nI think it’s better to set up examples where inferring causation comes naturally to people, then trip people up. Any good introduction to the idea “correlation does not imply causation” should first evoke a feeling of confusion. I always liked the example Daniel Kahneman (Rest In Peace) brought up in Thinking Fast and Slow when talking about our bias towards causal thinking:\n“Highly intelligent women tend to marry men who are less intelligent than they are.”"
  },
  {
    "objectID": "posts/20250417_First_Blogpost/index.html#reason-2-why-always-time-series-data",
    "href": "posts/20250417_First_Blogpost/index.html#reason-2-why-always-time-series-data",
    "title": "Against using Nicholas Cage movies to teach correlation and causation",
    "section": "Reason 2: Why always time-series data?",
    "text": "Reason 2: Why always time-series data?\nThis one is less of a consistent problem in intro-to-stats content but most examples I’ve seen have used time-series correlations to make this point. I think this is one of the reasons the demonstration slide down so easily. A lot of things happen in the world, just because thing A happened before thing B doesn’t mean there’s a causal relationship. Obviously! Although people sometimes do make that type of inference too easily, see for example this graph about how reduced institutionalization in mental hospitals in the U.S. was followed by a growing prison population, which I saw uncritically reposted multiple times last year, it is very clear to understand conceptually when no plausible causal link comes to mind.1\nI think using time-series correlations where there is a readily available causal idea would be an improvement pedagogically. But most correlations we come across as in research won’t be time-series data. Instead it will often concern inter-individual variation, like a relationship between interleukin-6 and depressive symptoms, and the result will instead be visualized on a scatterplot with a regression line. I think the nature of these types of correlations are much more easily imbued with a vague aura of causality. The very same individuals that had more interleukin had worse depressive symptoms!\nThis type of data is just as observational as the time-series examples. I think it’s better to use them since they’re closer to the most common form of correlation."
  },
  {
    "objectID": "posts/20250417_First_Blogpost/index.html#reason-3-random-correlation-vs.-spurious-correlation",
    "href": "posts/20250417_First_Blogpost/index.html#reason-3-random-correlation-vs.-spurious-correlation",
    "title": "Against using Nicholas Cage movies to teach correlation and causation",
    "section": "Reason 3: random correlation vs. spurious correlation",
    "text": "Reason 3: random correlation vs. spurious correlation\nI also think these funny and obviously non-causative correlations mix up (or at least combine) two different types of false relationships.\nIf you look at a lot of random things sometimes you’ll get a “statistically significant” correlation, even when there’s no possible way there’s a relationship. Indeed the example pictured above is barely different from finding a significant association between yearly Nicholas Cage movies and a string of random numbers I generate on my computer (after trying a bunch of times). This random-number-version seems to lack the same demonstrative oomph for showing the limits of observational data. That’s because these examples are teaching two lessons at the same time! Besides the difference between observational and experimental data they are, at the same time, teaching students about the limits of hypothesis testing itself. If there’s no true correlation between the things you’re looking at you’ll still get a “significant” p-value 5% of the time. Cue the xkcd comic:\n\n\n\nIf you’ve used this comic as an example of “correlation does not imply causality” you’ve made a mistake. The comic is about p-hacking/data-dredging. The scientists are presumably doing RCTs\n\n\nThis double-lesson-property could give the false impression that if you look at enough things, you simply need to correct for multiple comparisons and find causative links! (These types of corrections are necessary and looking for associations is of course important, it can be a promising way to figure out what to explore further. But all this presupposes some prior idea of a plausible causal link).\nRelatedly one might get the idea that replicating the correlation in a subsequent study gets you closer to proving a causal link. Teaching people about the limits of hypothesis testing and the correct interpretation of p-values is no doubt important, but it’s fundamentally different from the problem with observational data. Or to put it simply: Confounding replicates."
  },
  {
    "objectID": "posts/20250417_First_Blogpost/index.html#how-to-teach-this-then",
    "href": "posts/20250417_First_Blogpost/index.html#how-to-teach-this-then",
    "title": "Against using Nicholas Cage movies to teach correlation and causation",
    "section": "How to teach this then?",
    "text": "How to teach this then?\nI think good examples teaching this concept should first and foremost emphasize how confounders (e.g. common causes) can create plausible sounding and reliable statistical-but-non-causal relationships. And don’t use time-series data.2 Beyond that I think it should show the fundamental rift between experimental data with manipulation and randomization, and observational data about the state of the world. This really is the central thing: Even when we have plausible-sounding mechanisms, randomized experiments can reveal things aren’t as we thought. Until we’ve done those experiments, or if we can’t do them, we’re always in a deeply more uncertain place scientifically.\nI don’t know any examples that I’ve found perfectly elegant. But I remember that one of my favourite science communicators Ben Goldacre had a nice part in his book Bad Science where he talked about antioxidant vitamin pills and lung cancer. He first set up the observed correlation between these things, and also set up a plausible mechanism to explain the correlation (the free radical theory of aging). That all sounds well and good, but when researchers actually ran clinical trials they found that the people who took the beta-carotene and vitamin A pills were more (!) likely to die from lung cancer. I think examples like this are good for instilling some doom and gloom and doubt about correlational data, but in an ideal case one should also present what the source of the spurious correlation was.\nTo be fair, people rarely teach this concept with a single example. The problem I’m pointing at is a mostly unnecessary pet-peeve, I know. But I also do think the first impression people have of a concept matters, at least a bit. And for that purpose, I wish they would come up with something other that wacky time-series correlations.3"
  },
  {
    "objectID": "posts/20250417_First_Blogpost/index.html#footnotes",
    "href": "posts/20250417_First_Blogpost/index.html#footnotes",
    "title": "Against using Nicholas Cage movies to teach correlation and causation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThere is something extra fragile with time-series correlations. There’s only one world and history only happened once. I think this is one of the reasons subjects like political science is in an epistemically more difficult position than for example medicine.↩︎\nunless students are likely to primarily work with that.↩︎\nThis post may seem like an attack on Tyler Vigen’s spurious correlation website. I want to be clear I have nothing against the site and I think it’s a funny project and even a good viral type of science communication. It’s existence online is good! My problem here is with it’s repeated role in into-to-stats content for university level students.↩︎"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "R/stats/psych posts",
    "section": "",
    "text": "Here I share posts that connect either to psychology or to statistics. Some are cross-posted on substack.\n\n\n\n\n\n\n\n\n\n\n\n\nLongitudinal Mixed Model Explorer App\n\n\n\nshiny\n\n\nteaching\n\n\n\nI built a shiny app\n\n\n\nVilgot Huhn\n\n\nMay 6, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nAgainst using Nicholas Cage movies to teach correlation and causation\n\n\n\nteaching\n\n\n\nreposted from substack\n\n\n\nVilgot Huhn\n\n\nMar 31, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nOh No! Berkson’s paradox in clinical theories\n\n\n\ncausal inference\n\n\n\nreposted from substack\n\n\n\nVilgot Huhn\n\n\nDec 16, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notes/imai_notes_from_memory.html",
    "href": "notes/imai_notes_from_memory.html",
    "title": "Imai et al. (notes, mostly from memory)",
    "section": "",
    "text": "Loading required package: MASS\n\n\nLoading required package: Matrix\n\n\nLoading required package: mvtnorm\n\n\nLoading required package: sandwich\n\n\nmediation: Causal Mediation Analysis\nVersion: 4.5.1"
  },
  {
    "objectID": "notes/imai_notes_from_memory.html#potential-outcomes-method-for-mediation",
    "href": "notes/imai_notes_from_memory.html#potential-outcomes-method-for-mediation",
    "title": "Imai et al. (notes, mostly from memory)",
    "section": "Potential outcomes method for mediation",
    "text": "Potential outcomes method for mediation\nThe potential outcomes method seems (?) equivalent to the causal mediation method.\nIt is based around fitting two models and comparing parameters. Three effects are of interest, the total effect, the direct effect, and the causal mediated effect. The parameters of interest will all be averages (Imai 2011 calls it the ATE, ADE, and ACME).\nIn an observational context there are four assumptions that needs to be met (conventional exogeneity assumptions) for this estimation procedure to be valid.\n\nNo exposure (X) outcome (Y) confounding.\nNo exposure (X) mediator (M) confounding.\nNo mediator (M) outcome (Y) confounding.\nNo exposure-mediator interaction.\nNo mediator-outcome confounders affected by the exposure. I don’t fully understand this one.\n\n\nHow does/doesn’t randomization help\nRandomization deals with any outside influence on the exposure X, so common causes for X&M and X&Y are dealt with. I.e. we no longer have to worry about assumption 1 and 2. I think assumption 5 should also be dealt with.\n\n\nOld-school method\nBaron & Kenny (1986, or something) has been the go-to method for estimating a mediation. The size of the mediated effect are estimated through what’s called the “product method”. Two models are fit, and three paths are estimated. First a model is fit for the exposure-mediator relationship.\n\\[\nM_i=\\beta_0+\\beta_1x_i+\\epsilon_i\n\\]\nSo if x is a randomized treatment dummy variable (0,1) we get a prediction of how much being in the treatment group increases M, on average.\nThen an additional model is fit containing both the exposure and the mediator.\n\\[\nY_i=\\gamma_0+\\gamma_1x_i+\\gamma_2m_i+\\epsilon_{i}\n\\]\nHere our \\(\\gamma_1\\) is the estimate of the treatment effect (exposure) when conditioning on the mediator, and vice versa, \\(\\gamma_2\\) is the effect of the mediator when conditioning on the exposure. \\(\\gamma_1\\) is also referred to as \\(c'\\), the direct effect (the effect of X on Y that does not travel through the mediator).\nTo get an estimate of the indirect effect, the mediated effect, we take \\(\\beta_1\\gamma_2\\). In other words the effect of the exposure on the outcome times the conditional effect of the mediator on the outcome.\nLet’s do a simulation.\n\n#fully mediated effect\nn &lt;- 200\nX &lt;- rep(c(0,1), length.out = n)\nM &lt;- rnorm(n,X,1)\nY &lt;- rnorm(n,M,1)\n\n#fit first linear model\nmdl.m &lt;- lm(M ~ X)\nsummary(mdl.m) #beta_1 should be close to 1.\n\n\nCall:\nlm(formula = M ~ X)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.5211 -0.6823 -0.0317  0.7281  3.3249 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.08648    0.10742  -0.805    0.422    \nX            1.12116    0.15191   7.380 4.26e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.074 on 198 degrees of freedom\nMultiple R-squared:  0.2158,    Adjusted R-squared:  0.2118 \nF-statistic: 54.47 on 1 and 198 DF,  p-value: 4.261e-12\n\n#fit second linear model\nmdl.y &lt;- lm(Y ~ X + M)\nsummary(mdl.y) #conditional effect gamma_2 should be close to 1\n\n\nCall:\nlm(formula = Y ~ X + M)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.35325 -0.80872  0.00868  0.71447  2.30013 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.14745    0.10012  -1.473   0.1424    \nX            0.38995    0.15962   2.443   0.0154 *  \nM            0.97481    0.06613  14.741   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9995 on 197 degrees of freedom\nMultiple R-squared:  0.6243,    Adjusted R-squared:  0.6205 \nF-statistic: 163.7 on 2 and 197 DF,  p-value: &lt; 2.2e-16\n\n\nHere the code simulates a situation where there is no direct effect. The estimate for X from mdl.y should thus be close to zero, likely non-significant. The conditional effect of M on Y when controlling for X should be close to 1. In the first model the estimate for X on M should be close to 1 too. Now let’s consider what’s happening here. Half of our datapoints are getting treatment X, those who get that treatment are 1 SD higher on M, and those who are 1 SD higher on M are also 1 SD higher on Y when controlling for X. The indirect effect here can be estimated by taking the product, but it could also be indirectly estimated by taking the difference between the total effect, lm(Y ~ X), and the direct effect. Often that contrast/difference is our estimand.\nLet’s consider the product method with less strong effects.\n\nn &lt;- 2000\nX &lt;- rep(c(0,1), length.out = n)\nbeta_1 &lt;- 0.4\nM &lt;- rnorm(n, beta_1*X ,1)\ngamma_2 &lt;- 0.5\nY &lt;- rnorm(n, gamma_2*M, 1)\n\n#fit first linear model\nmdl.m &lt;- lm(M ~ X)\nsummary(mdl.m) #beta_1 should be close to 1.\n\n\nCall:\nlm(formula = M ~ X)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9668 -0.6786  0.0033  0.6697  3.9215 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.03296    0.03158   1.044    0.297    \nX            0.36543    0.04466   8.183 4.88e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9986 on 1998 degrees of freedom\nMultiple R-squared:  0.03243,   Adjusted R-squared:  0.03194 \nF-statistic: 66.96 on 1 and 1998 DF,  p-value: 4.884e-16\n\n#fit second linear model\nmdl.y &lt;- lm(Y ~ X + M)\nsummary(mdl.y)\n\n\nCall:\nlm(formula = Y ~ X + M)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.8768 -0.6893 -0.0014  0.6609  3.7945 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.03946    0.03241  -1.218    0.223    \nX            0.04927    0.04658   1.058    0.290    \nM            0.50378    0.02295  21.949   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.024 on 1997 degrees of freedom\nMultiple R-squared:  0.2027,    Adjusted R-squared:  0.2019 \nF-statistic: 253.9 on 2 and 1997 DF,  p-value: &lt; 2.2e-16\n\ncoef(mdl.m)[2]*coef(mdl.y)[3]\n\n        X \n0.1840973 \n\n\nGetting the treatment now increases the mediator M by 0.4, on average. The mediator then affects the outcome by 0.5 per unit, controlling for an eventual direct treatment effect (that we don’t have in the data generating process). Thus the indirect effect is 0.5 times 0.4 ~= 0.2.\n\n\nHow does the classical approach contrast with the potential outcomes approach?\nThe potential outcomes approach is instead based on counterfactuals. I find this quite conceptually confusing, but the gist of it is that the framework asks us to imagine we could time-travel and, for each individual, see what would have happened if we had switched the randomization (or set their mediator to a specified level). The confusing part is that this is impossible, but perhaps it’s valuable to remember that this is indeed what we wish we could do.1\nImai et al (2011) introduces a notation for potential outcomes that may be useful. \\(Y_i(1)\\) is the “potential” outcome if a patient \\(i\\) receives a treatment (if it’s dummy coded) and \\(Y_i(0)\\) is the potential outcome if they didn’t. Truly “causal” effects then are given by the contrast at the “unit level” \\(Y_i(1)-Y_i(0)\\). This can never really be known without time-travel, but supposedly there’s something gained by remembering that really, this is the question we’re actually interested in. With randomization an Average Treatment Effect (ATE) can be estimated since the probability of receiving the treatment is independent of the outcome \\(\\{ Y_i(1),Y_i(0) \\} \\perp T_i\\).\nAn even more clear notation could probably be \\(Y_i(t=0,m=0)\\)? But I don’t make the rules (I think there are alternative notation styles though).\nHowever when there’s no randomization we have to settle on the goal that the level of the exposure/treatment is “as if” randomized if we control for the right pre-treatment covariates. This is the first assumption of the sequential ignorability assumption. “treatment assignment is assumed to be ignorable”, also called exogeneity assumption, no omitted variable bias. The second assumption is more tricky. Even if we were to randomize the mediator, this would not satisfy sequential ignorability. It is sequential because “once” we’ve conditioned on a set of covariates, that renders the treatment/exposure ignorable, “as if randomized”, the mediator is ignorable (statistically independent of potential outcomes).\nThe mathy way to express the sequential ignorability assumption is:\n\\[\n\\{ Y_i(t',m),M_i(t) \\} \\perp T_i |X_i=x \\\\\nY_i(t',m) \\perp M_i(t) | T_i = t,  X_i=x\n\\]\nImai actually doesn’t explain in the paper what prim means here, but Claude suggested it was the counterfactual. Thus, reading it out we have line one: Let’s we have an individual who recieved treatment t, \\(T_i=t\\) ,then \\(t'\\) is their counter factual. The individual’s outcome on Y given the treatment they didn’t receive and the mediator, together with the individual outcome on the mediator for the treatment they actually did recieve, are assumed to both be “jointly” independent of the treatment, given we control for the right covariates. A confusing part here is the mediator level \\(m\\). This does not refer to the outcome on the mediator given a treatment, that’s \\(M_i(t)\\), instead it’s an arbitrary mediator level, not necessarily the one that would result from treatment.2 In other words we imagine we could manipulate both the treatment and the mediator. \\(m'\\) is then just another arbitrary mediator level.\nIn addition to this, the potential outcome should also be independent of the potential mediator value, given the treatment and these super nice well chosen covariates.\nI guess…\nThe implication of this where I managed to follow along with is that even if we could somehow randomize both the treatment and the mediator, what we’re actually after is the conditional effect. I.e. the mediated effect should not be mixed up with the effect of the mediator.\nAnyway, the best thing we can do for observational data, including randomized experiments, is to address confounding with sensitivity analysis. Se code below from https://imai.fas.harvard.edu/research/files/mediationR2.pdf\nThe tricky thing to interpret is the sensitivity parameter \\(\\rho\\) which describes the correlation between the error terms of the two models being compared. In other words, it seems like it’s a measure of total confounding. In a randomized scenario where the only3 possible source of “residual confounding” is mediator-outcome confounding4, it could probably be interpreted as that (right?). In an observational context it’s harder to interpret. Full quote in footnote5\nCode:\n\n#https://imai.fas.harvard.edu/research/files/mediationR2.pdf\n\ndata(\"framing\")\nhead(framing)\n\n  cond                anx age                        educ gender income emo\n1    3   a little anxious  45                 high school   male     13   7\n2    4   somewhat anxious  73 bachelor's degree or higher   male     16   6\n3    2   a little anxious  53                some college female      3   8\n4    1 not anxious at all  45                 high school   male     14   9\n5    3   somewhat anxious  55                some college female     12   5\n6    1   a little anxious  85                 high school female      3   5\n  p_harm tone eth treat         english immigr anti_info cong_mesg\n1      6    0   1     0          Oppose      4         0         1\n2      3    0   0     0           Favor      3         0         0\n3      7    1   0     0 Strongly Oppose      3         0         0\n4      8    1   1     1 Strongly Oppose      4         0         1\n5      5    0   1     0 Strongly Oppose      2         0         0\n6      6    1   1     1 Strongly Oppose      4         0         0\n\n#first fitting a model to predict the proposed mediator, anxiety (emo), controlling for a bunch of pre-treatment covariates\nmed.fit &lt;- lm(emo ~ treat + age + educ + gender + income, data = framing)\n\n#then fitting a model to predict the outcome, agreeing to send a message to a congresseman about immigration, controlling for same covariates\nout.fit &lt;- glm(cong_mesg ~ emo + treat + age + educ + gender + income, data = framing, family = binomial(\"probit\"))\n\n#then we use the function mediation::mediate, specifying both treatment variable (dummy?) and mediator.\nmed.out &lt;- mediate(med.fit, out.fit, treat = \"treat\", mediator = \"emo\", robustSE = TRUE, sims = 1000)\nsummary(med.out)\n\n\nCausal Mediation Analysis \n\nQuasi-Bayesian Confidence Intervals\n\n                          Estimate 95% CI Lower 95% CI Upper p-value    \nACME (control)            0.081611     0.034323     0.140369  &lt;2e-16 ***\nACME (treated)            0.082094     0.035396     0.139179  &lt;2e-16 ***\nADE (control)             0.013470    -0.097988     0.134226   0.858    \nADE (treated)             0.013953    -0.107508     0.142072   0.858    \nTotal Effect              0.095564    -0.031502     0.233847   0.148    \nProp. Mediated (control)  0.766724    -5.091557     6.889030   0.148    \nProp. Mediated (treated)  0.787056    -4.559368     6.197541   0.148    \nACME (average)            0.081852     0.035143     0.139569  &lt;2e-16 ***\nADE (average)             0.013712    -0.101688     0.137794   0.858    \nProp. Mediated (average)  0.776890    -4.825463     6.543285   0.148    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSample Size Used: 265 \n\n\nSimulations: 1000 \n\n#we can also allow for mediator-treatment interaction.\nout.fit &lt;- glm(cong_mesg ~ emo * treat + age + educ + gender + income, data = framing, family = binomial(\"probit\"))\nmed.out &lt;- mediate(med.fit, out.fit, treat = \"treat\", mediator = \"emo\", robustSE = TRUE, sims = 1000)\nsummary(med.out)\n\n\nCausal Mediation Analysis \n\nQuasi-Bayesian Confidence Intervals\n\n                           Estimate 95% CI Lower 95% CI Upper p-value    \nACME (control)            0.0762680    0.0285312    0.1305690  &lt;2e-16 ***\nACME (treated)            0.0983708    0.0362500    0.1760860  &lt;2e-16 ***\nADE (control)            -0.0052030   -0.1245914    0.1052578   0.932    \nADE (treated)             0.0168997   -0.1198561    0.1423289   0.812    \nTotal Effect              0.0931678   -0.0454880    0.2299640   0.196    \nProp. Mediated (control)  0.6766312   -6.4231779    5.6117670   0.196    \nProp. Mediated (treated)  0.8955596   -6.0687881    7.1155606   0.196    \nACME (average)            0.0873194    0.0354259    0.1467197  &lt;2e-16 ***\nADE (average)             0.0058484   -0.1170197    0.1196646   0.898    \nProp. Mediated (average)  0.7860954   -6.6267417    6.2875314   0.196    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSample Size Used: 265 \n\n\nSimulations: 1000 \n\n#testing the significance of the interaction\ntest.TMint(med.out, conf.level = .95)\n\n\n    Test of ACME(1) - ACME(0) = 0\n\ndata:  estimates from med.out\nACME(1) - ACME(0) = 0.022103, p-value = 0.35\nalternative hypothesis: true ACME(1) - ACME(0) is not equal to 0\n95 percent confidence interval:\n -0.02309668  0.07689059\n\n#section 3.2. describes moderated mediation, let's put a pin in that.\n\n#code for sensitivity analysis.\n#First let's switch back to the simple model without interaction\nout.fit &lt;- glm(cong_mesg ~ emo + treat + age + educ + gender + income, data = framing, family = binomial(\"probit\"))\nmed.out &lt;- mediate(med.fit, out.fit, treat = \"treat\", mediator = \"emo\", robustSE = TRUE, sims = 1000)\nsens.out &lt;- medsens(med.out, rho.by=0.1, effect.type = \"indirect\", sims = 100)\n\nWarning in rho^2 * (1 - r.sq.m) * (1 - r.sq.y): Recycling array of length 1 in vector-array arithmetic is deprecated.\n  Use c() or as.vector() instead.\n\n\nWarning in err.cr.d^2 * (1 - r.sq.m) * (1 - r.sq.y): Recycling array of length 1 in vector-array arithmetic is deprecated.\n  Use c() or as.vector() instead.\n\nsummary(sens.out) #works despite warning message...\n\n\nMediation Sensitivity Analysis: Average Mediation Effect\n\nSensitivity Region: ACME for Control Group\n\n     Rho ACME(control) 95% CI Lower 95% CI Upper R^2_M*R^2_Y* R^2_M~R^2_Y~\n[1,] 0.3        0.0059      -0.0073       0.0180         0.09       0.0493\n[2,] 0.4       -0.0088      -0.0320       0.0018         0.16       0.0877\n\nRho at which ACME for Control Group = 0: 0.3\nR^2_M*R^2_Y* at which ACME for Control Group = 0: 0.09\nR^2_M~R^2_Y~ at which ACME for Control Group = 0: 0.0493 \n\n\nSensitivity Region: ACME for Treatment Group\n\n     Rho ACME(treated) 95% CI Lower 95% CI Upper R^2_M*R^2_Y* R^2_M~R^2_Y~\n[1,] 0.3        0.0065      -0.0078       0.0178         0.09       0.0493\n[2,] 0.4       -0.0105      -0.0368       0.0018         0.16       0.0877\n\nRho at which ACME for Treatment Group = 0: 0.3\nR^2_M*R^2_Y* at which ACME for Treatment Group = 0: 0.09\nR^2_M~R^2_Y~ at which ACME for Treatment Group = 0: 0.0493 \n\n#we can then plot the sensitivity analysis\nplot(sens.out, sens.par = \"rho\", main = \"Anxiety\", ylim = c(-0.2, 0.2))"
  },
  {
    "objectID": "notes/imai_notes_from_memory.html#footnotes",
    "href": "notes/imai_notes_from_memory.html#footnotes",
    "title": "Imai et al. (notes, mostly from memory)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOn the other hand it is also valuable to remember you can never go back. No matter how much your heart breaks. You can never go back.↩︎\nEven with the powers of time travel fully picking apart the potential outcomes framework is difficult.↩︎\n“only”↩︎\nI may be wrong about this and \\(\\rho\\) actually specifically relates to mediator-outcome confounding even in non randomized contexts, I’m not sure.↩︎\n“Here, we focus on the outcomewhere subjects stated whether immigration shouldbe decreased or increased. The results are presented in Figure 2, which is generated using the mediation software. In the left panel, the true ACME is plotted against values of the sensitivity parameter ρ, which equals the correlation between the error terms in the mediator and outcome models and thus represents both the degree and direction of the unobserved confounding factor between anxiety and immigration preference. When ρ is zero, sequential ignorability holds and the true ACME coincides with the estimate reported in Table 2. The shaded region in the plot marks the 95% confidence intervals for each value of ρ.”↩︎"
  }
]