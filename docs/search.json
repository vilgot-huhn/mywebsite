[
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "notes",
    "section": "",
    "text": "This is meant as a more casual place to post more unstructured notes that I still want to share.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nMay 27, 2025\n\n\nmediation_sim_may_WIP\n\n\nVilgot Huhn\n\n\n\n\nMar 21, 2025\n\n\nStats2IndividualAssignment\n\n\nVilgot Huhn\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/20250506_lmm_explorer/index.html",
    "href": "posts/20250506_lmm_explorer/index.html",
    "title": "Longitudinal Mixed Model Explorer App",
    "section": "",
    "text": "So far learning about mixed models have been challenging but fun. I guess it’s par for the course for doctoral students to do a lot of self-supervised non-linear learning.\nWhen it comes to understanding statistics I feel like simulating data and applying methods to that has been helpful. I quite often get stuck, sometimes on dumb coding brainfarts, but other times because I have misunderstood something important. Especially with mixed models this has helped me understand why they’re interesting in the first place.\nStill I got a bit frustrated with having to re-run my data generating function when exploring parameters, so now that the function is good enough I threw it in a shiny app.\n\n\n\n\n\nThe app shows 20 “patients” over 8 weeks, modeled with random intercepts and slopes: lmer(y ~ week + (week|id), data). The intercept standard deviation of the intercepts is 1 (on average), so the fixed effect can be understood in relation to that.\nBuilding the app went very quickly, but my first version re-generated the data every time the user changed a parameter. I thought that would be less helpful as a teaching tool. The finished app instead has scaling parameters that affects the residuals and the random slopes. Remaking it meant making the app a bit more complicated, and since the app re-fits the lme4 model every time, the advantage wasn’t as clear as I had hoped. Oh well.\nOne small thing I learned was that I should probably be thinking about whether it makes sense to set the intercept at week 0 or not. (It’s visually very clear that the intercepts are more stable for changes in the data than the predicted random effects at week 8).\nI might improve the app further in the future (at least visually) if I find the time. For now it’s been a good learning experience that can hopefully be used as a minor teaching tool."
  },
  {
    "objectID": "posts/20231216_berkson/index.html",
    "href": "posts/20231216_berkson/index.html",
    "title": "Oh No! Berkson’s paradox in clinical theories",
    "section": "",
    "text": "Mike works as a clinical psychologist at a general psychiatric outpatient clinic. The people Mike sees have a wide range of issues, but all of them have real problems. Unemployment, broken relationships, loneliness, wasted potential, and usually a heavy mix of anxiety and hopelessness. In other words, mental illness, with all that comes with it. The word used among Mike’s colleagues is usually functional impairment, but he prefers to just think of misery.\nThe people Mike assess and treat are almost always referred from a primary care physician who believes this person needs specialized care. Sometimes the clinic gets a referral with a patient that isn’t really that impaired - some primary care doctors don’t know that much about psychiatry. In those cases Mike’s doctor colleagues send them somewhere else before he, or someone like him, even sees them.\nMike is a clever man, he’s good at spotting patterns. But he’s also careful. Humans in general are “pattern matchers”, and he knows this. We overfit our models. We see faces in clouds and rocks. After four heads in a row we think there’s bound to be a tail.\nLately, however, Mike is becoming more and more confident in a certain pattern. He’s even starting to become confident in his theory as to why the pattern arises. He has noticed a trade-off between two types of mental processes - excitingly two things that aren’t trivially related. Some people he meets have a bit of a loose grip on reality, more than others. They usually have “weird beliefs”, supernatural stuff, stuff about fate, alternative theories. Nothing psychotic, in his experience, but perhaps “schizotypal”. Other people are really Anxious. Interestingly the people that are more anxious seem to have a more realistic view of the world, and conversely people who have more weird1 spiritual beliefs about fate and the universe and how everything is connected seem to be less anxious.\nMaybe these beliefs and this style of thinking protects against anxiety? Maybe detaching yourself from the real world, giving yourself space to think more freely and openly guards you against the pain of reality. Since Mike is aware that the human mind often finds illusory correlations he instead chooses to gather data on his patients via other clinicians, blind to his theory. He explains his ideas of anxiousness and supernatural/weird beliefs and lets his colleagues get calibrated in scoring imaginary patient cases. Then they rate the patients they meet on these scales and hand the data to Mike.\nWhen the data has been collected the correlation is clearly visible and significant even at his relatively small sample! Heureka!\n\n\n\n\n\nIt really makes sense doesn’t it? Most of the people he meets at the clinic live lives full of real tangible problems. Taking a step away from reality is bound to protect emotionally against that, at least a bit. Meanwhile having a realistic outlook on your marginalized situation is bound to fill you with some anxiety. The mental processes that lead to “weird beliefs” do seem to come with their own problems, sometimes conflicts with others, or trouble with goal oriented behaviour. But this may be a small price to pay for some peace of mind."
  },
  {
    "objectID": "posts/20231216_berkson/index.html#lets-start-with-a-story.",
    "href": "posts/20231216_berkson/index.html#lets-start-with-a-story.",
    "title": "Oh No! Berkson’s paradox in clinical theories",
    "section": "",
    "text": "Mike works as a clinical psychologist at a general psychiatric outpatient clinic. The people Mike sees have a wide range of issues, but all of them have real problems. Unemployment, broken relationships, loneliness, wasted potential, and usually a heavy mix of anxiety and hopelessness. In other words, mental illness, with all that comes with it. The word used among Mike’s colleagues is usually functional impairment, but he prefers to just think of misery.\nThe people Mike assess and treat are almost always referred from a primary care physician who believes this person needs specialized care. Sometimes the clinic gets a referral with a patient that isn’t really that impaired - some primary care doctors don’t know that much about psychiatry. In those cases Mike’s doctor colleagues send them somewhere else before he, or someone like him, even sees them.\nMike is a clever man, he’s good at spotting patterns. But he’s also careful. Humans in general are “pattern matchers”, and he knows this. We overfit our models. We see faces in clouds and rocks. After four heads in a row we think there’s bound to be a tail.\nLately, however, Mike is becoming more and more confident in a certain pattern. He’s even starting to become confident in his theory as to why the pattern arises. He has noticed a trade-off between two types of mental processes - excitingly two things that aren’t trivially related. Some people he meets have a bit of a loose grip on reality, more than others. They usually have “weird beliefs”, supernatural stuff, stuff about fate, alternative theories. Nothing psychotic, in his experience, but perhaps “schizotypal”. Other people are really Anxious. Interestingly the people that are more anxious seem to have a more realistic view of the world, and conversely people who have more weird1 spiritual beliefs about fate and the universe and how everything is connected seem to be less anxious.\nMaybe these beliefs and this style of thinking protects against anxiety? Maybe detaching yourself from the real world, giving yourself space to think more freely and openly guards you against the pain of reality. Since Mike is aware that the human mind often finds illusory correlations he instead chooses to gather data on his patients via other clinicians, blind to his theory. He explains his ideas of anxiousness and supernatural/weird beliefs and lets his colleagues get calibrated in scoring imaginary patient cases. Then they rate the patients they meet on these scales and hand the data to Mike.\nWhen the data has been collected the correlation is clearly visible and significant even at his relatively small sample! Heureka!\n\n\n\n\n\nIt really makes sense doesn’t it? Most of the people he meets at the clinic live lives full of real tangible problems. Taking a step away from reality is bound to protect emotionally against that, at least a bit. Meanwhile having a realistic outlook on your marginalized situation is bound to fill you with some anxiety. The mental processes that lead to “weird beliefs” do seem to come with their own problems, sometimes conflicts with others, or trouble with goal oriented behaviour. But this may be a small price to pay for some peace of mind."
  },
  {
    "objectID": "posts/20231216_berkson/index.html#oh-no",
    "href": "posts/20231216_berkson/index.html#oh-no",
    "title": "Oh No! Berkson’s paradox in clinical theories",
    "section": "Oh no…",
    "text": "Oh no…\nYou know we were getting to this. His selection of patients is conditioned. In this example our psychologist isn’t falling victim to some “illusory correlation”. He’s not seeing a relationship where none exist. In his data-set the correlation really is quite strong. What he misses is why it exists.\nLet’s say “weird beliefs” are bad for your level of “functional impairment” in a simple linear way. The more weird beliefs the more impairment.\nLet’s assume that anxiety is also bad for your functioning (it is). Adding these together we have impairment = anxiety + weird beliefs. So with one unit of anxiety and five units of “weird beliefs” you have six units of impairment.\nWe can draw it like a gradient to make it really clear. The x and y axes represent weird beliefs and anxiety levels. Redder colour means more functional impairment.\n\n\n\n\n\nAt Mikes clinic, people who are not impaired enough don’t need to seek professional help at a clinic, or are sent elsewhere by whoever screens the referrals. Additionally, people who are in an acute crisis may be sent to even more specialized clinics, even inpatient care.\nThe effect is that Mike’s patients exist in a limited range of functional impairment. Meanwhile if we were to look at a larger random sample of the population we see that there’s no relationship between anxiety and weird beliefs.\n\n\n\nI remind you this is simulated data.\n\n\nI remind you that this is simulated data. Indeed, the paradoxiest2 part of Berkson’s paradox is that it can even reverse the direction of a correlation. Generally in psychiatry negative things are correlated, both for psychological reasons (for example negative emotions causing sleep issues) and for practical reasons (for example a depression having a negative impact on your financial security) and perhaps even for biological reason (there are positive genetic correlations between many different disorders). This has led to some researchers suggesting there’s a general factor of psychopathology3. But even in those conditions, where there’s a positive correlation between the traits we look at, we can get a reversed correlation given the right selection effect:"
  },
  {
    "objectID": "posts/20231216_berkson/index.html#is-this-common-does-this-happen",
    "href": "posts/20231216_berkson/index.html#is-this-common-does-this-happen",
    "title": "Oh No! Berkson’s paradox in clinical theories",
    "section": "Is this common? Does this happen?",
    "text": "Is this common? Does this happen?\nIn my experience the culture in clinical psychology can be a bit contradictory. On one hand our education teaches us to be very sceptical. We’re taught to be careful not to make scientifically unwarranted claims. We’re taught to be suspicious of simplifications. Aware of unknown confounders. All that jazz. At the same time psychology often reveres clinical intuition. From a scientific point of view we often don’t know more than the fact that a bunch of questionnaires are internally consistent and correlate with each other in this and that way. Meanwhile in the land of clinical experience we can imagine ourselves knowing with high precision exactly why someone has a symptom, given their personality, their defences, their context (depending on who you ask). I don’t want to come of as too negative about that. Maybe people really are too multifaceted for “reductionism”? At the very least I think people’s psychological intuitions are often based on a sound understanding of how other humans work. I’m a human living among humans, so that intuition should be based off good data at least.\nThe field of judgement and decision-making would have a lot to criticize about the above statement. They would point to our tendency to form bad intuitions in contexts where we can’t reasonably expect to form them, i.e. where feedback is delayed and noisy. Working as a therapist, unfortunately, is one such context. But in the story above that isn’t the problem. Even if you’re careful and well calibrated, you can’t make up for biased data. “Garbage in, garbage out” as we often say when it comes to meta-analyses and machine learning.\nI don’t mean to argue that this particular mistake is necessarily super common. I really don’t know. But no doubt it’s easy for clinicians to make their own theories, see potential trade-offs or see how patients cluster into types. Worry as a defence against sadness. Externalizing symptoms as a way to not feel anxiety. I get that these theories have supporting literature and arguments that go deeper than simple observations, but situations we often find ourselves in as clinicians may make these theories feel convincing for the wrong reasons.\nAnd I think this potential pattern is important to be aware of in a bunch of different contexts. For example you could imagine teachers at some sort of prestigious private school seeing a trade-off between being good at math and good at writing, in a way that makes them buy into questionable research about learning styles. Instead the pattern is because students get in to the school based on a sum of their grades, some more language-related, some more math-related. The same teachers could also plausibly spot false negative relationships between learning fast and being industrious (“they’re smart so they don’t have to learn any study-strategies”). Life is full of unrepresentative samples.\nBasically you always need to think about where your sample comes from, and if the reason your subjects get sampled could plausibly be related to the variables you’re looking at you need to be careful what you conclude. And it’s worthwhile to remember that even if you’re well calibrated and careful, there’s a good reason that scientific research has a higher status than your individual observations."
  },
  {
    "objectID": "posts/20231216_berkson/index.html#footnotes",
    "href": "posts/20231216_berkson/index.html#footnotes",
    "title": "Oh No! Berkson’s paradox in clinical theories",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWeird according to Mike, that is.↩︎\nYes.↩︎\nI have not looked into it and though I prima facie trust the finding I tend to have a negative reaction to these efforts to squish a bunch of things together and say they’re all part of the same underlying thing.↩︎"
  },
  {
    "objectID": "notes/mediation_sim_may_WIP.html",
    "href": "notes/mediation_sim_may_WIP.html",
    "title": "mediation_sim_may_WIP",
    "section": "",
    "text": "My goal here is to build a model that describes the essential aspects of the data-generating process that our statistical test will attempt to estimate.\nEach individual will be part of either the IU treatment group or the MC treatment group. For the duration of the treatment, the mediator is assumed to gradually change in an approximately linear way. Before considering the comparisons between groups, let’s first imagine a patient in one group with a changing mediator, \\(m^a\\). The residuals of the model \\(\\sigma\\) is assumed to be normally distributed with a mean of 0. \\(t\\) is a variable that represent time, which can be weeks from 0 to 9.\n\\[\nm_i \\sim N(\\mu_i,\\sigma) \\\\\n\\mu_i = \\alpha  +  \\beta t_i\n\\]\nFor now let’s just imagine the data-generating process for a single patient. When generating data parameters like \\(\\alpha\\) and \\(\\beta\\) will have “priors” that determine the distribution that data-points are drawn from. For our purposes now the input in those distribution functions are somewhat arbitrary, and they do not represent any thought through Bayesian priors. Because I’m not yet trying to model uncertainty, I’ll skip specifying \\(\\sigma\\) as a distribution, instead it will be set to = 0.3.\n\\[\n\\alpha \\sim N(0,1) \\\\\n\\beta \\sim N(-0.1,0.05)\n\\]\n\\(\\alpha\\) now contains the normal intercept (week = 0) standard deviation for the mediator, while \\(\\beta\\) represents the average slope of the mediator, as well as the heterogeneity of slopes. Let’s simulate and plot 1 patient:\n\nn &lt;- 1\nmediator_intercept_sd &lt;- 1\navg_mediator_slope &lt;- -0.1\nmediator_slope_sd &lt;- 0.05\nweeks &lt;- 0:9\nsigma &lt;- 0.3\n\nm_1 &lt;- rnorm(n, 0, mediator_intercept_sd) + rnorm(n, avg_mediator_slope, mediator_slope_sd)*weeks + rnorm((n*length(weeks)), 0, sigma)\n\nplot(weeks,m_1, ylim = c(-2,2))\n\n\n\n\n\n\n\n\nSome additional things to be explicit about: \\(m_i\\) is changing linearly, if not for \\(\\sigma\\). This \\(\\sigma\\) can be seen as representing both “measurement error” and exogenous influences. While our treatment is hypothesized to affect the mediator, it is not likely to be the only thing affecting the mediator. We also imagine that the change is heterogenous; the treatment is not expected to work equally for everyone.\nNow let’s model how this might look for many different patients \\(j=1,...,n\\). The value of a datapoint \\(\\mu_i\\) (if not for it’s error/exogenous influences) can now be given by this formula:\n\\[\nm_i \\sim N(\\mu_i,\\sigma) \\\\\n\\mu_{i} = \\alpha_{j[i]} \\ + \\ \\beta_{j[i]}t_i \\\\\n\\alpha_j \\sim N(0,1) \\\\\n\\beta_j \\sim N(-0.1,0.05) \\\\\n\\]\nThe only thing that has changed is that we’re now imagining that the slopes and intercepts beloing to many different patients. To generate this we could simply loop the code many times.\n\nn &lt;- 7 #note that the code calls nr patients \"n\" instead of j.\nmediator_intercept_sd &lt;- 1\navg_mediator_slope &lt;- -0.1\nmediator_slope_sd &lt;- 0.05\nweeks &lt;- 0:9\nsigma &lt;- 0.3\nrho &lt;- -0.15\n\ndf &lt;- data.frame(m = c(), week = c(), id = c())\n\nfor(i in 1:n){\nm_1 &lt;- rnorm(1, 0, mediator_intercept_sd) + rnorm(1, avg_mediator_slope, mediator_slope_sd)*weeks + rnorm((length(weeks)), 0, sigma)\ndf &lt;- rbind.data.frame(df,data.frame(m = m_1, week = weeks, id = rep(i, times = 10)))\n}\n\nIn real data it is common to see a negative slope-intercept correlation where a higher value on an individuals \\(\\alpha\\) would be associated with a more negative \\(\\beta\\). Our data-generating process should model this as well. To achieve this we need to let intercepts and slopes them be drawn from a multivariate normal distribution.\n\\[\n\\begin{align}\ny_i &\\sim N(\\mu_i,\\sigma) \\\\\n\\mu_i &= \\alpha_{j[i]} + \\beta_{j[i]}t_i \\\\\n\\begin{pmatrix} \\alpha_j \\\\ \\beta_j \\end{pmatrix} &\\sim \\text{MVN}\\left(\\begin{pmatrix} \\mu_\\alpha \\\\ \\mu_\\beta \\end{pmatrix}, \\Sigma\\right) \\\\\n\\mu_\\alpha &= 0 \\\\\n\\mu_\\beta &= -0.1 \\\\\n\\Sigma &= \\begin{pmatrix} \\sigma_\\alpha^2 & \\rho\\sigma_\\alpha\\sigma_\\beta \\\\ \\rho\\sigma_\\alpha\\sigma_\\beta & \\sigma_\\beta^2 \\end{pmatrix} \\\\\n\\rho &= -0.15 \\\\\n\\sigma_\\alpha &= 1 \\\\\n\\sigma_\\beta &= 0.05\n\\end{align}\n\\]\n\\(\\Sigma\\) now represent a common covariance matrix for the slopes and intercepts. The diagonal of the matrix represent the intercept and slope variances for earlier, but we now also have a \\(\\rho\\sigma_\\alpha \\sigma_\\beta\\) that represents the correlation. When generating data I’ve set it to -0.15.\nUpdating the code and looping for 7 patients looks like this:\n\n#Now with slope-intercept correlation\nn &lt;- 7 #note that the code calls nr patients \"n\" instead of j.\nmediator_intercept_sd &lt;- 1\navg_mediator_slope &lt;- -0.1\nmediator_slope_sd &lt;- 0.05\nweeks &lt;- 0:9\nsigma &lt;- 0.3\nrho &lt;- -0.15\n\n# Set up multivariate normal parameters\nmu &lt;- c(0, avg_mediator_slope)  # means for intercept and slope\nSigma &lt;- matrix(c(mediator_intercept_sd^2, \n                  rho * mediator_intercept_sd * mediator_slope_sd,\n                  rho * mediator_intercept_sd * mediator_slope_sd, \n                  mediator_slope_sd^2), \n                nrow = 2, ncol = 2)\n\ndf &lt;- data.frame(m = c(), week = c(), id = c())\n\nfor(i in 1:n){\n  # Generate correlated intercept and slope for this participant\n  params &lt;- MASS::mvrnorm(1, mu = mu, Sigma = Sigma)\n  alpha_i &lt;- params[1]  # intercept\n  beta_i &lt;- params[2]   # slope\n  \n  # Calculate values for all weeks for this participant\n  m_1 &lt;- alpha_i + beta_i * weeks + rnorm(length(weeks), 0, sigma)\n  \n  df &lt;- rbind.data.frame(df, data.frame(m = m_1, week = weeks, id = rep(i, times = length(weeks))))\n}\n\nLet’s plot it:\n\np &lt;- ggplot(df, aes(y = m, x = week, group = as.factor(id), color = as.factor(id))) + geom_point(aes(y = m), alpha = 0.8) + \n  geom_line(aes(y = m), alpha = 0.8) +\n  theme_minimal()\n\nprint(p)\n\n\n\n\n\n\n\n\n\n\nThe outcome (worry symptoms) we imagine will be affected by treatment in a similar way, decreasing linearly as patients spend time working with the treatment protocol, while being measured with some degree of error. Importantly the outcome will be partially affected by the mediator and partially will have exogenous influences.\nWe can expand the model to now instead describe two results \\(y_i\\) and \\(m_i\\). Since we model two “outcomes” we also get two residuals \\(\\sigma_y\\) and \\(\\sigma_m\\).\n\\[\n\\begin{align}\ny_i &\\sim N(\\mu_{y,i}, \\sigma_y) \\\\\nm_i &\\sim N(\\mu_{m,i}, \\sigma_m) \\\\\n\\mu_{y,i} &= \\alpha_{y,j[i]} + \\beta_{y,j[i]}t_i + \\gamma_{j[i]}(m_i-\\bar{m_j}) \\\\\n\\mu_{m,i} &= \\alpha_{m,j[i]} + \\beta_{m,j[i]}t_i \\\\\n\\begin{pmatrix} \\alpha_{m,j} \\\\ \\beta_{m,j} \\\\ \\alpha_{y,j} \\\\ \\beta_{y,j} \\\\ \\gamma_j \\end{pmatrix} &\\sim \\text{MVN}\\left(\\begin{pmatrix} \\mu_{\\alpha,m} \\\\ \\mu_{\\beta,m} \\\\ \\mu_{\\alpha,y} \\\\ \\mu_{\\beta,y} \\\\ \\mu_{\\gamma} \\end{pmatrix}, \\Sigma\\right) \\\\\n\\end{align}\n\\]\nA lot of things have happened in this step. Our alphas and betas now have subscripts that both relate them to either the outcome or the mediator. We’ve also added a \\(\\gamma_j\\) which describe the effect of the mediator. Note that this now assumes a constant mediator-outcome relationship for each participant \\(j\\). Note Not centering the parameter meant it basically pushed y values further from zero. Now while centered it instead represents some sort of average slope of the mediator (balancing on an axis in the middle of the dataset…) Constant in the sense that it doesn’t vary over time; the relationship is however allowed to vary by participant.\nImportantly we now have a joint covariance matrix \\(\\Sigma\\) that structures the multivariate normal distribution which all individual level parameters are drawn from.\nExpanded covariance matrix:\n\\[\n\\Sigma = \\begin{pmatrix}\n\\sigma_{\\alpha_m}^2 & \\rho_{\\alpha_m,\\beta_m}\\sigma_{\\alpha_m}\\sigma_{\\beta_m} & \\rho_{\\alpha_m,\\alpha_y}\\sigma_{\\alpha_m}\\sigma_{\\alpha_y} & \\rho_{\\alpha_m,\\beta_y}\\sigma_{\\alpha_m}\\sigma_{\\beta_y} & \\rho_{\\alpha_m,\\gamma}\\sigma_{\\alpha_m}\\sigma_{\\gamma} \\\\\n\\cdot & \\sigma_{\\beta_m}^2 & \\rho_{\\beta_m,\\alpha_y}\\sigma_{\\beta_m}\\sigma_{\\alpha_y} & \\rho_{\\beta_m,\\beta_y}\\sigma_{\\beta_m}\\sigma_{\\beta_y} & \\rho_{\\beta_m,\\gamma}\\sigma_{\\beta_m}\\sigma_{\\gamma} \\\\\n\\cdot & \\cdot & \\sigma_{\\alpha_y}^2 & \\rho_{\\alpha_y,\\beta_y}\\sigma_{\\alpha_y}\\sigma_{\\beta_y} & \\rho_{\\alpha_y,\\gamma}\\sigma_{\\alpha_y}\\sigma_{\\gamma} \\\\\n\\cdot & \\cdot & \\cdot & \\sigma_{\\beta_y}^2 & \\rho_{\\beta_y,\\gamma}\\sigma_{\\beta_y}\\sigma_{\\gamma} \\\\\n\\cdot & \\cdot & \\cdot & \\cdot & \\sigma_{\\gamma}^2\n\\end{pmatrix}\n\\]\nWhere \\(\\alpha_{m,j}, \\beta_{m,j}\\) are mediator intercept and slope for participant \\(j\\). \\(\\alpha_{y,j}, \\beta_{y,j}\\) are their outcome intercept and slope, \\(\\gamma_j\\) is their mediator-outcome relationship.\nThese slopes and intercepts drawn from a multivariate normal distribution. Like before \\(\\sigma_{\\alpha}\\) and \\(\\sigma_{\\beta}\\) describe the intercept standard deviation and heterogeneity of the change over time, but they now have subscripts that specify whether they belong to the mediator or the outcome. We describe heterogeneity of the mediator-outcome relationship with \\(\\sigma_{\\gamma}\\).\nWe now also have ten (!) \\(\\rho\\) terms that capture the correlations between individuals slopes and intercepts.\n\n\n\n\n\\(\\rho_{\\alpha_m,\\beta_m}\\) is the slope-intercept correlation that we previously defined in the model without the outcome. This should probably be slightly negative to reflect that people who are already at a high level in the mediator tend to have less room to get even worse (and vice versa). A “regression towards the mean”-like effect.\n\\(\\rho_{\\alpha_m,\\alpha_y}\\) is the correlation between the intercept of the mediator and the intercept of the outcome. This should be moderately positive to reflect that individuals with a high level of our mediating variable (e.g. intolerance of uncertainty) tend to be more worried (at the start of treatment).\n\\(\\rho_{\\alpha_m,\\beta_y}\\) is more conceptually tricky. This represents whether individuals high on the mediator at start of treatment tend to change more in the outcome. My stab at it would be that this should also be a slight negative correlation, but probably weaker than the slope-intercept correlation for the mediator itself.\n\\(\\rho_{\\alpha_m,\\gamma}\\) represents the relationship between an individuals mediator-outcome relationship, and their initial level of the mediator. For example, do individuals with a higher level of negative metacognitions beforehand also have a stronger overall relationship between their negative metacognitions and their worry? I’m not sure what to make of that. My first hunch is no; while there would be an overall relationship between the mediator and the outcome, \\(\\mu_\\gamma\\), it wouldn’t necessarily vary by initial negative metacognition level.\n\\(\\rho_{\\beta_m,\\alpha_y}\\) is similarly tricky but like \\(\\rho_{\\alpha_m,\\beta_y}\\) I would argue for “slightly negative but less so than within variable intercept-slope correlation”\n\\(\\rho_{\\beta_m,\\beta_y}\\) is potentially a very relevant parameter of interest. This should be positive. Individuals whose mediator change more should have outcomes that change more.\n\\(\\rho_{\\beta_m,\\gamma}\\) is also tricky. It represents how mediator slope relates to the mediator-outcome relationship, for a given participant. One could perhaps frame it as a type of individual differences in treatment response. Perhaps participants where the mediator changes more are the same participants where the mediator is strongly related to the outcome. Now that I type it out, that sounds pretty reasonable.\n\\(\\rho_{\\alpha_y,\\beta_y}\\) is the slope-intercept correlation for the outcome. In line with previous reasoning: probably slightly negative.\n\\(\\rho_{\\alpha_y,\\gamma}\\) is the relationship between initial worry and the mediator-outcome relationship. Since that’s the flipside of \\(\\rho_{\\alpha_m,\\gamma}\\) similar reasoning should hold.\n\\(\\rho_{\\beta_y,\\gamma}\\) is also tricky. It should probably be similar to \\(\\rho_{\\beta_m,\\gamma}\\), so that those whose outcome change more tend to have a stronger mediator-outcome relationship (since we’re imagining they change because of the changing mediator.)\n\nFinally we can complicate the model even further by adding a correlation between the residuals (\\(\\sigma_m\\) and \\(\\sigma_y\\)). The outcome is then instead a linear predictor \\(y_i = \\mu_{y,i} + \\epsilon_{y,i}\\) and \\(\\epsilon\\) comes from a multivariate normal distribution that contain both \\(\\sigma^2_y\\), \\(\\sigma^2_m\\) and their correlation \\(\\rho_{\\epsilon}\\sigma_m\\sigma_y\\).\nShould we posit such a relationship? I think it’s unclear. We’re then saying that despite all these things we’re describing, our \\(\\boldsymbol\\mu\\)s and our \\(\\rho\\)s and our \\(\\gamma\\)s, there’s still an additional, not yet captured, relationship between the mediator and the outcome.\nThe code below contains it, but I will set the relationship to zero when generating data.\n(Note. Unlike the previous code I haven’t done any checks on whether this fully works).\n\nn &lt;- 7 #note that the code calls nr patients \"n\" instead of j.\n\n# Parameters for the 5-dimensional MVN\nmu_alpha_m &lt;- 0        # specify mediator intercept mean\nmu_beta_m &lt;- -0.1      # specify mediator slope mean\nmu_alpha_y &lt;- 0        # specify outcome intercept mean\nmu_beta_y &lt;- -0.11     # specify outcome slope mean  \nmu_gamma &lt;- 0.8       # specify mediator-outcome relationship mean\n\n# Standard deviations for random effects\nsd_alpha_m &lt;- 1        # specify mediator intercept SD\nsd_beta_m &lt;- 0.06      # specify mediator slope SD\nsd_alpha_y &lt;- 0.95      # specify outcome intercept SD\nsd_beta_y &lt;- 0.04      # specify outcome slope SD\nsd_gamma &lt;- 0.2        # specify mediator-outcome relationship SD\n\n# Residual parameters\nsigma_m &lt;- 0.3\nsigma_y &lt;- 0.4\nrho_epsilon &lt;- 0     # residual correlation\n\nweeks &lt;- 0:9\n\n# Set up 5-dimensional multivariate normal parameters\nmu_vec &lt;- c(mu_alpha_m, mu_beta_m, mu_alpha_y, mu_beta_y, mu_gamma)\n\n# Create 5x5 covariance matrix (you can specify correlations as needed)\nrho_alpha_m_beta_m &lt;- -0.15    # mediator slope-intercept correlation\nrho_alpha_m_alpha_y &lt;- 0.9     # mediator-outcome intercept correlation\nrho_alpha_m_beta_y &lt;- -0.1     # mediator-intercept outcome-slope correlation\nrho_alpha_m_gamma &lt;- 0         # mediator intercept-mediator effect correlation\nrho_beta_m_alpha_y &lt;- 0        # mediator slope-outcome intercept correlation\nrho_beta_m_beta_y &lt;- 0.24      # mediator slope-outcome slope correlation\nrho_beta_m_gamma &lt;- 0.25       # mediator slope-mediator effect correlation\n\nrho_alpha_y_beta_y &lt;- - 0.16   # outcome slope-intercept correlation\nrho_alpha_y_gamma &lt;- 0         # outcome intercept-mediator effect correlation\nrho_beta_y_gamma &lt;- 0.20       # outcome slope-mediator effect correlation\n\nSigma_RE &lt;- matrix(0, nrow = 5, ncol = 5)\ndiag(Sigma_RE) &lt;- c(sd_alpha_m^2, sd_beta_m^2, sd_alpha_y^2, sd_beta_y^2, sd_gamma^2)\n\n# Fill in correlations (symmetric matrix)\nSigma_RE[1,2] &lt;- Sigma_RE[2,1] &lt;- rho_alpha_m_beta_m * sd_alpha_m * sd_beta_m\nSigma_RE[1,3] &lt;- Sigma_RE[3,1] &lt;- rho_alpha_m_alpha_y * sd_alpha_m * sd_alpha_y\nSigma_RE[1,4] &lt;- Sigma_RE[4,1] &lt;- rho_alpha_m_beta_y * sd_alpha_m * sd_beta_y\nSigma_RE[1,5] &lt;- Sigma_RE[5,1] &lt;- rho_alpha_m_gamma * sd_alpha_m * sd_gamma\n\nSigma_RE[2,3] &lt;- Sigma_RE[3,2] &lt;- rho_beta_m_alpha_y * sd_beta_m * sd_alpha_y\nSigma_RE[2,4] &lt;- Sigma_RE[4,2] &lt;- rho_beta_m_beta_y * sd_beta_m * sd_beta_y\nSigma_RE[2,5] &lt;- Sigma_RE[5,2] &lt;- rho_beta_m_gamma * sd_beta_m * sd_gamma\n\nSigma_RE[3,4] &lt;- Sigma_RE[4,3] &lt;- rho_alpha_y_beta_y * sd_alpha_y * sd_beta_y\nSigma_RE[3,5] &lt;- Sigma_RE[5,3] &lt;- rho_alpha_y_gamma * sd_alpha_y * sd_gamma\n\nSigma_RE[4,5] &lt;- Sigma_RE[5,4] &lt;- rho_beta_y_gamma * sd_beta_y * sd_gamma\n\n# Residual covariance matrix\nSigma_res &lt;- matrix(c(sigma_m^2, rho_epsilon * sigma_m * sigma_y,\n                      rho_epsilon * sigma_m * sigma_y, sigma_y^2), \n                    nrow = 2, ncol = 2)\n\ndf &lt;- data.frame(m = c(), y = c(), week = c(), id = c())\n\nfor(i in 1:n){\n  # Generate 5 correlated random effects for this participant\n  params &lt;- mvrnorm(1, mu = mu_vec, Sigma = Sigma_RE)\n  alpha_m_i &lt;- params[1]  # mediator intercept\n  beta_m_i &lt;- params[2]   # mediator slope\n  alpha_y_i &lt;- params[3]  # outcome intercept\n  beta_y_i &lt;- params[4]   # outcome slope\n  gamma_i &lt;- params[5]    # mediator-outcome relationship\n  \n  # Generate correlated residuals for all timepoints\n  residuals &lt;- mvrnorm(length(weeks), mu = c(0, 0), Sigma = Sigma_res)\n  epsilon_m &lt;- residuals[, 1]\n  epsilon_y &lt;- residuals[, 2]\n  \n  # Calculate mediator values\n  mu_m &lt;- alpha_m_i + beta_m_i * weeks\n  m_vals &lt;- mu_m + epsilon_m\n  \n    # CENTER THE MEDIATOR BY PARTICIPANT\n  m_centered &lt;- m_vals - mean(m_vals)\n  \n  # Calculate outcome values using CENTERED mediator\n  mu_y &lt;- alpha_y_i + beta_y_i * weeks + gamma_i * m_centered\n  y_vals &lt;- mu_y + epsilon_y\n  \n  df &lt;- rbind.data.frame(df, data.frame(m = m_vals, y = y_vals, week = weeks,\n                                        id = rep(i, times = length(weeks))))\n}\n\nMy aim for this code is that it can be used as a tool to test and make sense of different available methods for mediation analysis.\nFor now, let’s just plot both mediator and\n\np &lt;- ggplot(df, aes(x = week, group = as.factor(id), color = as.factor(id))) + geom_point(aes(y = m), alpha = 0.8) + \n  geom_line(aes(y = m), alpha = 0.8) +\n  geom_point(aes(y = y), alpha = 0.4) +\n  geom_line(aes(y = y), alpha = 0.4, linetype = \"dashed\") +\n  labs(color = \"ID\", y = \"Value\") +\n  theme_minimal()\n\nprint(p)\n\n\n\n\n\n\n\n\n\n\n\nNote. Not yet updated \\(\\gamma\\) to apply to centered \\(m\\)\n\\[\n\\begin{align}\ny_i &= \\mu_{y,i} + \\epsilon_{y,i} \\\\\nm_i &= \\mu_{m,i} + \\epsilon_{m,i} \\\\\n\\mu_{y,i} &= \\alpha_{y,j[i]} + \\beta_{y,j[i]}t_i + \\gamma_{j[i]}m_i \\\\\n\\mu_{m,i} &= \\alpha_{m,j[i]} + \\beta_{m,j[i]}t_i \\\\\n\\begin{pmatrix} \\alpha_{m,j} \\\\ \\beta_{m,j} \\\\ \\alpha_{y,j} \\\\ \\beta_{y,j} \\\\ \\gamma_j \\end{pmatrix} &\\sim \\text{MVN}\\left(\\begin{pmatrix} \\mu_{\\alpha,m} \\\\ \\mu_{\\beta,m} \\\\ \\mu_{\\alpha,y} \\\\ \\mu_{\\beta,y} \\\\ \\mu_{\\gamma} \\end{pmatrix}, \\Sigma_{RE}\\right) \\\\\n\\begin{pmatrix} \\epsilon_{m,i} \\\\ \\epsilon_{y,i} \\end{pmatrix} &\\sim \\text{MVN}\\left(\\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} \\sigma_m^2 & \\rho_{\\epsilon}\\sigma_m\\sigma_y \\\\ \\rho_{\\epsilon}\\sigma_m\\sigma_y & \\sigma_y^2 \\end{pmatrix}\\right)\n\\end{align}\n\\]"
  },
  {
    "objectID": "notes/mediation_sim_may_WIP.html#building-a-generative-model-step-by-step",
    "href": "notes/mediation_sim_may_WIP.html#building-a-generative-model-step-by-step",
    "title": "mediation_sim_may_WIP",
    "section": "",
    "text": "My goal here is to build a model that describes the essential aspects of the data-generating process that our statistical test will attempt to estimate.\nEach individual will be part of either the IU treatment group or the MC treatment group. For the duration of the treatment, the mediator is assumed to gradually change in an approximately linear way. Before considering the comparisons between groups, let’s first imagine a patient in one group with a changing mediator, \\(m^a\\). The residuals of the model \\(\\sigma\\) is assumed to be normally distributed with a mean of 0. \\(t\\) is a variable that represent time, which can be weeks from 0 to 9.\n\\[\nm_i \\sim N(\\mu_i,\\sigma) \\\\\n\\mu_i = \\alpha  +  \\beta t_i\n\\]\nFor now let’s just imagine the data-generating process for a single patient. When generating data parameters like \\(\\alpha\\) and \\(\\beta\\) will have “priors” that determine the distribution that data-points are drawn from. For our purposes now the input in those distribution functions are somewhat arbitrary, and they do not represent any thought through Bayesian priors. Because I’m not yet trying to model uncertainty, I’ll skip specifying \\(\\sigma\\) as a distribution, instead it will be set to = 0.3.\n\\[\n\\alpha \\sim N(0,1) \\\\\n\\beta \\sim N(-0.1,0.05)\n\\]\n\\(\\alpha\\) now contains the normal intercept (week = 0) standard deviation for the mediator, while \\(\\beta\\) represents the average slope of the mediator, as well as the heterogeneity of slopes. Let’s simulate and plot 1 patient:\n\nn &lt;- 1\nmediator_intercept_sd &lt;- 1\navg_mediator_slope &lt;- -0.1\nmediator_slope_sd &lt;- 0.05\nweeks &lt;- 0:9\nsigma &lt;- 0.3\n\nm_1 &lt;- rnorm(n, 0, mediator_intercept_sd) + rnorm(n, avg_mediator_slope, mediator_slope_sd)*weeks + rnorm((n*length(weeks)), 0, sigma)\n\nplot(weeks,m_1, ylim = c(-2,2))\n\n\n\n\n\n\n\n\nSome additional things to be explicit about: \\(m_i\\) is changing linearly, if not for \\(\\sigma\\). This \\(\\sigma\\) can be seen as representing both “measurement error” and exogenous influences. While our treatment is hypothesized to affect the mediator, it is not likely to be the only thing affecting the mediator. We also imagine that the change is heterogenous; the treatment is not expected to work equally for everyone.\nNow let’s model how this might look for many different patients \\(j=1,...,n\\). The value of a datapoint \\(\\mu_i\\) (if not for it’s error/exogenous influences) can now be given by this formula:\n\\[\nm_i \\sim N(\\mu_i,\\sigma) \\\\\n\\mu_{i} = \\alpha_{j[i]} \\ + \\ \\beta_{j[i]}t_i \\\\\n\\alpha_j \\sim N(0,1) \\\\\n\\beta_j \\sim N(-0.1,0.05) \\\\\n\\]\nThe only thing that has changed is that we’re now imagining that the slopes and intercepts beloing to many different patients. To generate this we could simply loop the code many times.\n\nn &lt;- 7 #note that the code calls nr patients \"n\" instead of j.\nmediator_intercept_sd &lt;- 1\navg_mediator_slope &lt;- -0.1\nmediator_slope_sd &lt;- 0.05\nweeks &lt;- 0:9\nsigma &lt;- 0.3\nrho &lt;- -0.15\n\ndf &lt;- data.frame(m = c(), week = c(), id = c())\n\nfor(i in 1:n){\nm_1 &lt;- rnorm(1, 0, mediator_intercept_sd) + rnorm(1, avg_mediator_slope, mediator_slope_sd)*weeks + rnorm((length(weeks)), 0, sigma)\ndf &lt;- rbind.data.frame(df,data.frame(m = m_1, week = weeks, id = rep(i, times = 10)))\n}\n\nIn real data it is common to see a negative slope-intercept correlation where a higher value on an individuals \\(\\alpha\\) would be associated with a more negative \\(\\beta\\). Our data-generating process should model this as well. To achieve this we need to let intercepts and slopes them be drawn from a multivariate normal distribution.\n\\[\n\\begin{align}\ny_i &\\sim N(\\mu_i,\\sigma) \\\\\n\\mu_i &= \\alpha_{j[i]} + \\beta_{j[i]}t_i \\\\\n\\begin{pmatrix} \\alpha_j \\\\ \\beta_j \\end{pmatrix} &\\sim \\text{MVN}\\left(\\begin{pmatrix} \\mu_\\alpha \\\\ \\mu_\\beta \\end{pmatrix}, \\Sigma\\right) \\\\\n\\mu_\\alpha &= 0 \\\\\n\\mu_\\beta &= -0.1 \\\\\n\\Sigma &= \\begin{pmatrix} \\sigma_\\alpha^2 & \\rho\\sigma_\\alpha\\sigma_\\beta \\\\ \\rho\\sigma_\\alpha\\sigma_\\beta & \\sigma_\\beta^2 \\end{pmatrix} \\\\\n\\rho &= -0.15 \\\\\n\\sigma_\\alpha &= 1 \\\\\n\\sigma_\\beta &= 0.05\n\\end{align}\n\\]\n\\(\\Sigma\\) now represent a common covariance matrix for the slopes and intercepts. The diagonal of the matrix represent the intercept and slope variances for earlier, but we now also have a \\(\\rho\\sigma_\\alpha \\sigma_\\beta\\) that represents the correlation. When generating data I’ve set it to -0.15.\nUpdating the code and looping for 7 patients looks like this:\n\n#Now with slope-intercept correlation\nn &lt;- 7 #note that the code calls nr patients \"n\" instead of j.\nmediator_intercept_sd &lt;- 1\navg_mediator_slope &lt;- -0.1\nmediator_slope_sd &lt;- 0.05\nweeks &lt;- 0:9\nsigma &lt;- 0.3\nrho &lt;- -0.15\n\n# Set up multivariate normal parameters\nmu &lt;- c(0, avg_mediator_slope)  # means for intercept and slope\nSigma &lt;- matrix(c(mediator_intercept_sd^2, \n                  rho * mediator_intercept_sd * mediator_slope_sd,\n                  rho * mediator_intercept_sd * mediator_slope_sd, \n                  mediator_slope_sd^2), \n                nrow = 2, ncol = 2)\n\ndf &lt;- data.frame(m = c(), week = c(), id = c())\n\nfor(i in 1:n){\n  # Generate correlated intercept and slope for this participant\n  params &lt;- MASS::mvrnorm(1, mu = mu, Sigma = Sigma)\n  alpha_i &lt;- params[1]  # intercept\n  beta_i &lt;- params[2]   # slope\n  \n  # Calculate values for all weeks for this participant\n  m_1 &lt;- alpha_i + beta_i * weeks + rnorm(length(weeks), 0, sigma)\n  \n  df &lt;- rbind.data.frame(df, data.frame(m = m_1, week = weeks, id = rep(i, times = length(weeks))))\n}\n\nLet’s plot it:\n\np &lt;- ggplot(df, aes(y = m, x = week, group = as.factor(id), color = as.factor(id))) + geom_point(aes(y = m), alpha = 0.8) + \n  geom_line(aes(y = m), alpha = 0.8) +\n  theme_minimal()\n\nprint(p)\n\n\n\n\n\n\n\n\n\n\nThe outcome (worry symptoms) we imagine will be affected by treatment in a similar way, decreasing linearly as patients spend time working with the treatment protocol, while being measured with some degree of error. Importantly the outcome will be partially affected by the mediator and partially will have exogenous influences.\nWe can expand the model to now instead describe two results \\(y_i\\) and \\(m_i\\). Since we model two “outcomes” we also get two residuals \\(\\sigma_y\\) and \\(\\sigma_m\\).\n\\[\n\\begin{align}\ny_i &\\sim N(\\mu_{y,i}, \\sigma_y) \\\\\nm_i &\\sim N(\\mu_{m,i}, \\sigma_m) \\\\\n\\mu_{y,i} &= \\alpha_{y,j[i]} + \\beta_{y,j[i]}t_i + \\gamma_{j[i]}(m_i-\\bar{m_j}) \\\\\n\\mu_{m,i} &= \\alpha_{m,j[i]} + \\beta_{m,j[i]}t_i \\\\\n\\begin{pmatrix} \\alpha_{m,j} \\\\ \\beta_{m,j} \\\\ \\alpha_{y,j} \\\\ \\beta_{y,j} \\\\ \\gamma_j \\end{pmatrix} &\\sim \\text{MVN}\\left(\\begin{pmatrix} \\mu_{\\alpha,m} \\\\ \\mu_{\\beta,m} \\\\ \\mu_{\\alpha,y} \\\\ \\mu_{\\beta,y} \\\\ \\mu_{\\gamma} \\end{pmatrix}, \\Sigma\\right) \\\\\n\\end{align}\n\\]\nA lot of things have happened in this step. Our alphas and betas now have subscripts that both relate them to either the outcome or the mediator. We’ve also added a \\(\\gamma_j\\) which describe the effect of the mediator. Note that this now assumes a constant mediator-outcome relationship for each participant \\(j\\). Note Not centering the parameter meant it basically pushed y values further from zero. Now while centered it instead represents some sort of average slope of the mediator (balancing on an axis in the middle of the dataset…) Constant in the sense that it doesn’t vary over time; the relationship is however allowed to vary by participant.\nImportantly we now have a joint covariance matrix \\(\\Sigma\\) that structures the multivariate normal distribution which all individual level parameters are drawn from.\nExpanded covariance matrix:\n\\[\n\\Sigma = \\begin{pmatrix}\n\\sigma_{\\alpha_m}^2 & \\rho_{\\alpha_m,\\beta_m}\\sigma_{\\alpha_m}\\sigma_{\\beta_m} & \\rho_{\\alpha_m,\\alpha_y}\\sigma_{\\alpha_m}\\sigma_{\\alpha_y} & \\rho_{\\alpha_m,\\beta_y}\\sigma_{\\alpha_m}\\sigma_{\\beta_y} & \\rho_{\\alpha_m,\\gamma}\\sigma_{\\alpha_m}\\sigma_{\\gamma} \\\\\n\\cdot & \\sigma_{\\beta_m}^2 & \\rho_{\\beta_m,\\alpha_y}\\sigma_{\\beta_m}\\sigma_{\\alpha_y} & \\rho_{\\beta_m,\\beta_y}\\sigma_{\\beta_m}\\sigma_{\\beta_y} & \\rho_{\\beta_m,\\gamma}\\sigma_{\\beta_m}\\sigma_{\\gamma} \\\\\n\\cdot & \\cdot & \\sigma_{\\alpha_y}^2 & \\rho_{\\alpha_y,\\beta_y}\\sigma_{\\alpha_y}\\sigma_{\\beta_y} & \\rho_{\\alpha_y,\\gamma}\\sigma_{\\alpha_y}\\sigma_{\\gamma} \\\\\n\\cdot & \\cdot & \\cdot & \\sigma_{\\beta_y}^2 & \\rho_{\\beta_y,\\gamma}\\sigma_{\\beta_y}\\sigma_{\\gamma} \\\\\n\\cdot & \\cdot & \\cdot & \\cdot & \\sigma_{\\gamma}^2\n\\end{pmatrix}\n\\]\nWhere \\(\\alpha_{m,j}, \\beta_{m,j}\\) are mediator intercept and slope for participant \\(j\\). \\(\\alpha_{y,j}, \\beta_{y,j}\\) are their outcome intercept and slope, \\(\\gamma_j\\) is their mediator-outcome relationship.\nThese slopes and intercepts drawn from a multivariate normal distribution. Like before \\(\\sigma_{\\alpha}\\) and \\(\\sigma_{\\beta}\\) describe the intercept standard deviation and heterogeneity of the change over time, but they now have subscripts that specify whether they belong to the mediator or the outcome. We describe heterogeneity of the mediator-outcome relationship with \\(\\sigma_{\\gamma}\\).\nWe now also have ten (!) \\(\\rho\\) terms that capture the correlations between individuals slopes and intercepts.\n\n\n\n\n\\(\\rho_{\\alpha_m,\\beta_m}\\) is the slope-intercept correlation that we previously defined in the model without the outcome. This should probably be slightly negative to reflect that people who are already at a high level in the mediator tend to have less room to get even worse (and vice versa). A “regression towards the mean”-like effect.\n\\(\\rho_{\\alpha_m,\\alpha_y}\\) is the correlation between the intercept of the mediator and the intercept of the outcome. This should be moderately positive to reflect that individuals with a high level of our mediating variable (e.g. intolerance of uncertainty) tend to be more worried (at the start of treatment).\n\\(\\rho_{\\alpha_m,\\beta_y}\\) is more conceptually tricky. This represents whether individuals high on the mediator at start of treatment tend to change more in the outcome. My stab at it would be that this should also be a slight negative correlation, but probably weaker than the slope-intercept correlation for the mediator itself.\n\\(\\rho_{\\alpha_m,\\gamma}\\) represents the relationship between an individuals mediator-outcome relationship, and their initial level of the mediator. For example, do individuals with a higher level of negative metacognitions beforehand also have a stronger overall relationship between their negative metacognitions and their worry? I’m not sure what to make of that. My first hunch is no; while there would be an overall relationship between the mediator and the outcome, \\(\\mu_\\gamma\\), it wouldn’t necessarily vary by initial negative metacognition level.\n\\(\\rho_{\\beta_m,\\alpha_y}\\) is similarly tricky but like \\(\\rho_{\\alpha_m,\\beta_y}\\) I would argue for “slightly negative but less so than within variable intercept-slope correlation”\n\\(\\rho_{\\beta_m,\\beta_y}\\) is potentially a very relevant parameter of interest. This should be positive. Individuals whose mediator change more should have outcomes that change more.\n\\(\\rho_{\\beta_m,\\gamma}\\) is also tricky. It represents how mediator slope relates to the mediator-outcome relationship, for a given participant. One could perhaps frame it as a type of individual differences in treatment response. Perhaps participants where the mediator changes more are the same participants where the mediator is strongly related to the outcome. Now that I type it out, that sounds pretty reasonable.\n\\(\\rho_{\\alpha_y,\\beta_y}\\) is the slope-intercept correlation for the outcome. In line with previous reasoning: probably slightly negative.\n\\(\\rho_{\\alpha_y,\\gamma}\\) is the relationship between initial worry and the mediator-outcome relationship. Since that’s the flipside of \\(\\rho_{\\alpha_m,\\gamma}\\) similar reasoning should hold.\n\\(\\rho_{\\beta_y,\\gamma}\\) is also tricky. It should probably be similar to \\(\\rho_{\\beta_m,\\gamma}\\), so that those whose outcome change more tend to have a stronger mediator-outcome relationship (since we’re imagining they change because of the changing mediator.)\n\nFinally we can complicate the model even further by adding a correlation between the residuals (\\(\\sigma_m\\) and \\(\\sigma_y\\)). The outcome is then instead a linear predictor \\(y_i = \\mu_{y,i} + \\epsilon_{y,i}\\) and \\(\\epsilon\\) comes from a multivariate normal distribution that contain both \\(\\sigma^2_y\\), \\(\\sigma^2_m\\) and their correlation \\(\\rho_{\\epsilon}\\sigma_m\\sigma_y\\).\nShould we posit such a relationship? I think it’s unclear. We’re then saying that despite all these things we’re describing, our \\(\\boldsymbol\\mu\\)s and our \\(\\rho\\)s and our \\(\\gamma\\)s, there’s still an additional, not yet captured, relationship between the mediator and the outcome.\nThe code below contains it, but I will set the relationship to zero when generating data.\n(Note. Unlike the previous code I haven’t done any checks on whether this fully works).\n\nn &lt;- 7 #note that the code calls nr patients \"n\" instead of j.\n\n# Parameters for the 5-dimensional MVN\nmu_alpha_m &lt;- 0        # specify mediator intercept mean\nmu_beta_m &lt;- -0.1      # specify mediator slope mean\nmu_alpha_y &lt;- 0        # specify outcome intercept mean\nmu_beta_y &lt;- -0.11     # specify outcome slope mean  \nmu_gamma &lt;- 0.8       # specify mediator-outcome relationship mean\n\n# Standard deviations for random effects\nsd_alpha_m &lt;- 1        # specify mediator intercept SD\nsd_beta_m &lt;- 0.06      # specify mediator slope SD\nsd_alpha_y &lt;- 0.95      # specify outcome intercept SD\nsd_beta_y &lt;- 0.04      # specify outcome slope SD\nsd_gamma &lt;- 0.2        # specify mediator-outcome relationship SD\n\n# Residual parameters\nsigma_m &lt;- 0.3\nsigma_y &lt;- 0.4\nrho_epsilon &lt;- 0     # residual correlation\n\nweeks &lt;- 0:9\n\n# Set up 5-dimensional multivariate normal parameters\nmu_vec &lt;- c(mu_alpha_m, mu_beta_m, mu_alpha_y, mu_beta_y, mu_gamma)\n\n# Create 5x5 covariance matrix (you can specify correlations as needed)\nrho_alpha_m_beta_m &lt;- -0.15    # mediator slope-intercept correlation\nrho_alpha_m_alpha_y &lt;- 0.9     # mediator-outcome intercept correlation\nrho_alpha_m_beta_y &lt;- -0.1     # mediator-intercept outcome-slope correlation\nrho_alpha_m_gamma &lt;- 0         # mediator intercept-mediator effect correlation\nrho_beta_m_alpha_y &lt;- 0        # mediator slope-outcome intercept correlation\nrho_beta_m_beta_y &lt;- 0.24      # mediator slope-outcome slope correlation\nrho_beta_m_gamma &lt;- 0.25       # mediator slope-mediator effect correlation\n\nrho_alpha_y_beta_y &lt;- - 0.16   # outcome slope-intercept correlation\nrho_alpha_y_gamma &lt;- 0         # outcome intercept-mediator effect correlation\nrho_beta_y_gamma &lt;- 0.20       # outcome slope-mediator effect correlation\n\nSigma_RE &lt;- matrix(0, nrow = 5, ncol = 5)\ndiag(Sigma_RE) &lt;- c(sd_alpha_m^2, sd_beta_m^2, sd_alpha_y^2, sd_beta_y^2, sd_gamma^2)\n\n# Fill in correlations (symmetric matrix)\nSigma_RE[1,2] &lt;- Sigma_RE[2,1] &lt;- rho_alpha_m_beta_m * sd_alpha_m * sd_beta_m\nSigma_RE[1,3] &lt;- Sigma_RE[3,1] &lt;- rho_alpha_m_alpha_y * sd_alpha_m * sd_alpha_y\nSigma_RE[1,4] &lt;- Sigma_RE[4,1] &lt;- rho_alpha_m_beta_y * sd_alpha_m * sd_beta_y\nSigma_RE[1,5] &lt;- Sigma_RE[5,1] &lt;- rho_alpha_m_gamma * sd_alpha_m * sd_gamma\n\nSigma_RE[2,3] &lt;- Sigma_RE[3,2] &lt;- rho_beta_m_alpha_y * sd_beta_m * sd_alpha_y\nSigma_RE[2,4] &lt;- Sigma_RE[4,2] &lt;- rho_beta_m_beta_y * sd_beta_m * sd_beta_y\nSigma_RE[2,5] &lt;- Sigma_RE[5,2] &lt;- rho_beta_m_gamma * sd_beta_m * sd_gamma\n\nSigma_RE[3,4] &lt;- Sigma_RE[4,3] &lt;- rho_alpha_y_beta_y * sd_alpha_y * sd_beta_y\nSigma_RE[3,5] &lt;- Sigma_RE[5,3] &lt;- rho_alpha_y_gamma * sd_alpha_y * sd_gamma\n\nSigma_RE[4,5] &lt;- Sigma_RE[5,4] &lt;- rho_beta_y_gamma * sd_beta_y * sd_gamma\n\n# Residual covariance matrix\nSigma_res &lt;- matrix(c(sigma_m^2, rho_epsilon * sigma_m * sigma_y,\n                      rho_epsilon * sigma_m * sigma_y, sigma_y^2), \n                    nrow = 2, ncol = 2)\n\ndf &lt;- data.frame(m = c(), y = c(), week = c(), id = c())\n\nfor(i in 1:n){\n  # Generate 5 correlated random effects for this participant\n  params &lt;- mvrnorm(1, mu = mu_vec, Sigma = Sigma_RE)\n  alpha_m_i &lt;- params[1]  # mediator intercept\n  beta_m_i &lt;- params[2]   # mediator slope\n  alpha_y_i &lt;- params[3]  # outcome intercept\n  beta_y_i &lt;- params[4]   # outcome slope\n  gamma_i &lt;- params[5]    # mediator-outcome relationship\n  \n  # Generate correlated residuals for all timepoints\n  residuals &lt;- mvrnorm(length(weeks), mu = c(0, 0), Sigma = Sigma_res)\n  epsilon_m &lt;- residuals[, 1]\n  epsilon_y &lt;- residuals[, 2]\n  \n  # Calculate mediator values\n  mu_m &lt;- alpha_m_i + beta_m_i * weeks\n  m_vals &lt;- mu_m + epsilon_m\n  \n    # CENTER THE MEDIATOR BY PARTICIPANT\n  m_centered &lt;- m_vals - mean(m_vals)\n  \n  # Calculate outcome values using CENTERED mediator\n  mu_y &lt;- alpha_y_i + beta_y_i * weeks + gamma_i * m_centered\n  y_vals &lt;- mu_y + epsilon_y\n  \n  df &lt;- rbind.data.frame(df, data.frame(m = m_vals, y = y_vals, week = weeks,\n                                        id = rep(i, times = length(weeks))))\n}\n\nMy aim for this code is that it can be used as a tool to test and make sense of different available methods for mediation analysis.\nFor now, let’s just plot both mediator and\n\np &lt;- ggplot(df, aes(x = week, group = as.factor(id), color = as.factor(id))) + geom_point(aes(y = m), alpha = 0.8) + \n  geom_line(aes(y = m), alpha = 0.8) +\n  geom_point(aes(y = y), alpha = 0.4) +\n  geom_line(aes(y = y), alpha = 0.4, linetype = \"dashed\") +\n  labs(color = \"ID\", y = \"Value\") +\n  theme_minimal()\n\nprint(p)\n\n\n\n\n\n\n\n\n\n\n\nNote. Not yet updated \\(\\gamma\\) to apply to centered \\(m\\)\n\\[\n\\begin{align}\ny_i &= \\mu_{y,i} + \\epsilon_{y,i} \\\\\nm_i &= \\mu_{m,i} + \\epsilon_{m,i} \\\\\n\\mu_{y,i} &= \\alpha_{y,j[i]} + \\beta_{y,j[i]}t_i + \\gamma_{j[i]}m_i \\\\\n\\mu_{m,i} &= \\alpha_{m,j[i]} + \\beta_{m,j[i]}t_i \\\\\n\\begin{pmatrix} \\alpha_{m,j} \\\\ \\beta_{m,j} \\\\ \\alpha_{y,j} \\\\ \\beta_{y,j} \\\\ \\gamma_j \\end{pmatrix} &\\sim \\text{MVN}\\left(\\begin{pmatrix} \\mu_{\\alpha,m} \\\\ \\mu_{\\beta,m} \\\\ \\mu_{\\alpha,y} \\\\ \\mu_{\\beta,y} \\\\ \\mu_{\\gamma} \\end{pmatrix}, \\Sigma_{RE}\\right) \\\\\n\\begin{pmatrix} \\epsilon_{m,i} \\\\ \\epsilon_{y,i} \\end{pmatrix} &\\sim \\text{MVN}\\left(\\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} \\sigma_m^2 & \\rho_{\\epsilon}\\sigma_m\\sigma_y \\\\ \\rho_{\\epsilon}\\sigma_m\\sigma_y & \\sigma_y^2 \\end{pmatrix}\\right)\n\\end{align}\n\\]"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "personal substack",
    "section": "",
    "text": "I write my personal blog at Substack. It’s called unconfusion and so far mostly concerns online discourse. I find some posts age quickly, but I stood by them when I wrote them and I can never change that. I’ll cross-post some statistics or psychology related under “R/stats posts”."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About Me",
    "section": "",
    "text": "I’m a PhD student in psychology at Karolinska Institutet, Stockholm. My research concerns Cognitive Behavioral Therapy as guided self-help online (ICBT) and Generalized Anxiety Disorder (GAD). I’m specifically focused on whether the mechanisms of change implied by our cognitive models of worry is actually at work within (I)CBT treatments. Other than that I have an interest in methods and meta-science and organize the ReproducibiliTea Journal Club at KI.\n\n\n\nI’ve been generally interested in psychology since I was a teenager. Some areas I’m especially nerdy about are:\n\nCognitive biases in decision making and belief formation.\nMotivated cognition.\nWhat are beliefs even?\nThe neuroscience of emotion.\nPsychiatric diagnostic reasoning and nosology.\n\n\n\n\nDungeons and Dragons. Kino. Going on a trek in the Swedish mountains. Graphic novels. Radiohead.\nThank you for checking out my website!"
  },
  {
    "objectID": "index.html#professional-research-interests",
    "href": "index.html#professional-research-interests",
    "title": "About Me",
    "section": "",
    "text": "I’m a PhD student in psychology at Karolinska Institutet, Stockholm. My research concerns Cognitive Behavioral Therapy as guided self-help online (ICBT) and Generalized Anxiety Disorder (GAD). I’m specifically focused on whether the mechanisms of change implied by our cognitive models of worry is actually at work within (I)CBT treatments. Other than that I have an interest in methods and meta-science and organize the ReproducibiliTea Journal Club at KI."
  },
  {
    "objectID": "index.html#less-professional-research-interests",
    "href": "index.html#less-professional-research-interests",
    "title": "About Me",
    "section": "",
    "text": "I’ve been generally interested in psychology since I was a teenager. Some areas I’m especially nerdy about are:\n\nCognitive biases in decision making and belief formation.\nMotivated cognition.\nWhat are beliefs even?\nThe neuroscience of emotion.\nPsychiatric diagnostic reasoning and nosology."
  },
  {
    "objectID": "index.html#abjectly-uprofessional-interests",
    "href": "index.html#abjectly-uprofessional-interests",
    "title": "About Me",
    "section": "",
    "text": "Dungeons and Dragons. Kino. Going on a trek in the Swedish mountains. Graphic novels. Radiohead."
  },
  {
    "objectID": "notes/Stats2IndividualAssignment.html",
    "href": "notes/Stats2IndividualAssignment.html",
    "title": "Stats2IndividualAssignment",
    "section": "",
    "text": "My data will be self-rated GAD-7 scores from a clinical trial. Patients receive one out of two possible active treatments over ten weeks. I want to use multi-level modeling to model the effect of treatment over time, and compare that between treatments.\nThe formula is presented below for 20 patients (\\(j\\)). My outcome \\(y\\) is GAD-7 score. \\(time\\) will be the week the measurement comes from, ranging from 0 to 9, so that the intercepts \\(\\alpha\\) represent model estimations at start of treatment.\n\\[\n\\begin{align*}\ny_i \\sim Normal(\\mu_i,\\sigma) \\\\\n\\mu_{i,j} = \\alpha_{patient[i]} \\ + \\ \\beta_{patient[i]}*time_i \\\\\n\\alpha_j \\sim Normal(\\bar{\\alpha},\\sigma_a) \\ \\ ,for \\ j = 1,2,...20 \\\\\n\\bar{\\alpha} \\sim Normal(0,1) \\\\\n\\sigma_a \\sim Exponential(1) \\\\\n\\beta_j \\sim Normal(\\bar{\\beta},\\sigma_b) \\ \\ ,for \\ j = 1,2,...20 \\\\\n\\bar{\\beta} \\sim Normal(0,1) \\\\\n\\sigma_b \\sim Exponential(1) \\\\\n\\sigma \\sim Exponential(1)\n\\end{align*}\n\\]\nI have not figured out how to specify a group effect yet but one cheap solution is to just fit two models and compare. In this case that solution is probably adequate since I don’t want the estimate for group A to affect the estimate from group B anyway. However, in time I want to look at contrasts between competing mediators and then it may be better to include group as a factor.\nI have a function that generates data in long format from a previous project. Since the data simulates intercepts ~ Normal(0,1), it can be read as if I were to standardize the scores at week 0. With this standardization the avg_slope argument times the length of the treatment nweeks corresponds to the standardized effect size for growth models following Feingold (2007).\n\n#function that simulates data\nlong.data.sim &lt;- function(n = 30, avg_slope = 0.1, slope_intercept_correlation = -0.15, slope_sigma = 0.1, nweeks = 10, error = 0.3){\n  intercepts &lt;- rnorm(n, m = 0, sd = 1)\n  slopes &lt;- avg_slope + (slope_intercept_correlation*intercepts + rnorm(n,m=0,sd=sqrt(1-slope_intercept_correlation^2)) )*slope_sigma\n  weeks &lt;- 0:(nweeks-1)\n  df &lt;- data.frame()\n  for(i in 1:n){\n    err &lt;- rnorm(nweeks, m = 0, sd = error) #will equal residual std.dev. = sigma(mdl)\n    df &lt;- rbind.data.frame(df,data.frame(id=rep(i, times = nweeks),y=intercepts[i]+slopes[i]*weeks+err,weeks))\n  }\n  return(df)\n}\n\n#generate data\nd &lt;- long.data.sim(n = 20, avg_slope = 0.2, slope_intercept_correlation = -0.15, slope_sigma = 0.1, nweeks = 10, error = 0.3)\n\n#Let's also see if it runs if some patients have missing data some weeks (it will)\nd &lt;- d[-sample(1:nrow(d), 15, replace = FALSE),]\n\nI then built the models step by step increasing complexity.\n\n#single level model (with quadratic aproximation)\nmdl_non &lt;- quap(alist(\n  y ~ dnorm(mu,sigma),\n  mu &lt;- a + b*weeks,\n  a ~ dnorm(0,1),\n  b ~ dnorm(0,0.3),\n  sigma ~ dexp(1)\n), data = d)\nprecis(mdl_non)\n\n            mean         sd      5.5%     94.5%\na     0.07157867 0.14607351 -0.161875 0.3050323\nb     0.21349254 0.02770973  0.169207 0.2577780\nsigma 1.09237163 0.05654070  1.002009 1.1827346\n\n#multi-level model, random intercepts.\nmdl_random_intercept &lt;- ulam(alist(\n  y ~ dnorm(mu,sigma),\n  mu &lt;- a[id] + b*weeks,\n  a[id] ~ dnorm(a_bar,sigma_a),\n  a_bar ~ dnorm(0,1),\n  sigma_a ~ dexp(1),\n  b ~ dnorm(0,0.3),\n  sigma ~ dexp(1)\n), data = d, chains = 4, cores = 4)\n\nIn file included from stan/lib/stan_math/stan/math/prim/prob/von_mises_lccdf.hpp:5,\n                 from stan/lib/stan_math/stan/math/prim/prob/von_mises_ccdf_log.hpp:4,\n                 from stan/lib/stan_math/stan/math/prim/prob.hpp:356,\n                 from stan/lib/stan_math/stan/math/prim.hpp:16,\n                 from stan/lib/stan_math/stan/math/rev.hpp:14,\n                 from stan/lib/stan_math/stan/math.hpp:19,\n                 from stan/src/stan/model/model_header.hpp:4,\n                 from C:/Users/vilgo/AppData/Local/Temp/Rtmpy0zvj8/model-28bc500540dc.hpp:2:\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp: In function 'stan::return_type_t&lt;T_x, T_sigma, T_l&gt; stan::math::von_mises_cdf(const T_x&, const T_mu&, const T_k&)':\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: '-Wmisleading-indentation' is disabled from this point onwards, since column-tracking was disabled due to the size of the code/headers\n  194 |       if (cdf_n &lt; 0.0)\n      | \n\n\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: adding '-flarge-source-files' will allow for more column-tracking support, at the expense of compilation time and memory\n\n\nRunning MCMC with 4 parallel chains, with 1 thread(s) per chain...\n\nChain 1 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 1 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 1 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 1 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 1 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 2 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 2 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 3 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 3 Iteration: 100 / 1000 [ 10%]  (Warmup) \n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: normal_lpdf: Scale parameter is 0, but must be positive! (in 'C:/Users/vilgo/AppData/Local/Temp/Rtmpy0zvj8/model-28bc500540dc.stan', line 19, column 4 to column 34)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 4 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 1 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 1 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 1 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 1 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 1 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 1 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 1 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 1 finished in 0.2 seconds.\nChain 2 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 2 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 2 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 2 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 2 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 2 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 2 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 2 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 2 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 2 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 2 finished in 0.2 seconds.\nChain 3 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 3 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 3 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 3 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 3 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 3 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 3 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 3 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 3 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 3 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 3 finished in 0.2 seconds.\nChain 4 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 4 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 4 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 4 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 4 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 4 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 4 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 4 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 4 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 4 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 4 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 4 finished in 0.2 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.2 seconds.\nTotal execution time: 0.4 seconds.\n\nprecis(mdl_random_intercept, depth = 2)  #seems to run fine with missing rows.\n\n               mean         sd        5.5%       94.5%      rhat ess_bulk\na[1]    -0.02520281 0.14921578 -0.26603355  0.21877512 1.0010833 2202.248\na[2]    -1.10287172 0.13450625 -1.31535830 -0.89130662 1.0038579 3926.797\na[3]    -0.73813835 0.13973373 -0.95917408 -0.51288988 1.0032422 3060.956\na[4]    -0.65834556 0.13519752 -0.86900740 -0.44759664 1.0067085 3126.022\na[5]    -1.15892221 0.13544008 -1.37606300 -0.94658577 1.0046948 3002.016\na[6]     0.75545638 0.13922541  0.53084204  0.97754093 0.9996994 2850.140\na[7]     0.06189972 0.13673783 -0.15652840  0.28013837 1.0027208 2742.055\na[8]    -0.66021047 0.14058414 -0.87995546 -0.43961912 0.9990020 2557.988\na[9]     0.77055111 0.13624650  0.55998342  0.99166465 1.0018397 3225.269\na[10]    2.70448036 0.14066007  2.47965205  2.93030730 1.0014380 2975.519\na[11]    2.28466059 0.14146399  2.04461900  2.50176265 1.0026248 3201.186\na[12]    0.40256383 0.13609763  0.18062447  0.61482962 1.0008079 2271.321\na[13]   -0.76115211 0.13820998 -0.98172314 -0.52767381 1.0057816 3545.426\na[14]   -1.07380916 0.13974519 -1.30386230 -0.84637135 0.9989985 2880.575\na[15]    0.51210060 0.13764298  0.29422378  0.72660891 1.0080993 2904.486\na[16]   -0.50313231 0.14101022 -0.73081864 -0.27814424 1.0016774 3045.534\na[17]    0.12823507 0.13916734 -0.09294399  0.34924909 1.0001611 2645.378\na[18]    0.35461277 0.13950420  0.13398911  0.57701194 1.0032782 3072.307\na[19]   -0.15271404 0.14287922 -0.37555582  0.07263233 1.0015358 3385.546\na[20]    0.57261419 0.14237918  0.34603018  0.79546032 1.0028629 3044.263\na_bar    0.08987243 0.24292037 -0.27879912  0.48183952 1.0004217 3048.228\nsigma_a  1.08438611 0.18586145  0.83071493  1.42053585 1.0031046 3442.261\nb        0.21272936 0.01015007  0.19638681  0.22871962 1.0000291 1129.199\nsigma    0.39928193 0.02262948  0.36390590  0.43656543 1.0156441 3378.770\n\nprecis(mdl_random_intercept) #most relevant parameters\n\n20 vector or matrix parameters hidden. Use depth=2 to show them.\n\n\n              mean         sd       5.5%     94.5%     rhat ess_bulk\na_bar   0.08987243 0.24292037 -0.2787991 0.4818395 1.000422 3048.228\nsigma_a 1.08438611 0.18586145  0.8307149 1.4205358 1.003105 3442.261\nb       0.21272936 0.01015007  0.1963868 0.2287196 1.000029 1129.199\nsigma   0.39928193 0.02262948  0.3639059 0.4365654 1.015644 3378.770\n\n#multi-level model, random intercepts and slopes.\nmdl_random &lt;- ulam(alist(\n  y ~ dnorm(mu,sigma),\n  mu &lt;- a[id] + b[id]*weeks,\n  a[id] ~ dnorm(a_bar,sigma_a),\n  a_bar ~ dnorm(0,1),\n  sigma_a ~ dexp(1),\n  b[id] ~ dnorm(b_bar,sigma_b),\n  b_bar ~ dnorm(0,1),\n  sigma_b ~ dexp(1),\n  sigma ~ dexp(1)\n), data = d, chains = 4, cores = 4)\n\nIn file included from stan/lib/stan_math/stan/math/prim/prob/von_mises_lccdf.hpp:5,\n                 from stan/lib/stan_math/stan/math/prim/prob/von_mises_ccdf_log.hpp:4,\n                 from stan/lib/stan_math/stan/math/prim/prob.hpp:356,\n                 from stan/lib/stan_math/stan/math/prim.hpp:16,\n                 from stan/lib/stan_math/stan/math/rev.hpp:14,\n                 from stan/lib/stan_math/stan/math.hpp:19,\n                 from stan/src/stan/model/model_header.hpp:4,\n                 from C:/Users/vilgo/AppData/Local/Temp/Rtmpy0zvj8/model-28bc22401029.hpp:2:\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp: In function 'stan::return_type_t&lt;T_x, T_sigma, T_l&gt; stan::math::von_mises_cdf(const T_x&, const T_mu&, const T_k&)':\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: '-Wmisleading-indentation' is disabled from this point onwards, since column-tracking was disabled due to the size of the code/headers\n  194 |       if (cdf_n &lt; 0.0)\n      | \n\n\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: adding '-flarge-source-files' will allow for more column-tracking support, at the expense of compilation time and memory\n\n\nRunning MCMC with 4 parallel chains, with 1 thread(s) per chain...\n\nChain 1 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 1 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 1 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 2 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 2 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 3 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 4 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 4 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 1 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 1 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 1 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 1 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 1 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 1 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 2 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 2 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 2 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 2 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 2 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 2 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 2 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 3 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 3 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 3 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 3 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 3 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 3 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 3 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 4 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 4 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 4 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 4 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 4 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 4 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 1 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 1 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 2 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 2 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 2 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 3 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 3 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 4 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 4 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 4 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 1 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 1 finished in 0.5 seconds.\nChain 2 finished in 0.4 seconds.\nChain 3 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 3 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 4 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 3 finished in 0.5 seconds.\nChain 4 finished in 0.4 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.5 seconds.\nTotal execution time: 0.7 seconds.\n\nprecis(mdl_random, depth = 2)\n\n               mean         sd         5.5%        94.5%      rhat ess_bulk\na[1]     0.13763362 0.20401689 -0.192673100  0.453791880 1.0016809 2400.392\na[2]    -1.16918868 0.18464673 -1.469531450 -0.868549215 1.0011623 2636.317\na[3]    -0.63657915 0.18564790 -0.930696490 -0.333380320 1.0004997 3304.281\na[4]    -0.94837438 0.18188301 -1.238314950 -0.646965225 1.0022509 2728.661\na[5]    -0.70659566 0.18588272 -1.013064950 -0.407646580 1.0002304 2441.167\na[6]     0.49739330 0.19678131  0.186874515  0.809785695 1.0002165 2855.974\na[7]     0.02333268 0.18673091 -0.270914670  0.329632750 0.9999923 2491.078\na[8]    -0.76884871 0.18410184 -1.058877250 -0.469463195 0.9991601 2677.698\na[9]     1.05371714 0.17832851  0.763279650  1.342631350 1.0007241 2479.162\na[10]    2.17960233 0.18870650  1.863835250  2.471498400 1.0013048 2302.449\na[11]    1.73940472 0.20379633  1.414371750  2.071898100 1.0003578 2338.700\na[12]    1.00962617 0.18431233  0.717254315  1.308446850 1.0026771 2955.093\na[13]   -0.78732622 0.18317331 -1.084791550 -0.491850020 1.0021275 2132.853\na[14]   -1.28735082 0.19064200 -1.605387700 -0.987822590 1.0004825 3185.551\na[15]    0.30584927 0.18758471  0.013459617  0.595883325 0.9999719 2612.923\na[16]   -0.28621499 0.18199055 -0.575899155 -0.001304038 0.9993551 2638.725\na[17]    0.09551850 0.18117889 -0.200388885  0.389704220 1.0046455 2603.652\na[18]    0.29510087 0.18552802  0.007532375  0.586815365 0.9999263 2773.765\na[19]    0.13720548 0.18507220 -0.159640890  0.428200625 1.0003737 2454.380\na[20]    0.75101203 0.22185446  0.408661745  1.095551350 1.0011165 3136.567\na_bar    0.07554967 0.22148420 -0.273502570  0.411122280 1.0002413 3887.325\nsigma_a  0.99857044 0.17603429  0.760254025  1.302518550 1.0029860 3147.878\nb[1]     0.17555283 0.03698009  0.116280280  0.235469125 1.0035438 2290.194\nb[2]     0.22601328 0.03282678  0.173460355  0.277233385 0.9996436 2763.655\nb[3]     0.18553127 0.03795384  0.126349360  0.245725895 1.0004982 3224.542\nb[4]     0.27818150 0.03423851  0.225094610  0.332742825 1.0025901 2709.904\nb[5]     0.11093633 0.03423799  0.057080752  0.165571915 1.0013364 2391.697\nb[6]     0.26696488 0.03458176  0.210526000  0.323312385 1.0008511 2661.686\nb[7]     0.22119065 0.03415080  0.166000185  0.275100210 0.9989249 2554.785\nb[8]     0.23681654 0.03334569  0.183751640  0.289308810 0.9989245 2609.819\nb[9]     0.14993050 0.03249067  0.098182279  0.202939615 1.0005990 2371.687\nb[10]    0.33164854 0.03448216  0.276640175  0.387383035 1.0000744 2236.169\nb[11]    0.34819513 0.04018706  0.283062535  0.411618505 1.0029311 2295.675\nb[12]    0.07602954 0.03424976  0.019293557  0.130471585 1.0029694 2609.973\nb[13]    0.21824780 0.03672063  0.161055030  0.276723140 1.0066972 2158.237\nb[14]    0.25820921 0.03389141  0.204913350  0.312336550 1.0028233 3318.883\nb[15]    0.26031897 0.03374351  0.207539000  0.315102325 0.9993097 2533.134\nb[16]    0.15981830 0.03510816  0.102849735  0.214442950 0.9994200 2727.007\nb[17]    0.22076857 0.03376817  0.164851810  0.274731880 1.0042257 2633.818\nb[18]    0.22793371 0.03656095  0.168902895  0.284702780 1.0008931 2824.816\nb[19]    0.14635664 0.03339399  0.091356924  0.199918775 0.9998916 2376.836\nb[20]    0.17686581 0.03819172  0.116701955  0.237775075 1.0006119 2964.347\nb_bar    0.21410393 0.02131508  0.180765165  0.248140580 1.0003901 2671.844\nsigma_b  0.08171234 0.01789594  0.057558535  0.110501015 1.0014209 2002.953\nsigma    0.33128625 0.01939546  0.302480820  0.363382790 1.0004200 2698.237\n\nprecis(mdl_random)\n\n40 vector or matrix parameters hidden. Use depth=2 to show them.\n\n\n              mean         sd        5.5%     94.5%     rhat ess_bulk\na_bar   0.07554967 0.22148420 -0.27350257 0.4111223 1.000241 3887.325\nsigma_a 0.99857044 0.17603429  0.76025402 1.3025186 1.002986 3147.878\nb_bar   0.21410393 0.02131508  0.18076517 0.2481406 1.000390 2671.844\nsigma_b 0.08171234 0.01789594  0.05755853 0.1105010 1.001421 2002.953\nsigma   0.33128625 0.01939546  0.30248082 0.3633828 1.000420 2698.237\n\n\nThe main estimand is the fixed effect, which turn out pretty similar regardless of model in this cleanly simulated data.\n\n\nNow let’s look at those priors I chose.\n\nprior &lt;- extract.prior(mdl_random)\n\nIn file included from stan/lib/stan_math/stan/math/prim/prob/von_mises_lccdf.hpp:5,\n                 from stan/lib/stan_math/stan/math/prim/prob/von_mises_ccdf_log.hpp:4,\n                 from stan/lib/stan_math/stan/math/prim/prob.hpp:356,\n                 from stan/lib/stan_math/stan/math/prim.hpp:16,\n                 from stan/lib/stan_math/stan/math/rev.hpp:14,\n                 from stan/lib/stan_math/stan/math.hpp:19,\n                 from stan/src/stan/model/model_header.hpp:4,\n                 from C:/Users/vilgo/AppData/Local/Temp/Rtmpy0zvj8/model-28bc5ee441d0.hpp:2:\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp: In function 'stan::return_type_t&lt;T_x, T_sigma, T_l&gt; stan::math::von_mises_cdf(const T_x&, const T_mu&, const T_k&)':\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: '-Wmisleading-indentation' is disabled from this point onwards, since column-tracking was disabled due to the size of the code/headers\n  194 |       if (cdf_n &lt; 0.0)\n      | \n\n\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: adding '-flarge-source-files' will allow for more column-tracking support, at the expense of compilation time and memory\n\n\nRunning MCMC with 1 chain, with 1 thread(s) per chain...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 0.8 seconds.\n\n\nWarning: 34 of 1000 (3.0%) transitions ended with a divergence.\nSee https://mc-stan.org/misc/warnings for details.\n\n\nWarning: 1 of 1 chains had an E-BFMI less than 0.2.\nSee https://mc-stan.org/misc/warnings for details.\n\n#20 patients random effects from different draws\n{plot(NULL, xlim = c(0,9), ylim = c(-3,3),\n      ylab = \"standardized symptoms\", xlab = \"weeks\", main = \"random effects\")\n  for(i in 1:20){\n    abline(a = prior$a[i,i], b = prior$b[i,i])\n  }}\n\n\n\n\n\n\n\n#20 draws fixedeffects\n{plot(NULL, xlim = c(0,9), ylim = c(-3,3),\n      ylab = \"standardized symptoms\", xlab = \"weeks\", main = \"fixed effects\")\n  for(i in 1:20){\n    abline(a = prior$a_bar[i], b = prior$b_bar[i])\n  }}\n\n\n\n\n\n\n\n\nWe can see that the priors I choose result in highly implausible effects, especially as it pertains to slopes. Let’s try to limit the variability in slopes by constraining the prior for \\(\\bar{b}\\) to 0.15.\n\nmdl_random_new &lt;- ulam(alist(\n  y ~ dnorm(mu,sigma),\n  mu &lt;- a[id] + b[id]*weeks,\n  a[id] ~ dnorm(a_bar,sigma_a),\n  a_bar ~ dnorm(0,1),\n  sigma_a ~ dexp(1.5),\n  b[id] ~ dnorm(b_bar,sigma_b),\n  b_bar ~ dnorm(0,0.15),\n  sigma_b ~ dexp(1),\n  sigma ~ dexp(1)\n), data = d, chains = 4, cores = 4)\n\nIn file included from stan/lib/stan_math/stan/math/prim/prob/von_mises_lccdf.hpp:5,\n                 from stan/lib/stan_math/stan/math/prim/prob/von_mises_ccdf_log.hpp:4,\n                 from stan/lib/stan_math/stan/math/prim/prob.hpp:356,\n                 from stan/lib/stan_math/stan/math/prim.hpp:16,\n                 from stan/lib/stan_math/stan/math/rev.hpp:14,\n                 from stan/lib/stan_math/stan/math.hpp:19,\n                 from stan/src/stan/model/model_header.hpp:4,\n                 from C:/Users/vilgo/AppData/Local/Temp/Rtmpy0zvj8/model-28bc642c4ca0.hpp:2:\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp: In function 'stan::return_type_t&lt;T_x, T_sigma, T_l&gt; stan::math::von_mises_cdf(const T_x&, const T_mu&, const T_k&)':\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: '-Wmisleading-indentation' is disabled from this point onwards, since column-tracking was disabled due to the size of the code/headers\n  194 |       if (cdf_n &lt; 0.0)\n      | \n\n\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: adding '-flarge-source-files' will allow for more column-tracking support, at the expense of compilation time and memory\n\n\nRunning MCMC with 4 parallel chains, with 1 thread(s) per chain...\n\nChain 1 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 1 Iteration: 100 / 1000 [ 10%]  (Warmup) \n\n\nChain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 1 Exception: normal_lpdf: Scale parameter is 0, but must be positive! (in 'C:/Users/vilgo/AppData/Local/Temp/Rtmpy0zvj8/model-28bc642c4ca0.stan', line 20, column 4 to column 34)\n\n\nChain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 1 \n\n\nChain 2 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 2 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 3 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 4 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 1 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 1 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 1 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 1 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 1 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 1 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 1 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 2 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 2 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 2 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 2 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 2 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 2 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 2 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 3 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 3 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 3 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 3 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 3 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 3 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 4 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 4 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 4 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 4 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 4 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 4 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 1 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 1 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 1 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 2 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 2 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 2 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 3 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 3 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 3 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 4 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 4 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 1 finished in 0.5 seconds.\nChain 2 finished in 0.5 seconds.\nChain 3 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 3 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 4 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 4 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 4 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 3 finished in 0.5 seconds.\nChain 4 finished in 0.5 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.5 seconds.\nTotal execution time: 0.7 seconds.\n\nprior &lt;- extract.prior(mdl_random_new)\n\nIn file included from stan/lib/stan_math/stan/math/prim/prob/von_mises_lccdf.hpp:5,\n                 from stan/lib/stan_math/stan/math/prim/prob/von_mises_ccdf_log.hpp:4,\n                 from stan/lib/stan_math/stan/math/prim/prob.hpp:356,\n                 from stan/lib/stan_math/stan/math/prim.hpp:16,\n                 from stan/lib/stan_math/stan/math/rev.hpp:14,\n                 from stan/lib/stan_math/stan/math.hpp:19,\n                 from stan/src/stan/model/model_header.hpp:4,\n                 from C:/Users/vilgo/AppData/Local/Temp/Rtmpy0zvj8/model-28bc5e826969.hpp:2:\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp: In function 'stan::return_type_t&lt;T_x, T_sigma, T_l&gt; stan::math::von_mises_cdf(const T_x&, const T_mu&, const T_k&)':\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: '-Wmisleading-indentation' is disabled from this point onwards, since column-tracking was disabled due to the size of the code/headers\n  194 |       if (cdf_n &lt; 0.0)\n      | \n\n\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: adding '-flarge-source-files' will allow for more column-tracking support, at the expense of compilation time and memory\n\n\nRunning MCMC with 1 chain, with 1 thread(s) per chain...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 1.0 seconds.\n\n\nWarning: 37 of 1000 (4.0%) transitions ended with a divergence.\nSee https://mc-stan.org/misc/warnings for details.\n\n\nWarning: 1 of 1 chains had an E-BFMI less than 0.2.\nSee https://mc-stan.org/misc/warnings for details.\n\n#20 patients random effects from different draws\n{plot(NULL, xlim = c(0,9), ylim = c(-3,3),\n      ylab = \"standardized symptoms\", xlab = \"weeks\", main = \"random effects\")\n  for(i in 1:20){\n    abline(a = prior$a[i,i], b = prior$b[i,i])\n  }}\n\n\n\n\n\n\n\n#20 draws fixed effects\n{plot(NULL, xlim = c(0,9), ylim = c(-3,3),\n      ylab = \"standardized symptoms\", xlab = \"weeks\", main = \"fixed effects\")\n  for(i in 1:20){\n    abline(a = prior$a_bar[i], b = prior$b_bar[i])\n  }}\n\n\n\n\n\n\n\n\nComment: Fixed effects now look more reasonable. Random/variable effects still vary a lot because even though their mean is constrained, their variability \\(\\sigma_b\\) is still large \\(\\mathbb{E}[Exponential(1)] = 1\\), which turns into a lot of variability if we recall this is per week change. Still, this shouldn’t matter too much when fitting the model since the exponential function with a rate of 1 has quite a lot of probability mass close to 0 anyway. Similarly \\(\\sigma_a\\) could probably be more narrow."
  },
  {
    "objectID": "notes/Stats2IndividualAssignment.html#my-data-and-model",
    "href": "notes/Stats2IndividualAssignment.html#my-data-and-model",
    "title": "Stats2IndividualAssignment",
    "section": "",
    "text": "My data will be self-rated GAD-7 scores from a clinical trial. Patients receive one out of two possible active treatments over ten weeks. I want to use multi-level modeling to model the effect of treatment over time, and compare that between treatments.\nThe formula is presented below for 20 patients (\\(j\\)). My outcome \\(y\\) is GAD-7 score. \\(time\\) will be the week the measurement comes from, ranging from 0 to 9, so that the intercepts \\(\\alpha\\) represent model estimations at start of treatment.\n\\[\n\\begin{align*}\ny_i \\sim Normal(\\mu_i,\\sigma) \\\\\n\\mu_{i,j} = \\alpha_{patient[i]} \\ + \\ \\beta_{patient[i]}*time_i \\\\\n\\alpha_j \\sim Normal(\\bar{\\alpha},\\sigma_a) \\ \\ ,for \\ j = 1,2,...20 \\\\\n\\bar{\\alpha} \\sim Normal(0,1) \\\\\n\\sigma_a \\sim Exponential(1) \\\\\n\\beta_j \\sim Normal(\\bar{\\beta},\\sigma_b) \\ \\ ,for \\ j = 1,2,...20 \\\\\n\\bar{\\beta} \\sim Normal(0,1) \\\\\n\\sigma_b \\sim Exponential(1) \\\\\n\\sigma \\sim Exponential(1)\n\\end{align*}\n\\]\nI have not figured out how to specify a group effect yet but one cheap solution is to just fit two models and compare. In this case that solution is probably adequate since I don’t want the estimate for group A to affect the estimate from group B anyway. However, in time I want to look at contrasts between competing mediators and then it may be better to include group as a factor.\nI have a function that generates data in long format from a previous project. Since the data simulates intercepts ~ Normal(0,1), it can be read as if I were to standardize the scores at week 0. With this standardization the avg_slope argument times the length of the treatment nweeks corresponds to the standardized effect size for growth models following Feingold (2007).\n\n#function that simulates data\nlong.data.sim &lt;- function(n = 30, avg_slope = 0.1, slope_intercept_correlation = -0.15, slope_sigma = 0.1, nweeks = 10, error = 0.3){\n  intercepts &lt;- rnorm(n, m = 0, sd = 1)\n  slopes &lt;- avg_slope + (slope_intercept_correlation*intercepts + rnorm(n,m=0,sd=sqrt(1-slope_intercept_correlation^2)) )*slope_sigma\n  weeks &lt;- 0:(nweeks-1)\n  df &lt;- data.frame()\n  for(i in 1:n){\n    err &lt;- rnorm(nweeks, m = 0, sd = error) #will equal residual std.dev. = sigma(mdl)\n    df &lt;- rbind.data.frame(df,data.frame(id=rep(i, times = nweeks),y=intercepts[i]+slopes[i]*weeks+err,weeks))\n  }\n  return(df)\n}\n\n#generate data\nd &lt;- long.data.sim(n = 20, avg_slope = 0.2, slope_intercept_correlation = -0.15, slope_sigma = 0.1, nweeks = 10, error = 0.3)\n\n#Let's also see if it runs if some patients have missing data some weeks (it will)\nd &lt;- d[-sample(1:nrow(d), 15, replace = FALSE),]\n\nI then built the models step by step increasing complexity.\n\n#single level model (with quadratic aproximation)\nmdl_non &lt;- quap(alist(\n  y ~ dnorm(mu,sigma),\n  mu &lt;- a + b*weeks,\n  a ~ dnorm(0,1),\n  b ~ dnorm(0,0.3),\n  sigma ~ dexp(1)\n), data = d)\nprecis(mdl_non)\n\n            mean         sd      5.5%     94.5%\na     0.07157867 0.14607351 -0.161875 0.3050323\nb     0.21349254 0.02770973  0.169207 0.2577780\nsigma 1.09237163 0.05654070  1.002009 1.1827346\n\n#multi-level model, random intercepts.\nmdl_random_intercept &lt;- ulam(alist(\n  y ~ dnorm(mu,sigma),\n  mu &lt;- a[id] + b*weeks,\n  a[id] ~ dnorm(a_bar,sigma_a),\n  a_bar ~ dnorm(0,1),\n  sigma_a ~ dexp(1),\n  b ~ dnorm(0,0.3),\n  sigma ~ dexp(1)\n), data = d, chains = 4, cores = 4)\n\nIn file included from stan/lib/stan_math/stan/math/prim/prob/von_mises_lccdf.hpp:5,\n                 from stan/lib/stan_math/stan/math/prim/prob/von_mises_ccdf_log.hpp:4,\n                 from stan/lib/stan_math/stan/math/prim/prob.hpp:356,\n                 from stan/lib/stan_math/stan/math/prim.hpp:16,\n                 from stan/lib/stan_math/stan/math/rev.hpp:14,\n                 from stan/lib/stan_math/stan/math.hpp:19,\n                 from stan/src/stan/model/model_header.hpp:4,\n                 from C:/Users/vilgo/AppData/Local/Temp/Rtmpy0zvj8/model-28bc500540dc.hpp:2:\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp: In function 'stan::return_type_t&lt;T_x, T_sigma, T_l&gt; stan::math::von_mises_cdf(const T_x&, const T_mu&, const T_k&)':\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: '-Wmisleading-indentation' is disabled from this point onwards, since column-tracking was disabled due to the size of the code/headers\n  194 |       if (cdf_n &lt; 0.0)\n      | \n\n\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: adding '-flarge-source-files' will allow for more column-tracking support, at the expense of compilation time and memory\n\n\nRunning MCMC with 4 parallel chains, with 1 thread(s) per chain...\n\nChain 1 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 1 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 1 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 1 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 1 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 2 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 2 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 3 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 3 Iteration: 100 / 1000 [ 10%]  (Warmup) \n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: normal_lpdf: Scale parameter is 0, but must be positive! (in 'C:/Users/vilgo/AppData/Local/Temp/Rtmpy0zvj8/model-28bc500540dc.stan', line 19, column 4 to column 34)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 4 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 1 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 1 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 1 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 1 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 1 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 1 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 1 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 1 finished in 0.2 seconds.\nChain 2 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 2 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 2 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 2 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 2 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 2 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 2 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 2 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 2 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 2 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 2 finished in 0.2 seconds.\nChain 3 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 3 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 3 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 3 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 3 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 3 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 3 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 3 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 3 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 3 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 3 finished in 0.2 seconds.\nChain 4 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 4 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 4 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 4 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 4 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 4 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 4 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 4 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 4 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 4 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 4 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 4 finished in 0.2 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.2 seconds.\nTotal execution time: 0.4 seconds.\n\nprecis(mdl_random_intercept, depth = 2)  #seems to run fine with missing rows.\n\n               mean         sd        5.5%       94.5%      rhat ess_bulk\na[1]    -0.02520281 0.14921578 -0.26603355  0.21877512 1.0010833 2202.248\na[2]    -1.10287172 0.13450625 -1.31535830 -0.89130662 1.0038579 3926.797\na[3]    -0.73813835 0.13973373 -0.95917408 -0.51288988 1.0032422 3060.956\na[4]    -0.65834556 0.13519752 -0.86900740 -0.44759664 1.0067085 3126.022\na[5]    -1.15892221 0.13544008 -1.37606300 -0.94658577 1.0046948 3002.016\na[6]     0.75545638 0.13922541  0.53084204  0.97754093 0.9996994 2850.140\na[7]     0.06189972 0.13673783 -0.15652840  0.28013837 1.0027208 2742.055\na[8]    -0.66021047 0.14058414 -0.87995546 -0.43961912 0.9990020 2557.988\na[9]     0.77055111 0.13624650  0.55998342  0.99166465 1.0018397 3225.269\na[10]    2.70448036 0.14066007  2.47965205  2.93030730 1.0014380 2975.519\na[11]    2.28466059 0.14146399  2.04461900  2.50176265 1.0026248 3201.186\na[12]    0.40256383 0.13609763  0.18062447  0.61482962 1.0008079 2271.321\na[13]   -0.76115211 0.13820998 -0.98172314 -0.52767381 1.0057816 3545.426\na[14]   -1.07380916 0.13974519 -1.30386230 -0.84637135 0.9989985 2880.575\na[15]    0.51210060 0.13764298  0.29422378  0.72660891 1.0080993 2904.486\na[16]   -0.50313231 0.14101022 -0.73081864 -0.27814424 1.0016774 3045.534\na[17]    0.12823507 0.13916734 -0.09294399  0.34924909 1.0001611 2645.378\na[18]    0.35461277 0.13950420  0.13398911  0.57701194 1.0032782 3072.307\na[19]   -0.15271404 0.14287922 -0.37555582  0.07263233 1.0015358 3385.546\na[20]    0.57261419 0.14237918  0.34603018  0.79546032 1.0028629 3044.263\na_bar    0.08987243 0.24292037 -0.27879912  0.48183952 1.0004217 3048.228\nsigma_a  1.08438611 0.18586145  0.83071493  1.42053585 1.0031046 3442.261\nb        0.21272936 0.01015007  0.19638681  0.22871962 1.0000291 1129.199\nsigma    0.39928193 0.02262948  0.36390590  0.43656543 1.0156441 3378.770\n\nprecis(mdl_random_intercept) #most relevant parameters\n\n20 vector or matrix parameters hidden. Use depth=2 to show them.\n\n\n              mean         sd       5.5%     94.5%     rhat ess_bulk\na_bar   0.08987243 0.24292037 -0.2787991 0.4818395 1.000422 3048.228\nsigma_a 1.08438611 0.18586145  0.8307149 1.4205358 1.003105 3442.261\nb       0.21272936 0.01015007  0.1963868 0.2287196 1.000029 1129.199\nsigma   0.39928193 0.02262948  0.3639059 0.4365654 1.015644 3378.770\n\n#multi-level model, random intercepts and slopes.\nmdl_random &lt;- ulam(alist(\n  y ~ dnorm(mu,sigma),\n  mu &lt;- a[id] + b[id]*weeks,\n  a[id] ~ dnorm(a_bar,sigma_a),\n  a_bar ~ dnorm(0,1),\n  sigma_a ~ dexp(1),\n  b[id] ~ dnorm(b_bar,sigma_b),\n  b_bar ~ dnorm(0,1),\n  sigma_b ~ dexp(1),\n  sigma ~ dexp(1)\n), data = d, chains = 4, cores = 4)\n\nIn file included from stan/lib/stan_math/stan/math/prim/prob/von_mises_lccdf.hpp:5,\n                 from stan/lib/stan_math/stan/math/prim/prob/von_mises_ccdf_log.hpp:4,\n                 from stan/lib/stan_math/stan/math/prim/prob.hpp:356,\n                 from stan/lib/stan_math/stan/math/prim.hpp:16,\n                 from stan/lib/stan_math/stan/math/rev.hpp:14,\n                 from stan/lib/stan_math/stan/math.hpp:19,\n                 from stan/src/stan/model/model_header.hpp:4,\n                 from C:/Users/vilgo/AppData/Local/Temp/Rtmpy0zvj8/model-28bc22401029.hpp:2:\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp: In function 'stan::return_type_t&lt;T_x, T_sigma, T_l&gt; stan::math::von_mises_cdf(const T_x&, const T_mu&, const T_k&)':\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: '-Wmisleading-indentation' is disabled from this point onwards, since column-tracking was disabled due to the size of the code/headers\n  194 |       if (cdf_n &lt; 0.0)\n      | \n\n\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: adding '-flarge-source-files' will allow for more column-tracking support, at the expense of compilation time and memory\n\n\nRunning MCMC with 4 parallel chains, with 1 thread(s) per chain...\n\nChain 1 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 1 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 1 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 2 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 2 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 3 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 4 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 4 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 1 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 1 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 1 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 1 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 1 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 1 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 2 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 2 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 2 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 2 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 2 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 2 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 2 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 3 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 3 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 3 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 3 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 3 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 3 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 3 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 4 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 4 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 4 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 4 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 4 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 4 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 1 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 1 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 2 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 2 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 2 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 3 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 3 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 4 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 4 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 4 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 1 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 1 finished in 0.5 seconds.\nChain 2 finished in 0.4 seconds.\nChain 3 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 3 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 4 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 3 finished in 0.5 seconds.\nChain 4 finished in 0.4 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.5 seconds.\nTotal execution time: 0.7 seconds.\n\nprecis(mdl_random, depth = 2)\n\n               mean         sd         5.5%        94.5%      rhat ess_bulk\na[1]     0.13763362 0.20401689 -0.192673100  0.453791880 1.0016809 2400.392\na[2]    -1.16918868 0.18464673 -1.469531450 -0.868549215 1.0011623 2636.317\na[3]    -0.63657915 0.18564790 -0.930696490 -0.333380320 1.0004997 3304.281\na[4]    -0.94837438 0.18188301 -1.238314950 -0.646965225 1.0022509 2728.661\na[5]    -0.70659566 0.18588272 -1.013064950 -0.407646580 1.0002304 2441.167\na[6]     0.49739330 0.19678131  0.186874515  0.809785695 1.0002165 2855.974\na[7]     0.02333268 0.18673091 -0.270914670  0.329632750 0.9999923 2491.078\na[8]    -0.76884871 0.18410184 -1.058877250 -0.469463195 0.9991601 2677.698\na[9]     1.05371714 0.17832851  0.763279650  1.342631350 1.0007241 2479.162\na[10]    2.17960233 0.18870650  1.863835250  2.471498400 1.0013048 2302.449\na[11]    1.73940472 0.20379633  1.414371750  2.071898100 1.0003578 2338.700\na[12]    1.00962617 0.18431233  0.717254315  1.308446850 1.0026771 2955.093\na[13]   -0.78732622 0.18317331 -1.084791550 -0.491850020 1.0021275 2132.853\na[14]   -1.28735082 0.19064200 -1.605387700 -0.987822590 1.0004825 3185.551\na[15]    0.30584927 0.18758471  0.013459617  0.595883325 0.9999719 2612.923\na[16]   -0.28621499 0.18199055 -0.575899155 -0.001304038 0.9993551 2638.725\na[17]    0.09551850 0.18117889 -0.200388885  0.389704220 1.0046455 2603.652\na[18]    0.29510087 0.18552802  0.007532375  0.586815365 0.9999263 2773.765\na[19]    0.13720548 0.18507220 -0.159640890  0.428200625 1.0003737 2454.380\na[20]    0.75101203 0.22185446  0.408661745  1.095551350 1.0011165 3136.567\na_bar    0.07554967 0.22148420 -0.273502570  0.411122280 1.0002413 3887.325\nsigma_a  0.99857044 0.17603429  0.760254025  1.302518550 1.0029860 3147.878\nb[1]     0.17555283 0.03698009  0.116280280  0.235469125 1.0035438 2290.194\nb[2]     0.22601328 0.03282678  0.173460355  0.277233385 0.9996436 2763.655\nb[3]     0.18553127 0.03795384  0.126349360  0.245725895 1.0004982 3224.542\nb[4]     0.27818150 0.03423851  0.225094610  0.332742825 1.0025901 2709.904\nb[5]     0.11093633 0.03423799  0.057080752  0.165571915 1.0013364 2391.697\nb[6]     0.26696488 0.03458176  0.210526000  0.323312385 1.0008511 2661.686\nb[7]     0.22119065 0.03415080  0.166000185  0.275100210 0.9989249 2554.785\nb[8]     0.23681654 0.03334569  0.183751640  0.289308810 0.9989245 2609.819\nb[9]     0.14993050 0.03249067  0.098182279  0.202939615 1.0005990 2371.687\nb[10]    0.33164854 0.03448216  0.276640175  0.387383035 1.0000744 2236.169\nb[11]    0.34819513 0.04018706  0.283062535  0.411618505 1.0029311 2295.675\nb[12]    0.07602954 0.03424976  0.019293557  0.130471585 1.0029694 2609.973\nb[13]    0.21824780 0.03672063  0.161055030  0.276723140 1.0066972 2158.237\nb[14]    0.25820921 0.03389141  0.204913350  0.312336550 1.0028233 3318.883\nb[15]    0.26031897 0.03374351  0.207539000  0.315102325 0.9993097 2533.134\nb[16]    0.15981830 0.03510816  0.102849735  0.214442950 0.9994200 2727.007\nb[17]    0.22076857 0.03376817  0.164851810  0.274731880 1.0042257 2633.818\nb[18]    0.22793371 0.03656095  0.168902895  0.284702780 1.0008931 2824.816\nb[19]    0.14635664 0.03339399  0.091356924  0.199918775 0.9998916 2376.836\nb[20]    0.17686581 0.03819172  0.116701955  0.237775075 1.0006119 2964.347\nb_bar    0.21410393 0.02131508  0.180765165  0.248140580 1.0003901 2671.844\nsigma_b  0.08171234 0.01789594  0.057558535  0.110501015 1.0014209 2002.953\nsigma    0.33128625 0.01939546  0.302480820  0.363382790 1.0004200 2698.237\n\nprecis(mdl_random)\n\n40 vector or matrix parameters hidden. Use depth=2 to show them.\n\n\n              mean         sd        5.5%     94.5%     rhat ess_bulk\na_bar   0.07554967 0.22148420 -0.27350257 0.4111223 1.000241 3887.325\nsigma_a 0.99857044 0.17603429  0.76025402 1.3025186 1.002986 3147.878\nb_bar   0.21410393 0.02131508  0.18076517 0.2481406 1.000390 2671.844\nsigma_b 0.08171234 0.01789594  0.05755853 0.1105010 1.001421 2002.953\nsigma   0.33128625 0.01939546  0.30248082 0.3633828 1.000420 2698.237\n\n\nThe main estimand is the fixed effect, which turn out pretty similar regardless of model in this cleanly simulated data.\n\n\nNow let’s look at those priors I chose.\n\nprior &lt;- extract.prior(mdl_random)\n\nIn file included from stan/lib/stan_math/stan/math/prim/prob/von_mises_lccdf.hpp:5,\n                 from stan/lib/stan_math/stan/math/prim/prob/von_mises_ccdf_log.hpp:4,\n                 from stan/lib/stan_math/stan/math/prim/prob.hpp:356,\n                 from stan/lib/stan_math/stan/math/prim.hpp:16,\n                 from stan/lib/stan_math/stan/math/rev.hpp:14,\n                 from stan/lib/stan_math/stan/math.hpp:19,\n                 from stan/src/stan/model/model_header.hpp:4,\n                 from C:/Users/vilgo/AppData/Local/Temp/Rtmpy0zvj8/model-28bc5ee441d0.hpp:2:\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp: In function 'stan::return_type_t&lt;T_x, T_sigma, T_l&gt; stan::math::von_mises_cdf(const T_x&, const T_mu&, const T_k&)':\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: '-Wmisleading-indentation' is disabled from this point onwards, since column-tracking was disabled due to the size of the code/headers\n  194 |       if (cdf_n &lt; 0.0)\n      | \n\n\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: adding '-flarge-source-files' will allow for more column-tracking support, at the expense of compilation time and memory\n\n\nRunning MCMC with 1 chain, with 1 thread(s) per chain...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 0.8 seconds.\n\n\nWarning: 34 of 1000 (3.0%) transitions ended with a divergence.\nSee https://mc-stan.org/misc/warnings for details.\n\n\nWarning: 1 of 1 chains had an E-BFMI less than 0.2.\nSee https://mc-stan.org/misc/warnings for details.\n\n#20 patients random effects from different draws\n{plot(NULL, xlim = c(0,9), ylim = c(-3,3),\n      ylab = \"standardized symptoms\", xlab = \"weeks\", main = \"random effects\")\n  for(i in 1:20){\n    abline(a = prior$a[i,i], b = prior$b[i,i])\n  }}\n\n\n\n\n\n\n\n#20 draws fixedeffects\n{plot(NULL, xlim = c(0,9), ylim = c(-3,3),\n      ylab = \"standardized symptoms\", xlab = \"weeks\", main = \"fixed effects\")\n  for(i in 1:20){\n    abline(a = prior$a_bar[i], b = prior$b_bar[i])\n  }}\n\n\n\n\n\n\n\n\nWe can see that the priors I choose result in highly implausible effects, especially as it pertains to slopes. Let’s try to limit the variability in slopes by constraining the prior for \\(\\bar{b}\\) to 0.15.\n\nmdl_random_new &lt;- ulam(alist(\n  y ~ dnorm(mu,sigma),\n  mu &lt;- a[id] + b[id]*weeks,\n  a[id] ~ dnorm(a_bar,sigma_a),\n  a_bar ~ dnorm(0,1),\n  sigma_a ~ dexp(1.5),\n  b[id] ~ dnorm(b_bar,sigma_b),\n  b_bar ~ dnorm(0,0.15),\n  sigma_b ~ dexp(1),\n  sigma ~ dexp(1)\n), data = d, chains = 4, cores = 4)\n\nIn file included from stan/lib/stan_math/stan/math/prim/prob/von_mises_lccdf.hpp:5,\n                 from stan/lib/stan_math/stan/math/prim/prob/von_mises_ccdf_log.hpp:4,\n                 from stan/lib/stan_math/stan/math/prim/prob.hpp:356,\n                 from stan/lib/stan_math/stan/math/prim.hpp:16,\n                 from stan/lib/stan_math/stan/math/rev.hpp:14,\n                 from stan/lib/stan_math/stan/math.hpp:19,\n                 from stan/src/stan/model/model_header.hpp:4,\n                 from C:/Users/vilgo/AppData/Local/Temp/Rtmpy0zvj8/model-28bc642c4ca0.hpp:2:\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp: In function 'stan::return_type_t&lt;T_x, T_sigma, T_l&gt; stan::math::von_mises_cdf(const T_x&, const T_mu&, const T_k&)':\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: '-Wmisleading-indentation' is disabled from this point onwards, since column-tracking was disabled due to the size of the code/headers\n  194 |       if (cdf_n &lt; 0.0)\n      | \n\n\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: adding '-flarge-source-files' will allow for more column-tracking support, at the expense of compilation time and memory\n\n\nRunning MCMC with 4 parallel chains, with 1 thread(s) per chain...\n\nChain 1 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 1 Iteration: 100 / 1000 [ 10%]  (Warmup) \n\n\nChain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 1 Exception: normal_lpdf: Scale parameter is 0, but must be positive! (in 'C:/Users/vilgo/AppData/Local/Temp/Rtmpy0zvj8/model-28bc642c4ca0.stan', line 20, column 4 to column 34)\n\n\nChain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 1 \n\n\nChain 2 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 2 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 3 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 4 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 1 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 1 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 1 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 1 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 1 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 1 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 1 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 2 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 2 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 2 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 2 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 2 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 2 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 2 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 3 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 3 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 3 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 3 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 3 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 3 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 4 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 4 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 4 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 4 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 4 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 4 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 1 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 1 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 1 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 2 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 2 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 2 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 3 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 3 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 3 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 4 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 4 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 1 finished in 0.5 seconds.\nChain 2 finished in 0.5 seconds.\nChain 3 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 3 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 4 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 4 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 4 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 3 finished in 0.5 seconds.\nChain 4 finished in 0.5 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.5 seconds.\nTotal execution time: 0.7 seconds.\n\nprior &lt;- extract.prior(mdl_random_new)\n\nIn file included from stan/lib/stan_math/stan/math/prim/prob/von_mises_lccdf.hpp:5,\n                 from stan/lib/stan_math/stan/math/prim/prob/von_mises_ccdf_log.hpp:4,\n                 from stan/lib/stan_math/stan/math/prim/prob.hpp:356,\n                 from stan/lib/stan_math/stan/math/prim.hpp:16,\n                 from stan/lib/stan_math/stan/math/rev.hpp:14,\n                 from stan/lib/stan_math/stan/math.hpp:19,\n                 from stan/src/stan/model/model_header.hpp:4,\n                 from C:/Users/vilgo/AppData/Local/Temp/Rtmpy0zvj8/model-28bc5e826969.hpp:2:\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp: In function 'stan::return_type_t&lt;T_x, T_sigma, T_l&gt; stan::math::von_mises_cdf(const T_x&, const T_mu&, const T_k&)':\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: '-Wmisleading-indentation' is disabled from this point onwards, since column-tracking was disabled due to the size of the code/headers\n  194 |       if (cdf_n &lt; 0.0)\n      | \n\n\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: adding '-flarge-source-files' will allow for more column-tracking support, at the expense of compilation time and memory\n\n\nRunning MCMC with 1 chain, with 1 thread(s) per chain...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 1.0 seconds.\n\n\nWarning: 37 of 1000 (4.0%) transitions ended with a divergence.\nSee https://mc-stan.org/misc/warnings for details.\n\n\nWarning: 1 of 1 chains had an E-BFMI less than 0.2.\nSee https://mc-stan.org/misc/warnings for details.\n\n#20 patients random effects from different draws\n{plot(NULL, xlim = c(0,9), ylim = c(-3,3),\n      ylab = \"standardized symptoms\", xlab = \"weeks\", main = \"random effects\")\n  for(i in 1:20){\n    abline(a = prior$a[i,i], b = prior$b[i,i])\n  }}\n\n\n\n\n\n\n\n#20 draws fixed effects\n{plot(NULL, xlim = c(0,9), ylim = c(-3,3),\n      ylab = \"standardized symptoms\", xlab = \"weeks\", main = \"fixed effects\")\n  for(i in 1:20){\n    abline(a = prior$a_bar[i], b = prior$b_bar[i])\n  }}\n\n\n\n\n\n\n\n\nComment: Fixed effects now look more reasonable. Random/variable effects still vary a lot because even though their mean is constrained, their variability \\(\\sigma_b\\) is still large \\(\\mathbb{E}[Exponential(1)] = 1\\), which turns into a lot of variability if we recall this is per week change. Still, this shouldn’t matter too much when fitting the model since the exponential function with a rate of 1 has quite a lot of probability mass close to 0 anyway. Similarly \\(\\sigma_a\\) could probably be more narrow."
  },
  {
    "objectID": "notes/Stats2IndividualAssignment.html#extra-model-with-slope-intercept-correlation",
    "href": "notes/Stats2IndividualAssignment.html#extra-model-with-slope-intercept-correlation",
    "title": "Stats2IndividualAssignment",
    "section": "Extra: Model with slope-intercept correlation",
    "text": "Extra: Model with slope-intercept correlation\nHere I attempt a model with a slope-intercept correlation for the varying effects.\n\\[\n\\begin{align}\ny_i \\sim Normal(\\mu_i,\\sigma) \\\\\n\\mu_{i,j} = \\alpha_{patient[i]} \\ + \\ \\beta_{patient[i]}*time_i \\\\\n\\bigl[ \\substack{\\alpha_j \\\\ \\beta_j} \\bigr] \\sim MVNormal(\\bigl[ \\substack{\\alpha \\\\ \\beta} \\bigr], S) \\\\\nS = \\begin{pmatrix} 0 & \\sigma_a \\\\ \\sigma_b & 0 \\end{pmatrix} R \\begin{pmatrix} 0 & \\sigma_a \\\\ \\sigma_b & 0 \\end{pmatrix}\\\\\n\\alpha \\sim Normal(0,1) \\\\\n\\beta \\sim Normal(0,1) \\\\\n\\sigma_a \\sim Exponential(1) \\\\\n\\sigma_b \\sim Exponential(1) \\\\\n\\sigma \\sim Exponential(1) \\\\\nR \\sim LKJcorr(1.5)\n\\end{align}\n\\]\n\nmdl_random_correlated &lt;- ulam(alist(\n  y ~ dnorm(mu,sigma),\n  mu &lt;- a_id[id] + b_id[id]*weeks,\n  c(a_id,b_id)[id] ~ multi_normal(c(a,b), Rho, sigma_a), #for some reason I can only specify one sigma here. c(sigma_a, sigma_b) does not work... It treats sigma_a as a repeated vector. Annoying since the priors for sigma_a and sigma_b have quite different implications.\n  a ~ dnorm(0,1),\n  sigma_a ~ dexp(1),\n  b ~ dnorm(0,1),\n  sigma ~ dexp(1),\n  Rho ~ lkj_corr(1.5)\n), data = d, chains = 4, cores = 4)\n\nIn file included from stan/lib/stan_math/stan/math/prim/prob/von_mises_lccdf.hpp:5,\n                 from stan/lib/stan_math/stan/math/prim/prob/von_mises_ccdf_log.hpp:4,\n                 from stan/lib/stan_math/stan/math/prim/prob.hpp:356,\n                 from stan/lib/stan_math/stan/math/prim.hpp:16,\n                 from stan/lib/stan_math/stan/math/rev.hpp:14,\n                 from stan/lib/stan_math/stan/math.hpp:19,\n                 from stan/src/stan/model/model_header.hpp:4,\n                 from C:/Users/vilgo/AppData/Local/Temp/Rtmpy0zvj8/model-28bc14ca71d1.hpp:2:\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp: In function 'stan::return_type_t&lt;T_x, T_sigma, T_l&gt; stan::math::von_mises_cdf(const T_x&, const T_mu&, const T_k&)':\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: '-Wmisleading-indentation' is disabled from this point onwards, since column-tracking was disabled due to the size of the code/headers\n  194 |       if (cdf_n &lt; 0.0)\n      | \n\n\nstan/lib/stan_math/stan/math/prim/prob/von_mises_cdf.hpp:194: note: adding '-flarge-source-files' will allow for more column-tracking support, at the expense of compilation time and memory\n\n\nRunning MCMC with 4 parallel chains, with 1 thread(s) per chain...\n\nChain 1 Iteration:   1 / 1000 [  0%]  (Warmup) \n\n\nChain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 1 Exception: lkj_corr_lpdf: Correlation matrix is not positive definite. (in 'C:/Users/vilgo/AppData/Local/Temp/Rtmpy0zvj8/model-28bc14ca71d1.stan', line 17, column 4 to column 26)\n\n\nChain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 1 \n\n\nChain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 1 Exception: lkj_corr_lpdf: Correlation matrix is not positive definite. (in 'C:/Users/vilgo/AppData/Local/Temp/Rtmpy0zvj8/model-28bc14ca71d1.stan', line 17, column 4 to column 26)\n\n\nChain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 1 \n\n\nChain 2 Iteration:   1 / 1000 [  0%]  (Warmup) \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: lkj_corr_lpdf: Correlation matrix is not positive definite. (in 'C:/Users/vilgo/AppData/Local/Temp/Rtmpy0zvj8/model-28bc14ca71d1.stan', line 17, column 4 to column 26)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 3 Iteration:   1 / 1000 [  0%]  (Warmup) \n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: lkj_corr_lpdf: Correlation matrix is not positive definite. (in 'C:/Users/vilgo/AppData/Local/Temp/Rtmpy0zvj8/model-28bc14ca71d1.stan', line 17, column 4 to column 26)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: lkj_corr_lpdf: Correlation matrix is not positive definite. (in 'C:/Users/vilgo/AppData/Local/Temp/Rtmpy0zvj8/model-28bc14ca71d1.stan', line 17, column 4 to column 26)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: lkj_corr_lpdf: Correlation matrix is not positive definite. (in 'C:/Users/vilgo/AppData/Local/Temp/Rtmpy0zvj8/model-28bc14ca71d1.stan', line 17, column 4 to column 26)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: lkj_corr_lpdf: Correlation matrix is not positive definite. (in 'C:/Users/vilgo/AppData/Local/Temp/Rtmpy0zvj8/model-28bc14ca71d1.stan', line 17, column 4 to column 26)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: lkj_corr_lpdf: Correlation matrix is not positive definite. (in 'C:/Users/vilgo/AppData/Local/Temp/Rtmpy0zvj8/model-28bc14ca71d1.stan', line 17, column 4 to column 26)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: lkj_corr_lpdf: Correlation matrix is not positive definite. (in 'C:/Users/vilgo/AppData/Local/Temp/Rtmpy0zvj8/model-28bc14ca71d1.stan', line 17, column 4 to column 26)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: lkj_corr_lpdf: Correlation matrix is not positive definite. (in 'C:/Users/vilgo/AppData/Local/Temp/Rtmpy0zvj8/model-28bc14ca71d1.stan', line 17, column 4 to column 26)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 4 Iteration:   1 / 1000 [  0%]  (Warmup) \n\n\nChain 4 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 4 Exception: lkj_corr_lpdf: Correlation matrix is not positive definite. (in 'C:/Users/vilgo/AppData/Local/Temp/Rtmpy0zvj8/model-28bc14ca71d1.stan', line 17, column 4 to column 26)\n\n\nChain 4 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 4 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 4 \n\n\nChain 4 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 4 Exception: lkj_corr_lpdf: Correlation matrix is not positive definite. (in 'C:/Users/vilgo/AppData/Local/Temp/Rtmpy0zvj8/model-28bc14ca71d1.stan', line 17, column 4 to column 26)\n\n\nChain 4 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 4 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 4 \n\n\nChain 4 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 4 Exception: lkj_corr_lpdf: Correlation matrix is not positive definite. (in 'C:/Users/vilgo/AppData/Local/Temp/Rtmpy0zvj8/model-28bc14ca71d1.stan', line 17, column 4 to column 26)\n\n\nChain 4 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 4 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 4 \n\n\nChain 4 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 4 Exception: lkj_corr_lpdf: Correlation matrix is not positive definite. (in 'C:/Users/vilgo/AppData/Local/Temp/Rtmpy0zvj8/model-28bc14ca71d1.stan', line 17, column 4 to column 26)\n\n\nChain 4 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 4 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 4 \n\n\nChain 1 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 2 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 3 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 4 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 1 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 2 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 4 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 1 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 2 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 3 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 4 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 1 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 2 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 3 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 3 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 4 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 1 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 1 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 2 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 2 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 3 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 3 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 4 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 4 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 1 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 1 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 2 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 3 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 4 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 1 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 2 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 3 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 4 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 1 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 2 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 3 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 4 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 1 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 2 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 2 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 3 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 4 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 4 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 1 finished in 1.2 seconds.\nChain 2 finished in 1.2 seconds.\nChain 4 finished in 1.2 seconds.\nChain 3 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 3 finished in 1.2 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 1.2 seconds.\nTotal execution time: 1.4 seconds.\n\n\nI get a lot of warnings. However I interpret the warning “If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine” as an indication that this problem is pretty expected here. Looking at precis() below the rhats seem fine.\n\nprecis(mdl_random_correlated, depth = 3) \n\n                  mean         sd        5.5%         94.5%      rhat ess_bulk\nb_id[1]     0.17669626 0.03553421  0.12093220  0.2329488050 1.0012134 2917.906\nb_id[2]     0.22145559 0.03304110  0.16990658  0.2748809400 1.0031264 3413.439\nb_id[3]     0.18434513 0.03732565  0.12533119  0.2422476400 1.0010469 3465.627\nb_id[4]     0.27567014 0.03340783  0.22317796  0.3318849200 0.9999968 3554.463\nb_id[5]     0.11039179 0.03355736  0.05631473  0.1656054950 1.0005530 3129.154\nb_id[6]     0.26820252 0.03454081  0.21209475  0.3242612900 1.0011195 3031.353\nb_id[7]     0.21997077 0.03334915  0.16494550  0.2748798500 0.9992327 3309.702\nb_id[8]     0.23357677 0.03369836  0.18028684  0.2870902500 1.0008546 2465.568\nb_id[9]     0.15297642 0.03355186  0.10113697  0.2058835550 1.0028276 3146.139\nb_id[10]    0.33455924 0.03388681  0.27781557  0.3885598500 0.9995131 3707.049\nb_id[11]    0.35225692 0.04028725  0.28910959  0.4159689750 1.0002963 3190.102\nb_id[12]    0.07875229 0.03607353  0.02170077  0.1373183200 1.0013993 1965.257\nb_id[13]    0.21548517 0.03741131  0.15609782  0.2747518500 1.0034483 3767.847\nb_id[14]    0.25496926 0.03462412  0.19914025  0.3102808700 1.0044420 2581.053\nb_id[15]    0.26020372 0.03215319  0.21058762  0.3120072900 0.9995984 2913.487\nb_id[16]    0.16113164 0.03397375  0.10637139  0.2159037700 1.0040639 3211.447\nb_id[17]    0.22021784 0.03313242  0.16788602  0.2729621650 1.0108866 3676.066\nb_id[18]    0.22946066 0.03493809  0.17211903  0.2857401850 1.0008865 2604.672\nb_id[19]    0.14795302 0.03323039  0.09532655  0.2017777250 1.0002610 2772.949\nb_id[20]    0.18003044 0.03897877  0.11715083  0.2426505850 1.0002832 2916.321\na_id[1]     0.13487389 0.19770209 -0.18096550  0.4436078300 1.0041735 3062.807\na_id[2]    -1.14948816 0.18226095 -1.43846575 -0.8618922800 1.0044148 4128.470\na_id[3]    -0.62658303 0.18259524 -0.90805529 -0.3313473100 1.0029558 3482.623\na_id[4]    -0.93541606 0.17882787 -1.23048155 -0.6551982050 0.9993739 3338.520\na_id[5]    -0.70430011 0.18383821 -0.99848009 -0.4040508750 1.0005783 2850.351\na_id[6]     0.49751725 0.19400697  0.18729833  0.7930405400 0.9993051 3034.489\na_id[7]     0.03051752 0.18176117 -0.25632471  0.3232140850 0.9997550 2879.166\na_id[8]    -0.75402800 0.18258381 -1.04538775 -0.4600424300 1.0005925 3102.486\na_id[9]     1.03435234 0.18137951  0.74835034  1.3176076000 1.0047276 3193.936\na_id[10]    2.16855664 0.18074470  1.88874835  2.4673220000 0.9991712 3390.995\na_id[11]    1.72489880 0.19878958  1.41419725  2.0456650000 1.0002048 3332.470\na_id[12]    0.98958858 0.19077018  0.68032082  1.2931812500 1.0014414 2397.772\na_id[13]   -0.77661055 0.18385641 -1.06203055 -0.4823086200 0.9997682 3576.332\na_id[14]   -1.26521125 0.19680436 -1.58571485 -0.9579144200 1.0011688 2637.880\na_id[15]    0.31142792 0.17880296  0.02786370  0.5915412550 1.0011379 3054.359\na_id[16]   -0.29175907 0.18237218 -0.58808523  0.0007084818 1.0092575 3148.936\na_id[17]    0.09980958 0.17871560 -0.18794585  0.3880118550 1.0055821 3765.294\na_id[18]    0.29349224 0.17673115  0.01241182  0.5768513550 0.9997779 2637.333\na_id[19]    0.12846113 0.18458530 -0.16016782  0.4309638000 0.9989320 2752.010\na_id[20]    0.73357723 0.22412747  0.37900075  1.0996244000 1.0007233 2905.838\na           0.07904683 0.21677731 -0.26738110  0.4154573500 1.0015865 3173.621\nsigma_a[1]  1.00988666 0.17990684  0.75924678  1.3322855000 0.9997528 3562.037\nsigma_a[2]  0.08276055 0.01778055  0.05833212  0.1130584600 1.0006874 1644.031\nb           0.21357904 0.02107141  0.17973075  0.2470694150 1.0000303 2703.184\nsigma       0.33154406 0.01925279  0.30297062  0.3617909350 1.0019984 2472.055\nRho[1,1]    1.00000000 0.00000000  1.00000000  1.0000000000        NA       NA\nRho[2,1]    0.11543247 0.23298630 -0.26088732  0.4750070500 0.9998740 2094.633\nRho[1,2]    0.11543247 0.23298630 -0.26088732  0.4750070500 0.9998740 2094.633\nRho[2,2]    1.00000000 0.00000000  1.00000000  1.0000000000        NA       NA\n\npost &lt;- extract.samples(mdl_random_correlated)\nmean(post$Rho[,1,2]) #should be close-ish to -0.15, but the variable has a lot of simulation variance in smaller samples.\n\n[1] 0.1154325"
  },
  {
    "objectID": "posts/20250417_First_Blogpost/index.html",
    "href": "posts/20250417_First_Blogpost/index.html",
    "title": "Against using Nicholas Cage movies to teach correlation and causation",
    "section": "",
    "text": "I recently took a short statistics course as part of my PhD-studies and came across one of my pet-peeves in introductory statistics content. In every intro to stats book or course, one of the objectives is to teach students that correlation does not imply causation. Very often this is taught with a graph like this:\nThe students in the audience laugh. What a silly correlation! Obviously there’s no causality there! They write down “correlation does not imply causation” and then the teacher move on to the next powerpoint slide.\nI don’t like these for a number of reasons."
  },
  {
    "objectID": "posts/20250417_First_Blogpost/index.html#reason-1-wheres-the-doom-and-gloom",
    "href": "posts/20250417_First_Blogpost/index.html#reason-1-wheres-the-doom-and-gloom",
    "title": "Against using Nicholas Cage movies to teach correlation and causation",
    "section": "Reason 1: Where’s the doom and gloom?",
    "text": "Reason 1: Where’s the doom and gloom?\nMy problem with these demonstrations is not that they don’t effectively communicate the point, but that they’re too effective. The non-validity of the correlation is so clear that you barely have to think about why anyone would ever come to draw causal conclusions from observational data. But we really do that all the time. Falling into causal thinking is a strong tendency in human cognition. Counteracting that takes effort. That means one should be suspicious of the pedagogical value of demonstrations that are absorbed so effortlessly.\nI think it’s better to set up examples where inferring causation comes naturally to people, then trip people up. Any good introduction to the idea “correlation does not imply causation” should first evoke a feeling of confusion. I always liked the example Daniel Kahneman (Rest In Peace) brought up in Thinking Fast and Slow when talking about our bias towards causal thinking:\n“Highly intelligent women tend to marry men who are less intelligent than they are.”"
  },
  {
    "objectID": "posts/20250417_First_Blogpost/index.html#reason-2-why-always-time-series-data",
    "href": "posts/20250417_First_Blogpost/index.html#reason-2-why-always-time-series-data",
    "title": "Against using Nicholas Cage movies to teach correlation and causation",
    "section": "Reason 2: Why always time-series data?",
    "text": "Reason 2: Why always time-series data?\nThis one is less of a consistent problem in intro-to-stats content but most examples I’ve seen have used time-series correlations to make this point. I think this is one of the reasons the demonstration slide down so easily. A lot of things happen in the world, just because thing A happened before thing B doesn’t mean there’s a causal relationship. Obviously! Although people sometimes do make that type of inference too easily, see for example this graph about how reduced institutionalization in mental hospitals in the U.S. was followed by a growing prison population, which I saw uncritically reposted multiple times last year, it is very clear to understand conceptually when no plausible causal link comes to mind.1\nI think using time-series correlations where there is a readily available causal idea would be an improvement pedagogically. But most correlations we come across as in research won’t be time-series data. Instead it will often concern inter-individual variation, like a relationship between interleukin-6 and depressive symptoms, and the result will instead be visualized on a scatterplot with a regression line. I think the nature of these types of correlations are much more easily imbued with a vague aura of causality. The very same individuals that had more interleukin had worse depressive symptoms!\nThis type of data is just as observational as the time-series examples. I think it’s better to use them since they’re closer to the most common form of correlation."
  },
  {
    "objectID": "posts/20250417_First_Blogpost/index.html#reason-3-random-correlation-vs.-spurious-correlation",
    "href": "posts/20250417_First_Blogpost/index.html#reason-3-random-correlation-vs.-spurious-correlation",
    "title": "Against using Nicholas Cage movies to teach correlation and causation",
    "section": "Reason 3: random correlation vs. spurious correlation",
    "text": "Reason 3: random correlation vs. spurious correlation\nI also think these funny and obviously non-causative correlations mix up (or at least combine) two different types of false relationships.\nIf you look at a lot of random things sometimes you’ll get a “statistically significant” correlation, even when there’s no possible way there’s a relationship. Indeed the example pictured above is barely different from finding a significant association between yearly Nicholas Cage movies and a string of random numbers I generate on my computer (after trying a bunch of times). This random-number-version seems to lack the same demonstrative oomph for showing the limits of observational data. That’s because these examples are teaching two lessons at the same time! Besides the difference between observational and experimental data they are, at the same time, teaching students about the limits of hypothesis testing itself. If there’s no true correlation between the things you’re looking at you’ll still get a “significant” p-value 5% of the time. Cue the xkcd comic:\n\n\n\nIf you’ve used this comic as an example of “correlation does not imply causality” you’ve made a mistake. The comic is about p-hacking/data-dredging. The scientists are presumably doing RCTs\n\n\nThis double-lesson-property could give the false impression that if you look at enough things, you simply need to correct for multiple comparisons and find causative links! (These types of corrections are necessary and looking for associations is of course important, it can be a promising way to figure out what to explore further. But all this presupposes some prior idea of a plausible causal link).\nRelatedly one might get the idea that replicating the correlation in a subsequent study gets you closer to proving a causal link. Teaching people about the limits of hypothesis testing and the correct interpretation of p-values is no doubt important, but it’s fundamentally different from the problem with observational data. Or to put it simply: Confounding replicates."
  },
  {
    "objectID": "posts/20250417_First_Blogpost/index.html#how-to-teach-this-then",
    "href": "posts/20250417_First_Blogpost/index.html#how-to-teach-this-then",
    "title": "Against using Nicholas Cage movies to teach correlation and causation",
    "section": "How to teach this then?",
    "text": "How to teach this then?\nI think good examples teaching this concept should first and foremost emphasize how confounders (e.g. common causes) can create plausible sounding and reliable statistical-but-non-causal relationships. And don’t use time-series data.2 Beyond that I think it should show the fundamental rift between experimental data with manipulation and randomization, and observational data about the state of the world. This really is the central thing: Even when we have plausible-sounding mechanisms, randomized experiments can reveal things aren’t as we thought. Until we’ve done those experiments, or if we can’t do them, we’re always in a deeply more uncertain place scientifically.\nI don’t know any examples that I’ve found perfectly elegant. But I remember that one of my favourite science communicators Ben Goldacre had a nice part in his book Bad Science where he talked about antioxidant vitamin pills and lung cancer. He first set up the observed correlation between these things, and also set up a plausible mechanism to explain the correlation (the free radical theory of aging). That all sounds well and good, but when researchers actually ran clinical trials they found that the people who took the beta-carotene and vitamin A pills were more (!) likely to die from lung cancer. I think examples like this are good for instilling some doom and gloom and doubt about correlational data, but in an ideal case one should also present what the source of the spurious correlation was.\nTo be fair, people rarely teach this concept with a single example. The problem I’m pointing at is a mostly unnecessary pet-peeve, I know. But I also do think the first impression people have of a concept matters, at least a bit. And for that purpose, I wish they would come up with something other that wacky time-series correlations.3"
  },
  {
    "objectID": "posts/20250417_First_Blogpost/index.html#footnotes",
    "href": "posts/20250417_First_Blogpost/index.html#footnotes",
    "title": "Against using Nicholas Cage movies to teach correlation and causation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThere is something extra fragile with time-series correlations. There’s only one world and history only happened once. I think this is one of the reasons subjects like political science is in an epistemically more difficult position than for example medicine.↩︎\nunless students are likely to primarily work with that.↩︎\nThis post may seem like an attack on Tyler Vigen’s spurious correlation website. I want to be clear I have nothing against the site and I think it’s a funny project and even a good viral type of science communication. It’s existence online is good! My problem here is with it’s repeated role in into-to-stats content for university level students.↩︎"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "R/stats/psych posts",
    "section": "",
    "text": "Here I share posts that connect either to psychology or to statistics. Some are cross-posted on substack.\n\n\n\n\n\n\n\n\n\n\n\n\nLongitudinal Mixed Model Explorer App\n\n\n\nshiny\n\n\nteaching\n\n\n\nI built a shiny app\n\n\n\nVilgot Huhn\n\n\nMay 6, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nAgainst using Nicholas Cage movies to teach correlation and causation\n\n\n\nteaching\n\n\n\nreposted from substack\n\n\n\nVilgot Huhn\n\n\nMar 31, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nOh No! Berkson’s paradox in clinical theories\n\n\n\ncausal inference\n\n\n\nreposted from substack\n\n\n\nVilgot Huhn\n\n\nDec 16, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  }
]